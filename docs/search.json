[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSC 477 - Group 4",
    "section": "",
    "text": "Project Scopes\nThis is a Quarto book for Group 4. Below are the project scopes for each of the datasets chosen by the team. Essentially a general direction in which we will be using our datasets for the midterm project.",
    "crumbs": [
      "Project Scopes"
    ]
  },
  {
    "objectID": "index.html#samuel-zelaya---nasa-earth-data",
    "href": "index.html#samuel-zelaya---nasa-earth-data",
    "title": "CSC 477 - Group 4",
    "section": "Samuel Zelaya - NASA Earth Data",
    "text": "Samuel Zelaya - NASA Earth Data\n\nThe purpose of this research is to evaluate the impacts of climate change on crop yields, specifically in relation to changes in temperature and precipitation. The study aims to identify potential adaptations and variations in yields across different countries and regions. It provides valuable insights into global food supply, price fluctuations, and the risks of hunger exacerbated by climate changes",
    "crumbs": [
      "Project Scopes"
    ]
  },
  {
    "objectID": "index.html#janice-oenardi---federal-reserve-economic-data",
    "href": "index.html#janice-oenardi---federal-reserve-economic-data",
    "title": "CSC 477 - Group 4",
    "section": "Janice Oenardi - Federal Reserve Economic Data",
    "text": "Janice Oenardi - Federal Reserve Economic Data\n\nI chose the FRED source because I would like to delve more into the US economy to conduct economic analysis and financial research as a business major. Additionally, there are a lot of different datasets, including from different countries, that I an explore from the source. There are several objectives/purposes of the data set that can be relevant fo the mid-tern project. I can delve into the economic indicators over time, such as:\n\nGDP Growth Rate: To observe economic expansion or contraction.\nUnemployment Rate: To identify periods of high or low employment.\nInflation Rate: To detect inflationary or deflationary trends.\nInterest Rates: To understand changes in the cost of borrowing.",
    "crumbs": [
      "Project Scopes"
    ]
  },
  {
    "objectID": "index.html#diego-mendoza---us-department-of-agriculture-data",
    "href": "index.html#diego-mendoza---us-department-of-agriculture-data",
    "title": "CSC 477 - Group 4",
    "section": "Diego Mendoza - US Department of Agriculture Data",
    "text": "Diego Mendoza - US Department of Agriculture Data\n\nThe USDA supports agriculture, food safety, rural development, and environmental protection in the U.S., collecting data on crop production, soil quality, and food security to inform policy and guide farmers. Research from fields in North Dakota focuses on the effects of tillage and crop rotation on soil health, highlighting the benefits of reduced tillage and crop diversity. The data evaluates key soil properties like bulk density, organic carbon, and microbial biomass to assess the impact of these agricultural practices. The goal is to promote sustainable farming by understanding how different management systems affect soil quality and productivity over time.\n\nMain objective: Assess the impact of tillage practices: Compare soil properties (e.g., bulk density, organic matter, pH) across different tillage systems (e.g., T1, T2) to determine how tillage affects soil health and suitability for agriculture.",
    "crumbs": [
      "Project Scopes"
    ]
  },
  {
    "objectID": "index.html#chris-jacob-karottukoikal---national-center-for-education-statistics-data",
    "href": "index.html#chris-jacob-karottukoikal---national-center-for-education-statistics-data",
    "title": "CSC 477 - Group 4",
    "section": "Chris Jacob Karottukoikal - National Center for Education Statistics Data",
    "text": "Chris Jacob Karottukoikal - National Center for Education Statistics Data\n\nThe National Center for Education Statistics (NCES) is a key provider of data on education in the U.S., offering a vast array of longitudinal studies, surveys, and tools. The NCES collects comprehensive data that is useful for understanding student pathways, outcomes, and how factors such as socioeconomic status, race, and school characteristics impact education.\n\nThe goal of this midterm project is to analyze data from the High School Longitudinal Study of 2009 (HSLS:09) to explore how early exposure to STEM education affects future career outcomes. The project will focus on:\n\nHow students’ interest in STEM subjects during high school influences their college major and career choices.\nThe role of parents and school environments in shaping STEM aspirations.\n(Possible consideration) A comparative analysis of HSLS:09 and other studies such as ELS:2002 and NELS:88 to identify broader educational trends.\n\nThis project will aim to answer key questions regarding the role of early STEM education in fostering career readiness and bridging gaps for underrepresented groups in STEM.",
    "crumbs": [
      "Project Scopes"
    ]
  },
  {
    "objectID": "diego.html",
    "href": "diego.html",
    "title": "1  US Department of Agriculture - Diego Mendoza",
    "section": "",
    "text": "1.1 Week 1 - 08/26 ~ 08/30",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>US Department of Agriculture - Diego Mendoza</span>"
    ]
  },
  {
    "objectID": "diego.html#week-1---0826-0830",
    "href": "diego.html#week-1---0826-0830",
    "title": "1  US Department of Agriculture - Diego Mendoza",
    "section": "",
    "text": "1.1.1 Wednesday\n\n1.1.1.1 Overview\n-Work on new data set based on previous skills -observe changes on different visual graphs -Explain any patterns or observe points -Test test and test again\n\n\n1.1.1.2 Attitude\n\n** Background Information **\n\nNo context was given for the data set\n7 columns: rating, complaint, privileges, leaning, raises, critical, advance\nRating attitude\n\n\n\n1.1.1.3 Filtering\n(How to Follow) -Use Condition Formating (In Home) -Highlight cells, equalt too or less than (58.5)\n\n\n\n1.1.1.4 Observations\n-Each column has at least 2 cells =&lt;50 -Advance attitude perform poorly in an standard normal look(Bigger = Better) -Critical attitude excelling in an standard normal look(Bigger = Better) -\n\n\n1.1.1.5 Filtering + Formulas\n(How to Follow) -Select group of Cells, Rows or Columns -Use Condition Formating (In Home) -Highlight cells, duplicate\n-Select group of Cells, Rows or Columns -Select Formulas -Use Average, Ma, Min\n\n\n\n1.1.1.6 Observations\n\n11 unique values that do not overlap/duplicated in another cell\nHighest value was Critical with 92, Complaint had the 2nd highest with 90, -&gt; However that value appears to an outliar\n\n-Minimum belongs to advance with 25 with privilege 2nd lowest with 30. -Critical with the highest average with 74.77 complaint 2nd highest with 66.59\n\n\n1.1.1.7 Graphs\n(How to Follow) -Select group of Cells, Rows or Columns -In the setting insert, select graph or recommend graph\n\n\n\n1.1.1.8 Observations\n\nBar graphs and pie graphs are not ver useful and do not provide any sort of information or trends\n\n-The scatter plot is useful seeing where the cluster is and having a visual provided of where those values are in relation to each other -&gt;Trends Complaints and learning has a postie correlation in related to rating. The higher the rating the higher the complaint attitude\n-Statics chart helpful in showing where the concentration of the overall data is.\n-Best graph that represent the data was Scatter plot, -&gt;Easier to individual points and groups. -&gt;Can apply basic functions, or understanding to see trends -&gt; Overall seem that those with Combative attitudes excelled, those who were humble seemed to a had a positive trend and privileged posh attitudes performed worse than counterparts.\n\n\n1.1.1.9 Summary\n-Applied skills learned from Monday -Found trends patterns and made conclusions -Researched possible data sets and other techniques\n\n\n\n1.1.2 Friday\n\n1.1.2.1 Overview\n-Cleaning and filtering data in relation to soil samples -Applying techniques learned from Monday and Wednesday to data set -Document all necessary actions for repeatable research -Find a story in the data\n\n\n1.1.2.2 Data Information\n-Data is from the USDA -Multiple different data sets that could be chosen -&gt;Friday’s focuses on solid yield from 1984 - 1993 -Specific data can be found pressing the link Data from Tillage and cropping effects on soil quality indicators in the northern Great Plains\n-The study provides insights into how soil properties respond to crop rotation and tillage practices under rainfed conditions in a semiarid continental climate\n\n1.1.2.2.1 Column Name and Information\nPLOT: Plot number, identifying the specific plot where the data was collected. REP: Replicate number, indicating the repetition of the experiment or sampling within the plot. ROTATION: Crop rotation system used in the experiment, typically represented by a code. -TILLAGE: Tillage treatment, also represented by a code (e.g., T1, T2). -DEPTH: Soil sampling depth (in cm). -SBD: Soil bulk density (g/cm³). -EC: Electrical conductivity (dS/m), a measure of soil salinity. -PH: Soil pH level. -NO3N: Nitrate nitrogen (mg/kg), an indicator of soil nitrogen content. -SOC: Soil organic carbon (mg/kg). -TN: Total nitrogen (mg/kg). -PMN: Potentially mineralizable nitrogen (mg/kg). -POMLF: Particulate organic matter light fraction (mg/kg). -POMSF: Particulate organic matter small fraction (mg/kg). -POMT: Total particulate organic matter (mg/kg). -POMSOM: Particulate organic matter as a percentage of soil organic matter. -MBC: Microbial biomass carbon (mg/kg). -MBN: Microbial biomass nitrogen (mg/kg)\n\n\n\n1.1.2.3 Intial Observation\n(How to Follow) -Select group of Cells, Rows or Columns -Select column with depth,(Step1) -select all 7.5cm (Step2) -shift arrow -&gt; 4 rows (Step3) -In the setting insert, select graph or recommend graph(Step4) -For this we used a Column cluster(reccomended)\n\n1.1.2.3.1 7.5 cm depth\n\n\n\n1.1.2.3.2 15 cm depth\n-Repeats steps, changing 7.5 to 15\n\n\n\n1.1.2.3.3 30 cm depth\n-Repeats steps, changing 7.5 to 30\n\n\n\n\n1.1.2.4 observations\n-On Graphs 2 & 3 seem to lacking a series 4 and both have high series 7 -On Graph 1 lacks series 5-7 and series\n\n\n1.1.2.5 Focus + 2nd graph\n-Focused on 2 columns ph and NO3N -&gt;PH level and nitrogen.\n-Highlighted each PH and NO3N for each respective depth -Selected graphs for each a line graph\n\n\n1.1.2.5.1 Observations\n-Depth seems to correlate to steadier levels in PH and N03N to 15cm -15cm depth seems to the best to hold consistent level in PH and N03N -7.5cm depth has the largest variance highest Max in NO3n -30cm depth had the lowest valley in NO3N\n\n\n\n1.1.2.6 Formatting\n-Highlight cells rule, less than value (6.25) -Highlighted each PH and NO3N for each respective depth\n ##### Observations\n-30 + 15 cm depths’ nitrogen is below 6.25 cm -7.5cm depth’s PH is below 6.25\n\n\n1.1.2.7 Summary\n-Worked with team lead and Professor at data sets and possible ideas -Discussed what expectations were set and what documentation should look like -Worked on USDA data set -Applied skills and tools to dataset\n\n\n1.1.2.8 Notes for next work\n-Larger and more background information on a dataset is useful -Lacked comparsion test to 1984 data set -Extended research into subject provides a working platform -Large data sets become complicated to work with and see at once on excel",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>US Department of Agriculture - Diego Mendoza</span>"
    ]
  },
  {
    "objectID": "diego.html#week-2---0902-0906",
    "href": "diego.html#week-2---0902-0906",
    "title": "1  US Department of Agriculture - Diego Mendoza",
    "section": "1.2 Week 2 - 09/02 ~ 09/06",
    "text": "1.2 Week 2 - 09/02 ~ 09/06\n\n1.2.1 Wednesday & Friday Pivot Graph\n\n1.2.1.1 Histogram\n\nI am using the airquality data set, which is has 6 columns. Columns: Ozone,Solar.R,Wind,Temp,Month,Day Ozone: Concentration of ozone in the atmosphere, measured in parts per billion (ppb). Solar.R: Solar radiation in the atmosphere, measured in langleys (a unit of energy per unit area). Wind: Wind speed, measured in miles per hour (mph). Temp: Temperature, measured in degrees Fahrenheit (°F). Month: Month of the year, represented as an integer (e.g., 1 for January, 2 for February, etc.). Day: Day of the month, represented as an integer.\nLocation: Recorded from New York\nSpan- May-September\nI use excel to clean the data, remove all the rows which have NA values,\n\n\n1.2.1.2 Steps\n1- Select all cells 2- Apply Filter 3- Select only Rows with NA 4- Delete Rows 5- Select other Rows 6- Repeat from 3 if more NA’s\nAfter I did an exploration analysis\n\n\n1.2.1.3 Histogram\n\nRanges and proportions are similar to before cleaning\n\n\n1.2.1.4 Focused Data\n\n\n\n1.2.1.5 Graph\n\n\n\n\n\n\n1.2.1.5.1 Observations\nThe months observed are May, Jun, July, August, and September Average Solar Radiation has an Inverse Relationship with Wind Ozone has a positive correlation with Solar R\nOn closer day to day Ozone and Temp have identical trends Ozone mimics radiation but on lower scale\nInverse relationship between Solar Radiation and Wind: This could occur because higher solar radiation typically results in warmer, more stable air masses, which may reduce wind speed. Wind is often driven by differences in air pressure, and if the atmosphere becomes more uniform due to increased solar heating, wind speeds may drop. Also, high-pressure areas, which are associated with clear skies and stronger solar radiation, often have lighter winds compared to low-pressure areas.\nPositive correlation between Ozone and Solar Radiation: Ozone concentration in the lower atmosphere (troposphere) can increase with solar radiation, especially during the summer months. This is because ozone is formed through photochemical reactions involving sunlight and pollutants like nitrogen oxides and volatile organic compounds (VOCs). Therefore, higher solar radiation provides the energy needed to drive these reactions, leading to higher ozone levels.\nDay-to-day identical trends between Ozone and Temperature: Ozone levels often increase with higher temperatures. This is largely due to the fact that photochemical reactions leading to ozone formation are more active at higher temperatures. Additionally, on warmer days, there is typically more stagnant air, which can trap pollutants and increase ozone levels. Hence, the similar trends between temperature and ozone levels.\nOzone mimicking Solar Radiation but on a smaller scale: The pattern here suggests that while solar radiation directly affects ozone formation, other factors such as pollution levels, temperature, and atmospheric mixing also play significant roles in determining actual ozone concentrations. So, while the trend is similar, the scale is smaller because ozone is dependent on several additional variables beyond solar radiation alone.\nThese relationships could reflect broader atmospheric and environmental patterns, particularly related to summer months when sunlight and temperature both peak, driving many of these processes.\n\n\n\n\n1.2.2 Friday\n\n1.2.2.1 Intro 1.0\nUSDA data set Focused data sets Projection of Midterm Project\n\n\n1.2.2.2 USDA 1.1\nThe USDA (United States Department of Agriculture) is a federal agency responsible for overseeing agriculture, food safety, natural resources, rural development, and related areas in the United States. Founded in 1862, its primary mission is to support farmers, promote agricultural production, and ensure a safe and affordable food supply. The USDA works to improve the economy, provide food assistance programs, and protect the environment. It plays a key role in creating and implementing policies that impact farming, forestry, rural communities, and conservation efforts.\nThe USDA collects and records a wide range of data across various sectors, including crop production, livestock, economic and environmental statistics, and food security. They compile data on crop yields, prices, weather patterns, and soil quality, along with data related to rural development and economic conditions. In addition, the USDA monitors food safety and nutrition, conducting surveys on food consumption, food assistance programs, and the nutritional health of Americans. They also record information on natural resource usage and conservation efforts, aiming to promote sustainable agricultural practices.\nThis data is used for several purposes. It helps guide policy decisions related to agriculture and food security, ensuring that food production remains stable and resilient. The data supports research, informs farmers and agribusinesses about market trends and conditions, and helps policymakers create programs that improve rural communities and the environment. Additionally, USDA data informs trade negotiations and decisions on subsidies or tariffs for agricultural products. On a broader scale, the data is used to track climate change’s effects on agriculture and promote practices that can mitigate these effects.\n\n\n1.2.2.3 Focused Data set\nGeographic location - description Fields H1, H4, and H5 on the Area IV Soil Conservation Districts Cooperative Research Farm near Mandan, North Dakota USA. Fields H1 and H5 align with the 1984 experiment, while field H4 aligns with the 1993 experiment. *ISO Topic Category environment farming Ag Data Commons Group Long-Term Agroecosystem Research Northern Plains National Agricultural Library Thesaurus terms tillage; soil quality; Great Plains region; long term experiments; cropping sequence; soil conservation; cooperative research; farms; North Dakota; spring; grazing intensity; pastures; soil types; soil sampling; soil density; electrical conductivity; soil pH; nitrate nitrogen; soil organic carbon; nitrogen; particulate organic matter; microbial carbon; infiltration rate; stover; biomass production; laboratory techniques; combustion; crop rotation; continental climates\n\n1.2.2.3.1 Overview of Research\nThe research on tillage and cropping effects on soil quality indicators in the northern Great Plains highlights the significant role of cropping systems in enhancing soil health. Various studies indicate that diverse cropping practices, particularly those incorporating cover crops and reduced tillage, can lead to improved soil quality metrics.\nImpact of Crop Diversity Increased crop diversity, such as incorporating cover crops, enhances soil organic carbon (SOC) and microbial biomass carbon (MBC) compared to monoculture systems(Feng et al., 2020). Crop rotation systems have shown to improve soil aggregate stability and enzyme activity, which are crucial for nutrient cycling(Feng et al., 2020). Tillage Practices No-till practices have been associated with higher SOC and improved soil structure, particularly in the topsoil(Fiorini et al., 2020)(Liebig et al., 2004). Long-term studies indicate that continuous cropping with no-till significantly enhances soil nutrient availability and reduces erosion risks(Liebig et al., 2004). Microbial Community Dynamics The composition and diversity of soil bacterial communities are influenced more by cropping systems than by crop identity, suggesting that management practices are critical for soil health(Ouverson et al., 2021). While these findings underscore the benefits of diverse cropping systems and reduced tillage, it is essential to consider that not all practices yield uniform results across different soil types and climatic conditions, indicating a need for tailored approaches in agricultural management.\n\n\n\n1.2.2.4 Objectives\n\n1.2.2.4.1 Main objective\nAssess the impact of tillage practices: Compare soil properties (e.g., bulk density, organic matter, pH) across different tillage systems (e.g., T1, T2) to determine how tillage affects soil health and suitability for agriculture.\n\n\n1.2.2.4.2 Other possible research points\nAnalyze crop rotation effects: Evaluate how different crop rotation practices impact soil fertility and microbial biomass by analyzing nitrogen levels, organic carbon, and other key metrics.\nExamine soil depth influence: Investigate how soil properties change with depth (e.g., 7.5 cm) and assess whether deeper soil layers are more or less fertile.\nTrack year-by-year changes: Observe trends in soil health and fertility over time by analyzing patterns in the dataset based on yearly measurements, if time-based data is included.\nMicrobial biomass and soil fertility: Investigate how microbial biomass (carbon and nitrogen) relates to soil fertility across different management practices.\nParticulate organic matter (POM) impact: Explore the role of particulate organic matter in contributing to overall soil organic matter (POMSOM) and its effect on agricultural productivity.\nCorrelation between EC, pH, and nutrient availability: Study how soil electrical conductivity (EC) and pH affect the availability of key nutrients like nitrate nitrogen (NO3N), organic carbon, and total nitrogen.\n\n\n\n1.2.2.5 Summary\nThe USDA supports agriculture, food safety, rural development, and environmental protection in the U.S., collecting data on crop production, soil quality, and food security to inform policy and guide farmers. Research from fields in North Dakota focuses on the effects of tillage and crop rotation on soil health, highlighting the benefits of reduced tillage and crop diversity. The data evaluates key soil properties like bulk density, organic carbon, and microbial biomass to assess the impact of these agricultural practices. The goal is to promote sustainable farming by understanding how different management systems affect soil quality and productivity over time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>US Department of Agriculture - Diego Mendoza</span>"
    ]
  },
  {
    "objectID": "diego.html#week-3---0909-0913",
    "href": "diego.html#week-3---0909-0913",
    "title": "1  US Department of Agriculture - Diego Mendoza",
    "section": "1.3 Week 3 - 09/09 ~ 09/13",
    "text": "1.3 Week 3 - 09/09 ~ 09/13\n\n1.3.1 Wednesday\n                   \n\n1.3.1.1 Overview\nDocument is CO2_DataSet Working with Tableau\n\n\n1.3.1.2 Background Information\nThe CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. uptake of six plants from Quebec and six plants from Mississippi was measured at several levels of ambient concentration. Half the plants of each type were chilled overnight before the experiment was conducted.\nThis dataset was originally part of package nlme, and that has methods (including for [, as.data.frame, plot and print) for its grouped-data classes.\n\n1.3.1.2.1 Column Information\nPlant an ordered factor with levels Qn1 &lt; Qn2 &lt; Qn3 &lt; … &lt; Mc1 giving a unique identifier for each plant.\nType a factor with levels Quebec Mississippi giving the origin of the plant\nTreatment a factor with levels nonchilled chilled\nconc a numeric vector of ambient carbon dioxide concentrations (mL/L).\nuptake a numeric vector of carbon dioxide uptake rates (mol/2μmol/m 2sec).\n\n\n\n1.3.1.3 Observations\nAll chilled plants had lower uptakes than there not chilled equvilent(Sheet 1)\nIf all chilled plants in the data show lower CO2 uptakes than their non-chilled equivalents, it indicates that lower temperatures may have negatively affected the plants’ ability to absorb CO2. This is likely because chilling can slow down metabolic processes, including photosynthesis, which is crucial for CO2 uptake. Reduced enzyme activity at lower temperatures could hinder the plants’ photosynthetic efficiency, leading to less CO2 being absorbed. Additionally, chilling may cause stress in some plant species, further limiting their ability to function optimally. It would be important to explore other environmental or physiological variables in the data to confirm if the drop in CO2 uptake is consistently linked to temperature or if other factors contribute to this trend.\nIf plants had higher CO2 uptake in Quebec compared to Mississippi, this difference could be influenced by various environmental factors. Quebec, with its cooler climate, may provide more optimal conditions for photosynthesis in some plant species, particularly if they are adapted to cooler temperatures. Lower temperatures can reduce the rate of respiration, allowing more energy to be directed toward photosynthesis, which increases CO2 uptake.\nIn contrast, Mississippi’s warmer climate could lead to higher respiration rates, reducing the net CO2 uptake. Additionally, other factors like humidity, soil composition, and light exposure might vary between the regions, contributing to the observed differences. Exploring these environmental and regional variables in the dataset would help clarify the specific reasons behind the greater CO2 uptake in Quebec.\nMississipi plant had tighter cluster inregarding to measurements taken(Sheet 4)\nA tighter cluster of measurements in Mississippi plants suggests that there is less variability in CO2 uptake under the conditions tested. This could be due to more consistent environmental factors such as temperature, humidity, and light intensity in Mississippi. If the climate in Mississippi is more stable during the period of measurement, the plants may experience fewer fluctuations in growing conditions, leading to more uniform physiological responses and, thus, more consistent CO2 uptake measurements.\nAdditionally, if the Mississippi plants are of the same species or have similar adaptive traits to their local environment, this genetic or physiological uniformity could result in less variation in how they respond to environmental stimuli. In contrast, regions with more variable climates, like Quebec, might show a wider range of responses due to changing temperatures or other fluctuating environmental factors.\nThe higher variance and standard deviation in CO2 uptake measurements for plants in Quebec, compared to Mississippi, suggest that there is greater variability in how plants in Quebec respond to environmental conditions. This could indicate that the growing conditions in Quebec are less stable or more variable, leading to more fluctuations in plant performance. For instance, Quebec’s climate may experience more extreme temperature shifts, varying levels of sunlight, or differences in humidity, all of which can affect CO2 uptake differently across plant species or individual plants.(Sheet 6)\nHigher variance also implies that certain plants in Quebec are either more sensitive or more adaptive to these environmental changes, showing a broader range of responses, from low to high CO2 uptake. In contrast, Mississippi’s plants, with their tighter clustering and lower variability, are likely experiencing more consistent conditions, resulting in more uniform uptake patterns. This highlights that the environmental or biological factors affecting plant performance are more varied in Quebec, possibly due to its cooler and more fluctuating climate.\n\n\n1.3.1.4 Summary\nDifferenet reagions provide different enviroments for plants to respond\nChilled plant perform at a lower rate than nonchilled regardless of enviromnet or species\n\n\n\n1.3.2 Friday\n                   \n\n1.3.2.1 Overview\nFocused look on plant usage on mineral B and Ca Understanding what future food production prediction may look from such results\n\n1.3.2.1.1 BackGround Information\nIn a study of wheat cultivation in the northern Great Plains, researchers investigated how different crop rotations affect wheat performance and the relationship between soil and grain mineral concentrations. The study compared spring wheat (fertilized, CSW) with wheat following 5 years of perennial forages, including alfalfa, intermediate wheatgrass (fertilized, IWG), and a mixture of alfalfa and intermediate wheatgrass (MIX). The wheat grown after alfalfa showed the highest yield, superior thousand kernel weight (TKW) compared to CSW, and increased crude protein (CP) concentration relative to IWG and CSW. However, wheat following alfalfa had lower grain zinc (Zn) concentration compared to IWG.\nWheat following IWG exhibited higher concentrations of iron (Fe) and manganese (Mn) compared to the MIX treatment, higher magnesium (Mg) than CSW, and lower sulfur (S) concentration across all treatments. Multivariate correlation analysis revealed that plant available soil minerals were positively correlated with grain boron (B), Mg, Mn, and S concentrations, while they were negatively associated with grain Zn and calcium (Ca) concentrations.\nThe results indicate that incorporating perennial forage phases into wheat cropping systems can enhance wheat yield and CP levels, but it may also reduce certain soil mineral availability. This suggests that while rotating perennials can improve some aspects of soil quality, it could also affect soil fertility by lowering the availability of certain essential minerals.\n\n\n1.3.2.1.2 Soil\nIn this study, soil measurements provide insights into the availability of essential minerals that plants need for growth. Specifically, the soil measurements include:\nPlant Available Soil Minerals: These measurements assess the concentration of minerals in the soil that are accessible to plants. Key minerals analyzed in this study include boron (B), magnesium (Mg), manganese (Mn), sulfur (S), zinc (Zn), calcium (Ca), and others. These soil concentrations help determine how well nutrients are supplied to the crops.\nCorrelation with Grain Mineral Concentrations: By examining the relationship between soil mineral availability and the mineral content in wheat grains, the study investigates how variations in soil nutrients influence the nutrient profile of the harvested grain. This correlation helps to understand how soil nutrient management affects the nutritional quality of the wheat produced.\nImpact of Crop Rotation: The study explores how rotating perennial forages into wheat cropping systems affects soil mineral availability. For instance, it assesses whether different crop rotations lead to changes in the levels of plant available soil minerals and how these changes impact wheat yield and grain mineral concentrations.\nOverall, soil measurements in this study help in understanding the dynamics of soil fertility, the efficiency of nutrient uptake by crops, and the broader implications for crop performance and nutritional quality.\nIn the context of this study, soil samples were likely collected before planting to assess the plant available mineral concentrations in the soil prior to wheat cultivation. This allows researchers to establish a baseline of nutrient availability and understand how different crop rotations and management practices affect soil fertility.\nBy analyzing soil conditions before planting, the study can correlate these initial nutrient levels with subsequent wheat performance and grain mineral concentrations. This approach helps to determine how soil nutrient availability influences crop yield and grain quality throughout the growing season.\n\n1.3.2.1.2.1 Focused Context\nIn this study, Boron (B) and Calcium (Ca) were measured both in soil (plant-available form) and wheat grain to assess their correlation. Boron and Calcium are essential nutrients for plant growth, with their availability in the soil directly influencing the nutrient content of harvested wheat grain. These measurements help in understanding how different cropping systems (such as continuous wheat or rotations with perennials) affect the nutrient dynamics in both soil and plant tissue.\n\n\n1.3.2.1.2.2 Background Information on Mineral Use\nBoron (B) is vital for cell wall formation, reproductive growth, and nutrient transport within plants. Deficiency in B can lead to reduced yields and poor seed development, especially in crops like wheat. In semi-arid soils, Boron levels can fluctuate, influencing plant uptake and grain mineral content.\nCalcium (Ca) plays a critical role in strengthening cell walls, regulating enzyme activity, and supporting root and shoot development. A sufficient supply of Calcium ensures healthy plant structure and resistance to disease, but excessive or insufficient levels can lead to nutrient imbalances that affect plant health and grain quality. Soil Calcium levels are key to determining its availability to plants and eventual presence in grains.\n\n\n\n1.3.2.1.3 Observations:\nDashboard 1 Sheets 2 and 3\nGrain Ca vs Soil Ca:\nAcross all perennial treatments (Alfalfa, CSW, IWG, and MIX), the grain calcium (Ca) concentration appears to be relatively consistent, even though the soil calcium levels vary slightly by depth (0-5 cm and 5-10 cm). The variation in soil Ca levels across the treatments does not appear to have a significant effect on the grain Ca concentration, as the bars representing grain Ca are mostly uniform across the treatments. Grain B vs Soil B:\nIn contrast, the grain boron (B) concentration shows a stronger association with soil B levels, especially in the Alfalfa and CSW treatments. For example, the Alfalfa treatment (0-5 cm) shows that higher soil B concentration is reflected in higher grain B concentration, while other treatments like CSW and MIX show more moderate levels.\n\n\n1.3.2.1.4 Inferences:\nCalcium (Ca):\nThe weak correlation between soil Ca and grain Ca suggests that wheat plants might regulate Ca uptake, maintaining relatively consistent levels in grain even when soil Ca availability varies. This could imply that wheat plants prioritize Ca for grain formation, regardless of soil supply, or that Ca uptake is less responsive to soil concentration changes within this range. Boron (B):\nThere is a clearer relationship between soil B and grain B, indicating that wheat’s B uptake may be more directly influenced by soil B availability. Boron is crucial for reproductive development in plants, and its availability in the soil could directly affect the levels found in wheat grains. General Patterns in Rotation:\nRotations involving perennial forages like alfalfa appear to increase soil B availability, which translates into higher grain B concentrations compared to continuous spring wheat (CSW). This suggests that integrating perennials into cropping systems might enhance the availability of specific micronutrients such as boron, potentially influencing the nutritional quality of the grain.\n\n\n1.3.2.1.5 Observations\nSheet 1\nPotassium (K) indeed has the highest concentration (mg/kg) among all the minerals across different perennial treatments (Alfalfa, CSW, IWG, and MIX). This indicates that K is more abundant in the soil compared to other minerals like Boron (B), Aluminum (Al), Calcium (Ca), and Copper (Cu).\nCalcium (Ca) also appears in relatively high concentrations, particularly in the CSW and MIX treatments, where it shows levels close to Potassium, though slightly lower.\nBoron (B), in contrast, has much lower concentrations across all treatments, which confirms your earlier observation of lower Boron levels. The variation in Boron levels across treatments is minimal, with no significant spikes.\nAluminum (Al) and Copper (Cu) show the lowest concentrations overall, with minimal differences between the different treatments.\nAdditional Observations: CSW and MIX treatments appear to have the highest levels of K and Ca compared to IWG and Alfalfa treatments. Boron and Aluminum remain consistently low across all treatments, reinforcing the idea that these minerals might be less available in the soil or more rapidly depleted during crop growth.\n\n\n1.3.2.1.6 Observations\nSheet 4\nSoil Boron (B) Concentration:\nThe soil Boron concentrations vary across different treatments (Alfalfa, CSW, IWG, and MIX) and soil depths (0-5 cm and 5-10 cm). In the Alfalfa treatment, soil B is consistently higher, with values reaching around 0.6 mg/kg in both depth layers. The CSW treatment shows relatively lower soil Boron concentrations, mostly clustering between 0.2-0.4 mg/kg. IWG and MIX show intermediate soil Boron levels, with slightly higher concentrations at 0-5 cm compared to 5-10 cm. Grain Boron (B) Concentration:\nGrain B levels are much lower than soil B levels, with most of the values falling between 0.02 mg/kg and 0.07 mg/kg, indicating that only a small fraction of available Boron in the soil is taken up by the plants and transferred to the grain. The MIX treatment shows the highest grain B concentrations, particularly at the 0-5 cm depth. This suggests that plants under the MIX rotation were more efficient in uptaking Boron from the soil compared to other treatments. Other treatments like CSW and IWG have lower grain B levels, despite having moderate soil B concentrations, indicating a lower Boron uptake efficiency. No Measurements in Some Cells:\nCertain cells show “No Measure,” indicating gaps in the data for specific treatment and depth combinations. Inferences: Boron Uptake Efficiency: The difference between soil B and grain B suggests that Boron uptake efficiency varies across treatments. For example, even though soil B levels are high under Alfalfa, the grain B concentration remains relatively low, indicating that the plants might not be fully utilizing the available Boron.\nTreatment Effect: The MIX treatment seems to have enhanced Boron uptake, resulting in higher grain B levels despite moderate soil B concentrations, suggesting that this treatment may improve nutrient acquisition.\nDepth Influence: Soil depth plays a role in Boron availability, with slightly higher soil B concentrations in the 0-5 cm layer across most treatments. However, this does not always translate into higher grain B concentrations.\n\n\n\n1.3.2.2 Summary\nThe data provided shows the relationship between soil and grain concentrations of minerals like Boron (B) and Calcium (Ca) across different cropping systems and soil depths. In the case of Boron, soil concentrations are generally higher than the corresponding levels in the grain, indicating that although Boron is present in the soil, the plants’ uptake and transfer to the grain are limited. Among the different treatments, the MIX rotation (alfalfa and intermediate wheatgrass) appears to facilitate the highest Boron uptake efficiency, showing relatively higher grain B concentrations despite moderate soil levels. Other treatments, such as CSW (continuous spring wheat) and IWG (intermediate wheatgrass), demonstrate lower Boron uptake efficiency, with grain B concentrations remaining low even when soil Boron levels are moderate.\nFor Calcium, the relationship between soil and grain concentrations is more linear, with higher soil Ca levels correlating with higher grain Ca concentrations. This is especially evident in the CSW and MIX treatments, where soil Ca levels are high, and grain Ca uptake is relatively efficient. The consistent uptake of Ca suggests that plants have a more stable mechanism for absorbing this macronutrient, regardless of the cropping system. In contrast, the variability in Boron uptake indicates that B availability and plant uptake are more sensitive to crop type, rotation, and soil depth. This insight highlights the potential for certain perennial rotations, like the MIX treatment, to enhance micronutrient uptake efficiency, particularly for nutrients like Boron that are essential in small quantities but crucial for plant growth.\nThese findings have significant implications for future food production, especially in the context of nutrient management in soils. As the global population grows, there will be increasing pressure to optimize crop yields while maintaining or enhancing nutritional quality. The data suggests that certain cropping systems can enhance the uptake of key nutrients like Calcium, but micronutrients such as Boron may be more challenging to manage. If future agricultural systems focus on rotations with perennial forages or mixed systems, it could lead to better nutrient efficiency and soil health, making food production more sustainable. However, continuous cropping of annual cereals without proper nutrient replenishment could result in micronutrient deficiencies, impacting both crop health and the nutritional quality of food.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>US Department of Agriculture - Diego Mendoza</span>"
    ]
  },
  {
    "objectID": "diego.html#week-4---0916-0920",
    "href": "diego.html#week-4---0916-0920",
    "title": "1  US Department of Agriculture - Diego Mendoza",
    "section": "1.4 Week 4 - 09/16 ~ 09/20",
    "text": "1.4 Week 4 - 09/16 ~ 09/20\n\n1.4.1 This is a markdown title\nin markdown we can create lists: - item 1 - item 2 - item 3 - item 4\nalso we can create enumerate list 1. Hola 2. Hi 3. Namaste\nwe can do bold, also italic\n# Here we are importing numpy with a nickname np\nimport numpy as np\narr = np.array([1, 2, 3, 4, 5])\nprint(np.absolute(-1))\nprint(arr)\n1\n[1 2 3 4 5]\n# list are native to python\nmy_list = [1,2,3,4,5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n# We will using a lot of data frames, so we need pandas library\n\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\n\n1.4.1.1 4 Loading csv files\nTo load .csv files into a DataFrame. we use the pandas function read_csv:\ndf = pd.read_csv('airquality_datasets.csv')\n# Summary of the dataset\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\n\n\n1.4.1.2 5. Visualizing the dataset\nLet’s dive into visualizations using matplotlib. We’ll start with simple histograms and boxplots, then move on to correlation plots.\n\n1.4.1.2.1 Histograms\nHistograms help us understand the distribution of the variables. We’ll create histograms for Ozone and Temp.\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\nDistribution of Ozone Levels\n\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\nDistribution of Temperature",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>US Department of Agriculture - Diego Mendoza</span>"
    ]
  },
  {
    "objectID": "diego.html#week-5---0923-0927",
    "href": "diego.html#week-5---0923-0927",
    "title": "1  US Department of Agriculture - Diego Mendoza",
    "section": "1.5 Week 5 - 09/23 ~ 09/27",
    "text": "1.5 Week 5 - 09/23 ~ 09/27\n\n1.5.1 Friday\n\n\n\nTillage VS Erosion\n\n\nThis graph compares irrigation patterns against soil erosion across various soil types. The first chart shows the variation in the average values of irrigation (Avg. F11) across different soil types, with visible peaks indicating periods of higher water usage. The other two metrics (F22 and F29) show more subtle changes in factors potentially affecting erosion or soil quality. Overall, the data emphasizes the complex relationship between irrigation levels and the associated impacts on soil health and erosion under different agricultural conditions.\nThe USDA supports agriculture, food safety, rural development, and environmental protection in the U.S., collecting data on crop production, soil quality, and food security to inform policy and guide farmers. Research from fields in North Dakota focuses on the effects of tillage and crop rotation on soil health, highlighting the benefits of reduced tillage and crop diversity. The data evaluates key soil properties like bulk density, organic carbon, and microbial biomass to assess the impact of these agricultural practices. The goal is to promote sustainable farming by understanding how different management systems affect soil quality and productivity over time.\n\n1.5.1.1 Focused Data set\nGeographic location - description Fields H1, H4, and H5 on the Area IV Soil Conservation Districts Cooperative Research Farm near Mandan, North Dakota USA. Fields H1 and H5 align with the 1984 experiment, while field H4 aligns with the 1993 experiment. *ISO Topic Category environment farming Ag Data Commons Group Long-Term Agroecosystem Research Northern Plains National Agricultural Library Thesaurus terms tillage; soil quality; Great Plains region; long term experiments; cropping sequence; soil conservation; cooperative research; farms; North Dakota; spring; grazing intensity; pastures; soil types; soil sampling; soil density; electrical conductivity; soil pH; nitrate nitrogen; soil organic carbon; nitrogen; particulate organic matter; microbial carbon; infiltration rate; stover; biomass production; laboratory techniques; combustion; crop rotation; continental climates\n\n1.5.1.1.1 Overview of Research\nThe research on tillage and cropping effects on soil quality indicators in the northern Great Plains highlights the significant role of cropping systems in enhancing soil health. Various studies indicate that diverse cropping practices, particularly those incorporating cover crops and reduced tillage, can lead to improved soil quality metrics.\nImpact of Crop Diversity Increased crop diversity, such as incorporating cover crops, enhances soil organic carbon (SOC) and microbial biomass carbon (MBC) compared to monoculture systems(Feng et al., 2020). Crop rotation systems have shown to improve soil aggregate stability and enzyme activity, which are crucial for nutrient cycling(Feng et al., 2020). Tillage Practices No-till practices have been associated with higher SOC and improved soil structure, particularly in the topsoil(Fiorini et al., 2020)(Liebig et al., 2004). Long-term studies indicate that continuous cropping with no-till significantly enhances soil nutrient availability and reduces erosion risks(Liebig et al., 2004). Microbial Community Dynamics The composition and diversity of soil bacterial communities are influenced more by cropping systems than by crop identity, suggesting that management practices are critical for soil health(Ouverson et al., 2021). While these findings underscore the benefits of diverse cropping systems and reduced tillage, it is essential to consider that not all practices yield uniform results across different soil types and climatic conditions, indicating a need for tailored approaches in agricultural management.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>US Department of Agriculture - Diego Mendoza</span>"
    ]
  },
  {
    "objectID": "samuel.html",
    "href": "samuel.html",
    "title": "2  NASA Earth Data - Samuel Zelaya",
    "section": "",
    "text": "2.1 Week 1 - 08/26 ~ 08/30",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NASA Earth Data - Samuel Zelaya</span>"
    ]
  },
  {
    "objectID": "samuel.html#week-1---0826-0830",
    "href": "samuel.html#week-1---0826-0830",
    "title": "2  NASA Earth Data - Samuel Zelaya",
    "section": "",
    "text": "2.1.1 Wednesday\n\n2.1.1.1 Recap and Quarto Document Guidance\nActivity: The class began with a more in-depth explanation of Quarto documents for those unfamiliar with the process.\nTopics Covered:\n1.Naming and building Quarto notes.\n2.Professor V. used a student example to create a small Excel sheet based on the student’s daily food intake and corresponding calorie count.\n3.He built a graph of the food items and their calorie counts, then demonstrated how to insert the graph into a Quarto document.\n4.Provided guidance on how to log the information effectively within Quarto to meet his expectations.\n\n\n2.1.1.2 Practical Application: Excel Data Visualization\nActivity: Continued from Monday’s class.\n1.Task: Repeated the data cleaning and visualization process with a different dataset.\n2.Dataset Used: Economics dataset.\n3.Data Preparation: - Checked for NA values or empty cells (none were found). - Sorted the data in ascending order based on date values. - Focused on the relationship between psavert (personal savings rate) and uempmed (median duration of unemployment).\n\n\n2.1.1.3 Data Visualization Process\n1.Graphs Created: I Created a line , bar graphs and scatter plot, to find which ne was the easiest to interpreted to explore the relationship between psavert and uempmed from different perspectives.\n2.Final Choice: I selected scatter plot graph, the line bar graph i found it unclear how to interpret the scatter plot provided a clearer and easier-to-read visualization of the data.\n3.Additional Observations:\nThere are a few points where the personal saving rate is relatively low. This might indicate special circumstances or different economic conditions during those periods. The two variables don’t seem to have a strong linear relationship. The points are fairly scattered, indicating that changes in the personal saving rate don’t consistently correlate with changes in the median duration of unemployment.\n\n\n2.1.1.4 Key Takeaways from Professor V.\nUnderstanding Context:\n\nIt’s crucial to understand the context of your dataset before deciding on the type of visualization.\nOnce the dataset is understood, choose visualizations that effectively compare relationships between columns (e.g., petal length vs. petal width in another example; in this case, psavert vs. uempmed).\n\n\n\n\n\n\n\n2.1.2 Friday\n\n2.1.2.1 Class Activity\nToday, we worked in class with our groups, focusing on our individual datasets. I chose to work with NASA Earth Data, a resource I’ll be utilizing throughout the semester.\n\n\n2.1.2.2 Dataset Exploration\nI began by exploring the NASA Earth Data portal, which offers a broad range of datasets across various topics like the atmosphere, earth quality, biosphere, human dimensions, and sun-earth interactions. Each main topic is further divided into subtopics, where users can download data in formats like CSV and Excel.\nThe portal also features a “Learn” section that provides tutorials and guides on how to read and use the data particularly useful for those new to the platform.\n\n\n2.1.2.3 Dataset Selection\nI was particularly interested in the “Human Dimensions” topic, which explores how humans interact with Earth’s resources. After filtering the available datasets by format (CSV and Excel) and sorting by the oldest end date, I selected the dataset titled Effects of Climate Change on Global Food Production from SRES Emissions and Socioeconomic Scenarios.\nThis dataset caught my attention because global food production has always been of interest to me, especially considering the drastic changes in soil quality and agricultural methods over the years. Additionally, the way human activities have affected the soil and food production varies significantly by country.\n\n\n2.1.2.4 Dataset Overview\nClimate Change and Global Food Production\nSummary:\nThe agricultural sector is facing significant challenges due to population growth, land degradation, and urbanization, all of which threaten global food production. Climate change is expected to intensiy these challenges, particularly in regions vulnerable to drought and famine.\nA NASA study used crop modeling to assess the impacts of climate change on food production. The study emphasizes that water availability and temperature are critical factors affecting crop yields. It also considers the effects of CO2 and suggests that climate change could have a significant impact on global food production, prices, and the risk of hunger.\n\n\n2.1.2.5 Data Analysis\n\nI downloaded the dataset and opened it in Excel to begin the data cleaning process. The dataset covers average crop production for various countries from 2000-2006. For this analysis, I focused on the variables: country code, wheat, rice, maize, and added a new variable for the year. I first calculated the mean, maximum, and minimum production values for each country for wheat, rice, and maize. China had the highest production values across all three variables—wheat, rice, and maize. On the other hand, Venezuela had the lowest production for wheat, Saudi Arabia for rice, and Japan for maize. To visualize this, I created a 100% stacked column chart. The graph shows that maize is the most produced crop across all countries, while wheat and rice exhibit irregular consistency over the years from 2000 to 2006. \nNext, I created a pie chart to confirm my hypothesis. The chart reveals that maize has the highest average production among the three crops, suggesting it might be more widely cultivated or more important in global agriculture compared to wheat and rice. Rice also has a relatively high average production, indicating its global importance as a staple food. Wheat, despite having the lowest average production among the three, still shows a substantial amount.\n. Additionally, I created another stacked column chart to view the crop production across all 74 countries. This broader analysis revealed that China has the highest production numbers, while Switzerland has the lowest. I calculated the average production for each crop for each country and determined the maximum and minimum average production across the three crops for each country. The results show Switzerland with the lowest average production overall, and India with the second highest average production, as reflected in the graph.\nIn the scatter plot, I observed a strong relationship between the production of wheat and rice. China and India were identified as clear outliers with extremely high production values for all crops. This plot helps in visualizing the tendencies and relationships between the production levels of different crops across countries.\nThen created a line graph to analyze production trends over time. Although China and India have the highest production points, the chart reveals a lack of consistency in their production levels. In contrast, wheat and maize, while showing lower production values, exhibit more consistent levels across countries. Nations with consistently low production lines may face agricultural challenges or have less focus on these crops.\n\n\n\n2.1.2.6 Summarize insights\n\nChina (CHN) leads in production with the highest values across all crops—wheat, rice, and maize—showing significant output in each category.\nSwitzerland (SW) exhibits the lowest production values across all crops, indicating minimal agricultural output.\nChina has the highest average production per country, reflecting its extensive agricultural operations and capacity.\nCountries with low average production, such as Switzerland and Swaziland, highlight their limited agricultural activities and output.\nMaize has the highest maximum production values across several countries, underscoring its significant role in global agriculture.\nRice shows high production values in several countries, including China and India, emphasizing its importance as a staple food.\nWheat production is relatively high in several countries but is not as dominant as maize or rice.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NASA Earth Data - Samuel Zelaya</span>"
    ]
  },
  {
    "objectID": "samuel.html#week-2---0902-0906",
    "href": "samuel.html#week-2---0902-0906",
    "title": "2  NASA Earth Data - Samuel Zelaya",
    "section": "2.2 Week 2 - 09/02 ~ 09/06",
    "text": "2.2 Week 2 - 09/02 ~ 09/06\n\n2.2.1 Wednesday\n\n2.2.1.1 New York Air Quality Dataset\n\n2.2.1.1.1 Description\nThe dataset contains daily air quality measurements in New York from May to September 1973. It focuses on six key variables related to atmospheric conditions: ozone levels, solar radiation, wind speed, temperature, month, and day. The data helps analyze the relationship between these variables and environmental factors.\n\n\n2.2.1.1.2 Variables\n\nOzone (ppb): Measured between 1:00 PM and 3:00 PM at Roosevelt Island.\nSolar.R (Langleys): Solar radiation measured from 8:00 AM to 12:00 PM at Central Park.\nWind (mph): Average wind speed at LaGuardia Airport, recorded between 7:00 AM and 10:00 AM.\nTemp (°F): Maximum daily temperature at LaGuardia Airport.\nMonth: Numeric (1 = May, 9 = September).\nDay: Day of the month.\n\n\n\n2.2.1.1.3 Source\nData was provided by the New York State Department of Conservation (ozone) and the National Weather Service (meteorological data).\n\n\n\n2.2.1.2 Dataset and Data Cleaning\n\nWe revisited the airquality_datasets.csv and created pivot tables.\nI Cleaned the data by creating filters and deleting NA values and empty cells, similar to what was done in the first week.\n\nIntroduction to Histograms Professor introduced histograms and their functions, showing their simplicity and effectiveness in presenting data. Exploratory Analysis: Ozone Levels Histogram: The histogram illustrates the variability in ozone levels over time. Key Insights: High ozone levels are notable, but moderate and low levels are more prevalent. Elevated levels are less frequent, while moderate and low concentrations are common.\n\n\n\n2.2.1.3 Saving Files and Folder Organization\nWe learned how to save files properly for each week to ensure Quarto documents render correctly. Instructions: Save everything in the same folder for efficient document rendering.\nOzone vs. Temperature: Scatter Plot Analysis The professor demonstrated a scatter plot comparing ozone levels and temperature. Key Insights: Positive correlation: Higher ozone levels generally occur with higher temperatures. Extreme ozone levels (115, 135, 168) appear with temperatures ranging from 79 to 97. Moderate ozone levels occur between temperatures of 70s to mid-80s. Lower ozone levels align with cooler temperatures. Overall, the scatter plot shows a clear trend of higher ozone levels being linked to warmer temperatures but with significant variability.\n\n\n\n2.2.1.4 Pivot Tables and Charts (Real Class Topic)\nPivot Table Basics:\n\nI Inserted a pivot table on a new sheet.\nSelected variables from the air quality dataset are to be inputted into rows and values. Month will go in row and temp , wind, solar r, ozone in values\nThe professor explained the count, sum, and average options for data analysis.\nFinally, in class, we were asked to create our own version of a pivot table to understand, analyze, and interpret the data.\n\nKey Insights: Higher temperatures and increased solar radiation are linked to higher ozone levels. As heat and sunlight rise, ozone levels generally increase, reflecting their strong correlation. Cooler temperatures and less sunlight lead to lower ozone concentrations, showing that both factors significantly influence ozone levels.\n\n\n\n\n\n2.2.2 Friday 09/07/24\n\n2.2.2.1 Work in class\n\ntoday the professor wanted us to go back into the creation of pivot tables,but now we had to make charts from the pivot tables\nwe kept on using on the same data set from from airquality and now we had to come up with as much as possible pivot table.\n\n\n2.2.2.1.1 Pivot Table 1: Comparative Analysis of Ozone, Solar Radiation, Wind, and Temperature by Month\n\nI did A “Comparative Analysis of Ozone, Solar Radiation, Wind, and Temperature by Month” using a clustered column chart\n\n\n2.2.2.1.1.1 Key Insights:\n1. Temperature: Highest in row 9 (2230) and lowest in row 6 (704), likely reflecting seasonal or geographic factors.\n2.Wind: Highest wind speed in row 9 (292.2). Row 7 shows lower wind (221.6) despite high temperatures, suggesting terrain or local weather effects.\n3 Solar Radiation: High solar radiation corresponds with high temperatures in rows 7 (5627) and 9 (4878). Lower radiation in row 8 (3981) could be due to cloud cover or atmospheric conditions.\n4. Ozone: Ozone peaks in row 7 (1537), with no clear link to temperature, suggesting factors like pollution or altitude may influence levels.\n5.Wind:Wind generally increases with temperature (row 9: 292.2), but in row 7, lower wind (221.6) despite high temperatures suggests local factors like terrain or weather conditions affecting wind patterns.\n6.Solar Radiation:High solar radiation aligns with higher temperatures (row 7: 5627, row 9: 4878), though row 8’s lower radiation (3981) may be due to cloud cover or atmospheric conditions.\n7.Ozone: Ozone peaks in row 7 (1537), but there’s no clear link to temperature, suggesting factors like pollution, altitude, or industrial activity might influence ozone levels.\n\n\n\n\n\n2.2.2.1.2 Pivot Table 2: Minimum Levels of Ozone, Solar Radiation, Wind, and Temperature by Month\n1.Solar Radiation: The lowest solar radiation is in row 7 (7), indicating minimal sunlight exposure, possibly due to increased cloud cover, shorter daylight hours, or winter months. Row 6 (37) and row 8 (24) are higher, suggesting more sunlight in those periods.\n2.Temperature:Row 5 has the lowest temperature (57), possibly reflecting a colder season or location. Row 7 (73) represents a slightly warmer period but still marks the minimum temperature in that context.\n3.Wind: The lowest wind speed is found in row 8 (2.3), likely due to calm conditions, while row 6 (8) shows stronger winds, possibly due to more open areas or seasonal wind patterns.\nThese findings point to cooler, calmer, and less sunny conditions in specific months or locations, affected by seasonal or environmental factors.\n\n\n\n\n2.2.2.1.3 Pivot Table 3: Maximum Levels of Ozone, Solar Radiation, Wind, and Temperature by Month\n1. Solar Radiation: The highest solar radiation is in row 5 (334), followed closely by row 6 (323). This suggests that these months or locations receive the most sunlight, likely due to clearer skies or longer daylight hours. Temperature:\n2.Temperature: is in row 8 (97), reflecting a very warm period or location. Rows 7 (92) and 9 (93) also show high temperatures, indicating consistently hot conditions during these months. Wind:\n3.Wind: is in row 6 (20.7), which may indicate exposure to more intense weather conditions or open areas. Row 5 (20.1) also shows similarly high wind levels, while other rows exhibit slightly lower values. These findings suggest that rows 5 and 6 experience peak solar radiation and wind, while row 8 stands out for its extreme temperature, reflecting the influence of seasonal or geographical factors on these maximum values.\n\n\n\n\n2.2.2.1.4 Pivot Table 4: Average Levels of Ozone, Solar Radiation, Wind, and Temperature by Month\n1,Solar Radiation: The highest average solar radiation is in row 7 (216.4), indicating that this month or location experiences the most consistent sunlight. Row 6 (184.2) and row 5 (182.0) also show relatively high averages, suggesting good sunlight exposure. Row 9 (168.2) and row 8 (173.1) have lower averages, possibly due to cloudier or less sunny periods. Temperature:\n2.Temperature The highest average temperature is in row 7 (83.9), reflecting warmer conditions. Row 8 (83.7) also has a high average temperature, indicating similar warm conditions. Row 6 (78.2) and row 9 (76.9) are cooler, and row 5 (66.5) is the lowest, suggesting cooler temperatures during this period. Wind:\n3. Wind The highest average wind speed is in row 6 (12.2), indicating stronger winds in that period. Row 5 (11.5) also shows relatively high wind speeds, while rows 7 (8.5), 8 (8.9), and 9 (10.1) have lower averages, suggesting calmer conditions.\n\n\n\n\n2.2.2.1.5 Pivot Table 5: Sum of Ozone, Solar Radiation, Wind, and Temperature by Month\n1.Temperature: The highest sum of temperature is in row 9 (2230), followed by row 7 (2181). This indicates warmer conditions in these periods. The lowest sum is in row 6 (704), showing cooler conditions. Ozone:\n2.Ozone Row 7 has the highest total ozone (1537), suggesting higher ozone levels during this period. Row 5 has the lowest total ozone (579), indicating reduced ozone levels. Solar Radiation:\n3.Solar R The highest sum of solar radiation is in row 7 (5627), suggesting the most sunlight exposure. Row 6 has the lowest sum (1658), indicating less solar radiation. Wind:\n4.Wind The highest total wind is in row 9 (292.2), showing the most wind activity. Row 6 has the lowest total wind (109.6), reflecting calmer conditions. Trends:\nTemperature and solar radiation generally increase over time from row 6 to row 9, indicating a trend towards warmer and sunnier conditions. Wind shows variability, with the highest value in row 9 and lower values in other rows, suggesting fluctuating wind patterns. Ozone levels are highest in row 7 and lower in row 5, indicating varying ozone concentrations over time.\n\n\n\n\n\n2.2.2.2 Last Week 1 Dataset Overview\nClimate Change and Global Food Production\nYou can download the data used in this analysis from the following link: https://www.earthdata.nasa.gov/\nStudy Overview\nThe study titled Effects of Climate Change on Global Food Production under SRES Emissions and Socio-Economic Scenarios is authored by Ana Iglesias from the Universidad Politécnica de Madrid and Cynthia Rosenzweig from NASA’s Goddard Institute for Space Studies. Published by NASA’s Socioeconomic Data and Applications Center (SEDAC) and managed by CIESIN at Columbia University in March 2010, this research delves into the impact of climate change on global staple crop production, focusing on wheat, rice, and maize.\nMain Focus\nThe primary objective of this study is to assess how climate change might influence the production of staple crops on a global scale. By utilizing crop models and climate scenarios, the study simulates potential yield changes resulting from various environmental and socio-economic factors. It particularly highlights the risks posed by global warming on food security, with a focus on regions prone to drought and famine.\nKey Variables\nThe dataset includes several key variables the only used right now where:\n1. BLS_2_Countries_(SRES)_ABBREVNAME: This represents the name of the country (e.g., Australia).\n2.Fips_code: The country code (e.g., AS for Australia)\n3.WH_2000: Average wheat production from 2000 to 2006 in metric tons, sourced from the FAO.\n4.RI_2000: Average rice production from 2000 to 2006 in metric tons, sourced from the FAO.\n5.MZ_2000: Average maize production from 2000 to 2006 in metric tons, sourced from the FAO\nThe purpose of this research is to evaluate the impacts of climate change on crop yields, specifically in relation to changes in temperature and precipitation. The study aims to identify potential adaptations and variations in yields across different countries and regions. It provides valuable insights into global food supply, price fluctuations, and the risks of hunger exacerbated by climate changes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NASA Earth Data - Samuel Zelaya</span>"
    ]
  },
  {
    "objectID": "samuel.html#week-3---0909-0913",
    "href": "samuel.html#week-3---0909-0913",
    "title": "2  NASA Earth Data - Samuel Zelaya",
    "section": "2.3 Week 3 - 09/09 ~ 09/13",
    "text": "2.3 Week 3 - 09/09 ~ 09/13\n\n2.3.1 Wednesday 9/11/24\nHere’s a review of Monday: 1. We were introduced to Tableau and spent time familiarizing ourselves with the platform. 2. Tableau Public is a free data visualization tool that allows users to create interactive and shareable dashboards. It is widely used due to its ability to handle large datasets, provide rich visualizations, and perform data cleaning and transformation tasks. This tutorial covered the basics of Tableau Public, including its main features, advantages over Excel, and step-by-step instructions on importing, cleaning, and visualizing data, as well as creating pivot tables and building dashboards using a sample dataset. 3. We created accounts to log in to Tableau Public. 4. Our professor gave us a guided tour of the website, showing how to start working with datasets and create simple visualizations. We also learned how to export datasets from Excel to Tableau.\nHere’s Wednesday’s work:\n\nWe continued working with Tableau, diving deeper into its functionality.\nI created my first visualization using the Orange_Dataset, focusing on the relationship between tree age and trunk circumference.\nThis visualization was relatively simple, showcasing the difference and relationship between these variables.\nWe posted our graphs in a discussion thread to compare progress and share insights with classmates.\n\nAverage of age and circumference :\nhttps://public.tableau.com/app/profile/samuel.zelaya/viz/averageofageandcircumference/Sheet1\n                   \n\n\n2.3.2 Friday 9/13/24\n\n2.3.2.1 Data Preview\nThis dataset provides an assessment of potential climate change impacts on global staple crop production, focusing on wheat, rice, and maize. The dataset was developed to offer quantitative estimates of crop yield changes under different climate scenarios, including the impacts of temperature and precipitation changes on crop yields. It also incorporates the physiological effects of rising carbon dioxide levels on plant growth, and adaptation measures are considered in evaluating potential yields.\n\n\n2.3.2.2 Data Categories:Production Data (2000-2006):\nWheat (WH_2000)\nRice (RI_2000)\nMaize (MZ_2000)\n\n\n2.3.2.3 Climate Scenario Yield Changes (2020, SRES A1FI):\nWheat (WHA1F2020): Projected percentage change in wheat yield.\nRice (RIA1F2020): Projected percentage change in rice yield.\nMaize (MZA1F2020): Projected percentage change in maize yield.\n\n\n2.3.2.4 Total Production Changes:\nWheat (ActChWHA1F2020): Change in total wheat production under the SRES A1FI 2020 scenario.\nRice (ActChRIAIF2020): Change in total rice production under the SRES A1FI 2020 scenario.\nMaize (ActChMZA1F2020): Change in total maize production under the SRES A1FI 2020 scenario.\n\n\n2.3.2.5 Actual Work\n\nToday, we worked with NASA Earth Data as our primary dataset.\nI cleaned the data and focused on the same variables as my previous homework: wheat, maize, and rice production levels. I also added three new important variables\nThe change in crop growth for wheat, maize, and rice from the year 2000 to 2020.\nThe projected changes in yield for these crops in 2020 due to climate change, using the A1F scenario.\nCO2 levels and their impact on the change in crop yields from 1990 to 2020.\nKey variables included:\nWH_2000, RI_2000, MZ_2000 – Production levels of wheat, rice, and maize in 2000.\nWHA1F2020, RIA1F2020, MZA1F2020 – Projected percentage change in yield for these crops in 2020 under the A1F climate scenario.\nActChWHA1F2020, ActChRIA1F2020, ActChMZA1F2020 – Actual changes in yield for these crops in 2020 under the A1F scenario.\nCO2 levels and dY% – CO2 levels and percentage change in yield (dY%) for wheat, rice, and maize under different scenarios like A1FI, A2, B1, and B2.\nI cleaned the data in Excel, highlighting the values that showed a decrease in red and those that showed an increase in green.\nAfter cleaning, I imported the file into Tableau and started creating graphs to visualize the changes and trends.\n\n\n\n2.3.2.6 Links to Visualization\nOverall Crop Production - Sheet 1\nCrop Production Per Country - Sheet 2\nCrop Production Forecast for 2020 per Country - Sheet 3\nSum Of Actual Change 2000-2020 Per Country - Sheet 4\nCrop Production Forecast Relationship - Sheet 5\nComparison between 2000 Production and 2020 Forecast Production - Sheet 6",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NASA Earth Data - Samuel Zelaya</span>"
    ]
  },
  {
    "objectID": "samuel.html#week-4---0916-0920",
    "href": "samuel.html#week-4---0916-0920",
    "title": "2  NASA Earth Data - Samuel Zelaya",
    "section": "2.4 Week 4 - 09/16 ~ 09/20",
    "text": "2.4 Week 4 - 09/16 ~ 09/20\n\n2.4.1 This is a markdown\nin a markdown, we can create lists \n\n-item 1 \n- item2 \n-item 3\n\nalso we can create enumae,rsted lists \n1. hola \n2. hello \n3. namaste \n\n\n2.4.2 Here we are importing numpy with a nickname np\nprint (np.absolute(-1))\nimport numpy as np\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n1,2,3,4,5\n\n\n2.4.3 List are native to python\nmy_list = [1, 2, 3, 4, 5]\nprint( my_list)\n[1, 2, 3, 4, 5]\n\n\n2.4.4 We will be using a lot of data frames , so we need pandas library\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)  \nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\n\n2.4.4.1 Load CSV files\nto load a.csv file into a Data Frame, use the pandas function read_.csv\ndf = pd.read_csv('airquality_datasets.csv')\nprint(df.info())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n\n\n2.4.4.2 Visualizing data set\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\nDistribution of Ozone Levels\n\n\n\n\n\nDistribution of Temperature\n\n\n#box plot \n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n\n\n\nBoxplot of Ozone Levels\n\n\n\n\n\nBoxplot of Temperature",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NASA Earth Data - Samuel Zelaya</span>"
    ]
  },
  {
    "objectID": "samuel.html#week-5---0923-0927",
    "href": "samuel.html#week-5---0923-0927",
    "title": "2  NASA Earth Data - Samuel Zelaya",
    "section": "2.5 Week 5 - 09/23 ~ 09/27",
    "text": "2.5 Week 5 - 09/23 ~ 09/27\n\n2.5.1 Wednesday\n\n2.5.1.1 Plotnine Tutorial: Understanding the Grammar of Graphics\n\n2.5.1.1.1 1. Introduction to Plotnine\nplotnine is a data visualization package for Python based on the Grammar of Graphics, which is a system for understanding and building plots. The grammar describes how plots are constructed by combining data, aesthetic mappings, geometric objects, and other components.\nTo begin, you’ll need to install the plotnine package if you don’t have it installed\npip install plotnine\n\n\n\n2.5.1.2 2. The Grammar of Graphics\n\n2.5.1.2.1 The Grammar of Graphics consists of the following key components:\nData: The data you want to visualize. Aesthetics (aes): How the data is mapped to visual properties, such as x and y coordinates, color, size, etc. Geometries (geom): The type of plot, like points, lines, bars, etc. Facets: Subplots based on the data. Scales: Control the mapping from data to aesthetic properties. Coordinate systems: Adjust how data is projected on the plane (Cartesian, rotations, polar, etc.). Themes: Adjust the non-data elements like background, labels, gridlines, etc.\n\n\n\n2.5.1.3 3. Creating Your First Plot\nLet’s begin by creating a simple scatter plot using the famous mtcars dataset. We’ll show how to set up the basic structure and gradually build complexity.\n# Import required libraries\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_point, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\n\nmtcars.info()\n\n\n\n# Create a basic scatter plot\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n labs(title='Scatter Plot of MPG vs Weight',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32 entries, 0 to 31\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   mpg      32 non-null     float64\n 1   cyl      32 non-null     int64  \n 2   disp     32 non-null     float64\n 3   hp       32 non-null     int64  \n 4   drat     32 non-null     float64\n 5   wt       32 non-null     float64\n 6   qsec     32 non-null     float64\n 7   vs       32 non-null     int64  \n 8   am       32 non-null     int64  \n 9   gear     32 non-null     int64  \n 10  carb     32 non-null     int64  \n 11  fast     32 non-null     int64  \n 12  cars     32 non-null     object \n 13  carname  32 non-null     object \ndtypes: float64(5), int64(7), object(2)\nmemory usage: 3.6+ KB\n\n\n\nScatter Plot\n\n\n\n\n2.5.1.4 4. Adding Aesthetic Mappings\nIn the Grammar of Graphics, aesthetics control how data points are represented visually. You can map variables to size, color, shape, and more.\n\n2.5.1.4.1 Example: Color by cyl (number of cylinders)\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\n\n\n\nMPG vs Weight by Cylinder\n\n\n\n\n\n2.5.1.5 Example: Size by horsepower (hp)\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\n\n\n\nMPG vs Weight by Cylinder and Horsepower\n\n\n\n\n2.5.1.6 5. Geometric Objects\ngeom_* specifies the type of plot. You can create scatter plots, line charts, bar plots, histograms, etc.\nfrom plotnine import geom_smooth\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\nMPG vs Weight with Regression Line\n\n\n\n\n2.5.1.7 6. Faceting\nFaceting allows you to split your plot into multiple panels based on a factor.\n\n2.5.1.7.1 Example: Facet by cyl\nfrom plotnine import facet_wrap\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\nMPG vs Weight Faceted\n\n\n\n\n\n2.5.1.8 7. Customizing Scales\nScales control the mapping from data to aesthetic attributes. You can customize scales for color, size, and more.\n\n2.5.1.8.1 Example: Custom Color Scale\nfrom plotnine import scale_color_manual\n\n(ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n scale_color_manual(values=['#1f77b4', '#ff7f0e', '#2ca02c']) +  # Custom colors\n labs(title='MPG vs Weight with Custom Colors',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\n\n\nMPG vs Weight with Custom Colors\n\n\n\n\n\n2.5.1.9 8. Flip Coordinates\nCreate a bar plot showing distribution of cylinders\n\n2.5.1.9.1 Example: Fliping coordinates axis\nimport pandas as pd\nfrom plotnine import  geom_bar, coord_flip, labs\n\n# Create a bar plot showing distribution of cylinders\n(ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\n\n\n\nDistribution of Cylinders\n\n\n\n\n\n2.5.1.10 9. Themes\nThemes allow you to adjust the non-data aspects of the plot, such as background, axis labels, and gridlines.\n\n2.5.1.10.1 Example: Apply a Minimal Theme\nfrom plotnine import theme_minimal\n\n(ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\n\n\nMinimal Theme\n\n\n\n\n\n2.5.1.11 10. Saving the Plot\nYou can save your plot using the save method.\n\n2.5.1.11.1 Example: Save the plot\n# Save the plot to a file\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n     geom_point() +\n     labs(title='MPG vs Weight',\n          x='Weight (1000 lbs)',\n          y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight.png\")\n\np = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\np.save(\"mpg_vs_weight_by_cylinder.png\")\n\np = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\n\np.save(\"mpg_vs_weight_by_cylinder_and_horsepower.png\")\n\n\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight_with_regression_line.png\")\n\n\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight_faceted_by_cylinder.png\")\n\np = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n scale_color_manual(values=['#1f77b4', '#ff7f0e', '#2ca02c']) +  # Custom colors\n labs(title='MPG vs Weight with Custom Colors',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\np.save(\"mpg_vs_weight_with_custom_colors.png\")\n\n\n\np = (ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\np.save(\"distribution_of_cylinders.png\")\n\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\np.save(\"MPG vs Weight with Minimal Theme.png\")\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight.png\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight_by_cylinder.png\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight_by_cylinder_and_horsepower.png\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight_with_regression_line.png\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight_faceted_by_cylinder.png\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight_with_custom_colors.png\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: distribution_of_cylinders.png\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nC:\\Users\\Samuel Zelaya\\venv477\\Lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG vs Weight with Minimal Theme.png\n\n\n\n\n2.5.2 Friday\n\n\n\nCrop Yield Changes Over Time By Scenario\n\n\nThis bar plot provides a visual comparison of crop yield changes influenced by climate change, specifically for essential staple crops such as wheat, rice, and maize. It illustrates how different scenarios and time periods impact agricultural production and compares crop yield changes across different scenarios and time periods.\n\n2.5.2.1 Dataset Overview\nClimate Change and Global Food Production\nSummary:\nThe agricultural sector is facing significant challenges due to population growth, land degradation, and urbanization, all of which threaten global food production. Climate change is expected to intensiy these challenges, particularly in regions vulnerable to drought and famine.\nA NASA study used crop modeling to assess the impacts of climate change on food production. The study emphasizes that water availability and temperature are critical factors affecting crop yields. It also considers the effects of CO2 and suggests that climate change could have a significant impact on global food production, prices, and the risk of hunger.\nYou can download the data used in this analysis from the following link: https://www.earthdata.nasa.gov/\nStudy Overview\nThe study titled Effects of Climate Change on Global Food Production under SRES Emissions and Socio-Economic Scenarios is authored by Ana Iglesias from the Universidad Politécnica de Madrid and Cynthia Rosenzweig from NASA’s Goddard Institute for Space Studies. Published by NASA’s Socioeconomic Data and Applications Center (SEDAC) and managed by CIESIN at Columbia University in March 2010, this research delves into the impact of climate change on global staple crop production, focusing on wheat, rice, and maize.\nMain Focus\nThe primary objective of this study is to assess how climate change might influence the production of staple crops on a global scale. By utilizing crop models and climate scenarios, the study simulates potential yield changes resulting from various environmental and socio-economic factors. It particularly highlights the risks posed by global warming on food security, with a focus on regions prone to drought and famine.\nKey Variables\nThe dataset includes several key variables the only used right now where:\n1. BLS_2_Countries_(SRES)_ABBREVNAME: This represents the name of the country (e.g., Australia).\n2.Fips_code: The country code (e.g., AS for Australia)\n3.WH_2000: Average wheat production from 2000 to 2006 in metric tons, sourced from the FAO.\n4.RI_2000: Average rice production from 2000 to 2006 in metric tons, sourced from the FAO.\n5.MZ_2000: Average maize production from 2000 to 2006 in metric tons, sourced from the FAO\nThe purpose of this research is to evaluate the impacts of climate change on crop yields, specifically in relation to changes in temperature and precipitation. The study aims to identify potential adaptations and variations in yields across different countries and regions. It provides valuable insights into global food supply, price fluctuations, and the risks of hunger exacerbated by climate changes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NASA Earth Data - Samuel Zelaya</span>"
    ]
  },
  {
    "objectID": "janice.html",
    "href": "janice.html",
    "title": "3  Federal Reserve Economic Data - Janice Oenardi",
    "section": "",
    "text": "3.1 Week 1 - 08/26 ~ 08/30",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Federal Reserve Economic Data - Janice Oenardi</span>"
    ]
  },
  {
    "objectID": "janice.html#week-1---0826-0830",
    "href": "janice.html#week-1---0826-0830",
    "title": "3  Federal Reserve Economic Data - Janice Oenardi",
    "section": "",
    "text": "3.1.1 Wednesday\n\n3.1.1.1 Cleaning the dataset\nBefore processing and creating visualizations of the dataset, it is important to prep the dataset by cleaning it. There might be N/A values or outliers in the dataset, which needs to be removed and cleaned to create better and more accurate visualizations.\nTo clean the data, there are a few things that can be done: - Finding and highlighting NA values by selecting the range of cells and doing ‘Conditional Formatting’ to ‘Highlight Cells Rules’. - Removing NA values by using the ‘Find and Replace’. Find ‘NA’ and replace it with ‘0’. - Using IF function to check for NA values and replace it accordingly. Ex: =IF(A2=“NA”, “0”, A2)\nAdditionally, upon converting the Swiss dataset from a .csv format into an Excel, the first column of the dataset became combined into one cell.\n\nI fixed the titles that are separated by commas in the one cell by separating the titles into the different respective columns.\n\n\n\n3.1.1.2 Context of dataset\nBefore creating plots and visualizations of the dataset, it is important to know what each title in the columns mead and understand the context of the dataset.\nhttps://stat.ethz.ch/R-manual/R-patched/library/datasets/html/swiss.html\nThe dataset contains 47 French-speaking “provinces” in Switzerland at around 1888. During that year, Switzerland was entering a demographic transition, where its fertility rates was beginning to fall from the typical level of underdeveloped countries. There are 6 variables in the dataset, where variables are scaled from 0 to 100, except Cathloic that is scaled from 0 to 1. The definitions of the dataset are as below:\n\nFertility = common standardized fertility measure\nAgriculture = % of males involved in agriculture as occupation\nExamination = % draftees receiving highest mark on army examination\nEducation = % education beyond primary school for draftees.\nCatholic = % ‘catholic’ (as opposed to ‘protestant’).\nInfant.Mortality = live births who live less than 1 year.\n\n\n\n3.1.1.3 Creating Different Basic Visualizations\n\n3.1.1.3.1 Scatterplot 1 - Fertility and Cathlic appear to cluster in two groups with four outliers\nIn creating plots, I utilized Excel’s tool of Recommended Charts, which included a scatterplot of ‘Fertility’ and ‘Catholic’. The scatterplot shown below shows two groups of cluster with four outliers.\n\nAs shown in the scatterplot, provinces with higher percentage of Catholic people seem to have a slightly higher fertility compared to provinces with low percentage of Catholic people.\n\n\n3.1.1.3.2 Scatterplot 2 - Relationship between infant mortality and percentage of Catholic people in a province\n Based on the scatterplot above, infant mortality and Catholic appear to have no apparent relationship. However, provinces with higher percentage of Catholic population appear to has a slightly higher infant mortality, with a few provinces having 25 and more live births in a year. Meanwhile, the provinces with a low percentage of Catholic population appear to have infant mortality mostly in the 15-25 range.\nAside from that, the provinces in Swiss appear to have either a very high percentage of Catholic population or a very low percentage of Catholic population, with the scatterplot showing two clusters on the lower end of Catholic percentage (0-20%) and a higher end of Catholic percentage (80-100%). What do you observe?\n\n\n3.1.1.3.3 Bar Charts\n As mentioned in the discussion above, the provinces in Swiss appear to have either a very high percentage of Catholic population, or a very low percentage of Catholic population. The bar chart shows a high frequency within the 2.15-7.15% of Catholic population that gradually decreases as the percentage approaches 17.15-22.15 and gradually increases again as it approaches the 87.15-92.15 percentage of Catholic population. The bar chart above better visualize the two groups of highly populated Catholic province or lowly populated Catholic province.\n\n\n3.1.1.3.4 Line Chart\n The line chart above clearly shows that there is a direct relationship between examination and education. As the percentage of education beyond primary school for draftees increases, the percentage of draftees receiving highest mark on army examination also increases. The two variables showcase a positive directly correlated relationship. Aside from that, the graph also shows that the 46th province outperformed all of the other provinces with the highest percentage of education beyond primary school for draftees and also the highest percentage of draftees receiving highest mark on army examination. The 46th province is followed by the 42th province which has the second highest percentage of education beyond primary school and second highest percentage of draftees receiving highest mark on army examination.\n\n\n3.1.1.3.5 Pie Charts\n The pie chart does not really visualize the agriculture data well, as there are too many percentage classification of males involved in agriculture occupation. However, the pie chart can be made better by making the classifications into 5 groups to see which range of percentage of males involved in agriculture occupation is higher among the other provinces.\n\n\n3.1.1.3.6 Histograms\n The histogram shows a slightly negative skew on the frequency of males involved in agriculture as occupation. The majority of the provinces have males involved in agriculture on the 61.2-71.2% range. The 1.2-11.2% range and the 21.2-31.2% range of males involved in agriculture have the lowest frequency.\n\n\n\n3.1.1.4 What plot represent better the data? why?\nThere isn’t any plot that is more superior that the other, but it wholly depends on the variables being plotted. For instance, the relationship between examination and education can be better represented using a line chart, as a line chart shows a more visible distinction between the variable of examination and education. If using a scatterplot, the many plots in the chart may cause the distinction between the two variables to be less visible, as the two variables are directly correlated and have values very closely to one another.\nOn the other hand, using a scatterplot can better highlight the two groups of highly-populated Catholic provinces and lowly-populated Catholic provinces better than a barchart. The scatterplot can visualize the two groups of clusters on opposite ends, highlighting there there are two major groups in the Swiss: Catholic heavily-populated province or Protestant heavily-populated province\nTherefore,different plots have different uses and can highlight the relationship of some variables better than the other, depending on the context and relationship of the variables being plotted. It is important to explore the different relationships of variables using different charts and plots to better visualize the relationships of the variables in the dataset.\n\n\n\n3.1.2 Friday\n\n3.1.2.1 Choosing the source and dataset\n\nThe source that I chose for this class is the Federal Reserve Economic Data (FRED), with the link: https://fred.stlouisfed.org/.\n\nI chose the FRED source because I would like to delve more into the US economy to conduct economic analysis and financial research as a business major. Additionally, there are a lot of different datasets, including from different countries, that I an explore from the source.\n\nFor this assignment, I used ChatGPT to brainstorm on which dataset I should choose from and why.\n\nAfter using ChatGPT and exploring the source website some more, I decided to choose the dataset containing Corporate Profits by Industry with this link: https://fredaccount.stlouisfed.org/public/datalist/6257. I downloaded the whole data lists, as it all involves the relevant corporate profits of different industries and sub-sectors.\n\n\nI think that it would be interesting to visualize how the different industries’ profits vary throughout the years, and whether there are correlations between any industries. I downloaded the dataset as Excel format.\n\n\n\n3.1.2.2 Context of the dataset\nThe dataset contains quarterly data on the corporate profits with inventory valuation adjustment for different industries, including non-financial, financial, federal, and many other sub-categories such as manufacturing, wholesale trade, and utilities. The data is presented in billions of dollars, seasonally adjusted annual rate. To set parameter, the study for this assignment only looks at the 10 year span from first quarter of 2014 to the first quarter of 2024.\n\n\n3.1.2.3 Cleaning and organizing the dataset\n\nReplacing Series ID with Titles\n\nThe dataset contains columns with Series ID instead of titles. Therefore, it needs to be replaced with the titles, which can be viewed in the other sheet in the same Excel document. - To do this, I copied the first Series ID and paste it on the ‘Find’ tool.\n\n\nThen, I go to the other sheet in the same Excel document to find the same Series ID and locate the title\n\n\n\nAfter locating the title, I copied it and paste it on the dataset sheet to replace the Series ID with the right title.\n\n\n\nRepeat this until all other Series ID are replaced with its title\n\n\nMaking the titles more concise When copy and pasting back the titles to the dataset sheet, I realized that the titles are very long and repetitive. Therefore, I reorganized the title and make it shorter and more concise.\nFormatting the columns to be more organized and neat To do this, I expanded the columns to the right width, and centered the text. Additionally, I also deleted the rows with data outside of the 10 year span from 2014 to 2024, as it is outside the parameter of the this exploration.\n\n\n\n\n3.1.2.4 Making Visualizations\nIn making visualizations, I utilized Excel’s ‘Recommended Charts’ tool under ‘Insert’. After using the tool, I looked over the data and explore to create more meaningful visualizations on my own.\n\n3.1.2.4.1 Line chart: Average Total Profits in 10 Years\n\nWhen I added a graph from the ‘Recommended Charts’, a new sheet is created along with corporate profits summed into yearly instead of quarterly.\n\n\nTotal corporate profits increase over the 10 year period.\nThere are some significant declines in corporate profits The two major declines occurred in the span of end of quarter of 2019 to end of quarter of 2020. in the late quarter of 2019 to 2020. It is evident that the COVID-19 pandemic caused the sudden drop in corporate profits, with the lock-down, inflation, and slowdown of the economy. The economy healed in the first two quarters of 2020 as the lock-downs are alleviated, however declined again in the end of quarter of 2020.\nThe private enterprises’ profits appear to be very steadily increasing from first quarter of 2022 to the first quarter of 2024. The total corporate profits appear to be higher than the average total corporate profits dotted line, signifying that the increase in total corporate profits are quite significant.\n\n\n\n3.1.2.4.2 Scatterplot 1: Domestic Industries (Non-financial: Manufacturing) vs. Federal\n\n\n“Domestic Industries: Non-financial: Manufacturing” appears to have an inverse relationship with “Federal”\n\nAs ‘Manufacturing’ is high at a range of 675-825 billion dollars, ‘Federal’ appears to be on hundreds of billion dollars loss. However, as ‘Manufacturing’ decreases to below 600 billion dollars, ‘Federal’s’ corporate profits improved to a range of 50-125 billion dollars.\n\n\n3.1.2.4.3 Scatterplot 2: Wholesale trade vs. Transportation and warehousing\n\n\nAs wholesale trade’s corporate profits increases, the corporate profits in the sub-sector of transportation and warehousing also increases.\n\nThe direct and proportional relationship of the two sub-sectors make sense, as wholesale trade involves purchasing goods in large quantities and reselling them in smaller quantities to businesses and other wholesalers, directly involving transportation and warehousing of the goods.\n\n\n3.1.2.4.4 Barchart: Non-financial vs. Financial Domestic Industry\n\n\nThe non-financial domestic industry’s corporate profits are growing more significantly compared to the financial domestic industry’s stable corporate profits\n\nThe non-financial domestic industry are growing more rapidly, especially in the last half of the decade. The financial domestic industry has over 2500 billions of dollars of corporate profits in the first quarter of 2024. Meanwhile, the financial domestic industry appears to have stable corporate profits throughout the past 10 years, as its corporate profits are varying slightly from year to year and are stuck under 750 billion dollars.\n\n\n3.1.2.4.5 Piechart: Non-financial makes up the majority of the domestic industry’s corporate profits\n\n\nNon-financial domestic industry takes 70% of the domestic industry’s corporate profits, while the rest of the world takes up only 14% of the domestic industry’s corporate profits.\nAs stated in a study, non-financial corporations are responsible for a large share of the economic activity in most advanced economies (Tebrake & O’hagan, n.d.), which maakes sense as a lot of the important sub-sectors belong to the non-financial industry (including manufacturing of durable and non-durable goods, wholesale trade, retail trade, utilities, and information)\n\n\n\n\n3.1.2.5 References\n\nTebrake, J., & O’hagan, P. (n.d.). Understanding Financial Accounts The financing of non-financial corporations. Retrieved August 31, 2024, from https://www.oecd-ilibrary.org/docserver/9789264281288-8-en.pdf?expires=1725145368&id=id&accname=guest&checksum=CE2ED779ABF58E8BEB69C041927BC309\nhttps://fred.stlouisfed.org/.\nhttps://fredaccount.stlouisfed.org/public/datalist/6257",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Federal Reserve Economic Data - Janice Oenardi</span>"
    ]
  },
  {
    "objectID": "janice.html#week-2---0902-0906",
    "href": "janice.html#week-2---0902-0906",
    "title": "3  Federal Reserve Economic Data - Janice Oenardi",
    "section": "3.2 Week 2 - 09/02 ~ 09/06",
    "text": "3.2 Week 2 - 09/02 ~ 09/06\n\n3.2.1 Wednesday and Friday Work\n\n3.2.1.1 Context of data set\nI am using the ‘airquality’ data set, which is the daily air quality measurements in New York from May 1, 1973, to September 30, 1973. The format is a data frame with 153 observations on 6 variables:\n\nOzone: numeric Ozone (ppb)\nSolar.R: numeric Solar R (lang)\nWind: numeric Wind (mph)\nTemp: numeric Temperature (degrees F)\nMonth: numeric Month (1–12)\nDay: numeric Day of month (1–31)\n\nDetails:\n\nOzone: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island\nSolar.R: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park\nWind: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport\nTemp: Maximum daily temperature in degrees Fahrenheit at La Guardia Airport.\n\nSource and reference:\nThe data were obtained from the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data).\nChambers, J. M., Cleveland, W. S., Kleiner, B. and Tukey, P. A. (1983) Graphical Methods for Data Analysis. Belmont, CA: Wadsworth.\n\n\n3.2.1.2 Uploading and converting the document\n\nI downloaded the airquality data set in .csv format from D2L.\nThen, I uploaded it onto Excel.\n\nSince the format is in .csv, I need to change from ‘Viewing’ to ‘Editing’, and convert the Excel into .xlsx format\n\nAfterwards, I renamed the Excel file and saved it onto the right folder.\n\n\n\n3.2.1.3 Cleaning the data set\nBefore processing and creating visualizations of the dataset, it is important to prep the data set by cleaning it. There might be N/A values or outliers in the dataset, which needs to be removed and cleaned to create better and more accurate visualizations.\nTo clean the data, I used Excel to clean the data and remove all the rows which as NA values.\nHere are the things I did step by step to clean the data set:\n\nFiltering the data set and checking only the NA values\n\n\n\nDelete the rows with NA values\n\n\n\nRepeat on the other columns\n\n\n\n\n3.2.1.4 Creating visualizations (Charts + Pivot Tables)\n\n3.2.1.4.1 Histogram: Ozone Distribution\n\n\nThe histogram shows a positively skewed (or right-skewed) distribution which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer\nThe mode of the ozone level is 1-25 ppb.\n\n\n\n3.2.1.4.2 Scatter plot: Ozone vs. Temperature\n\n\nThe scatter plot shows an increasing in trend of both variables. It suggests that ozone depend on temperature, but other factors like the day, month, and solar wind also affect temperature.\nOzone and temperature shows a directly related relationship, where the increase in ozone may cause to an increase in temperature.\n\n\n\n\n3.2.1.5 Creating a Pivot Table\n\nSelect the data you want to include in the pivot table\n\n\n\nCreate pivot table in a new sheet\n\n\n3. Drag ‘Month’ into the Rows\n\n4. Drag ‘Day’ into Columns\n\n5. Drag ‘Ozone’, ‘Solar.R’, ‘Wind’, ‘Temp’ into Values\n\n\nExplore and drag around the variables in different ways to create more insightful pivot tables\n\n\n3.2.1.5.1 Pivot Table 1: Average of Variables Summary\nMaking visualizations in the form of a pivot table allows us to help understand the data set by allowing the data to be presented from another perspective. We can also choose where to put the values on the pivot table to present the data in different ways.\n\n\nAverage Ozone Levels: Peaks in July (59.1) and August (60.0), while May (24.1) and September (31.4) have the lowest levels.\nAverage Temperature: Highest in July (83.9) and August (83.7), with May (66.5) being the coolest.\nAverage Wind Speed: Highest in June (12.2) and lowest in July (8.5).\nAverage Solar Radiation (Solar.R): Peaks in July (216.4), and the lowest in September (168.2).\nOverall, July and August show higher ozone and temperature levels. Ozone and temperature levels appear to have a directly proportional relationship, with average ozone increasing in May to August and decreasing in September, and average temperature following a similar upward and downward trend throughout the months.\nAverage temperature and solar levels appear to also have a directly proportional relationship, as solar directly affects the temperature.\nOn the other hand, wind speed varies inversely with temperature. Wind speed appears to be increasing from May to June, sharply drops in July, and increasing again in August and September.\n\n\n\n\n3.2.1.6 Pivot Table 2: Temperature\n\n\nThe pivot table allows us to delve more deeply on the average temperature trend throughout the months and days.\nAs seen from the pivot table, the hottest days appear to be on the end of August (August 28 to August 31) to the beginning of September (September 1 to September 4), with the temperature consistently reaching above 90 degrees throughout those days).\nMeanwhile, the coldest days seem to be in early May, with temperature ranging from 58 degrees to 79 degrees.\n\n\n\n3.2.1.7 Pivot Table 3: Temperature vs. Ozone, Wind, and Solar\n\n\nSolar generally increases with temperature and peaks at temperature 86 lang with an average of 211.6 lang. As seen from the chart, solar fluctuates most significantly among other variables, with values ranging from 20 to 298.5 lang.\nOzone increases as temperature rises, with notable jumps at 71 degrees (15 ppb), 77 degrees (23 ppb), and peaks at 89 degrees (103.5 ppb).\nThe highest average ozone level is 103.5 ppb at 89 degrees in. Meanwhile, ozone appears to be low (&lt;30 degrees) for temperatures below 70 degrees, such as in May and September. This further suggests that ozone and temperature has a directly proportional relationship.\nWind shows a decreasing trend as the temperature rises. Wind peaks at 18.4 mph for 57 degrees and consistently reduces, reaching as low as 4.3 mph at 93 degrees, which shows that wind and temperature are inversely related.\nWind has relatively stable values between 6-10 mph for temperatures above 80 degrees.\nHigher temperatures correlate with lower wind speeds and increased ozone levels. Meanwhile, solar has a fluctuating trend and does not show a consistent pattern with temperature changes.\n\n\n\n3.2.1.8 Bar chart from pivot table\nI selected the pivot table and inserted a bar chart using Excel to visualize the relationship of temperature with solar, ozone, and wind by month. \n\nOzone shows a steady increase from May (5) to July (7), peaks in August (8), and then decreases in September (9).\nTemperature increases gradually from May (5) to July (7), maintains a similar level in August (8) and September (9).\nWind remains relatively low and stable throughout all months, with slight fluctuations.\nSolar peaks sharply in May (5) and July (7), slightly decreases in June (6) and August (8), and remains high in September (9). Solar radiation shows high peaks in May and July.\n\n\n\n3.2.1.9 Line chart from pivot table\nI selected another pivot table and inserted a line chart using Excel to visualize the relationship of temperature with solar, ozone, and wind.\n\n\nThe pivot table allows us to look more closely on the effect of temperature on the other variables (ozone, wind, and solar).\nAs seen on the pivot table and the line chart, the solar levels fluctuate the most among the other variables and seem to generally increase as temperature approaches 97.\nBesides that, ozone follow a similar upward trend as temperature increases, suggesting a directly proportional relationship.\nAdditionally, wind appears to be inversely related with temperature, as it decreases when temperature approaches its maximum.\n\n\n\n3.2.1.10 Findings and takeaways\nWhat and how does the findings pertains to our overall project objective?\n\nThe findings in this assignment allowed me to learn that making visualizations are not just about making different kinds of charts, but also can be in the form of tables -in this case, pivot tables.\nPivot tables are important and can be used strategically to present data from a new set of perspectives by arranging, formatting, and organizing the data in different rows, columns, and values. By presenting the data in a more organized way, we can draw more analysis and correlations of how different variables relate to one another in the data set.\nMaking a more insightful and in-depth analysis of the data set requires a deep exploration by trying to plot data in different ways, formatting them differently (ex: adjusting the min/max values or the bin range of a chart),\n\nI will incorporate the findings from this assignment into the mid-term project, to allow a better exploration and analysis of the FRED data set. Specifically, aside from making different charts (scatter plots, distribution, line chart, etc.), I will also deeply explore the data set variables through pivot tables. Pivot tables allow us to see the data in a different dimension, which is more concise and summarized for better comprehension. As the FRED data set is huge, pivot tables would be very useful in effectively visualizing and finding meaningful correlations and analyses of the variables.\n\n\n\n3.2.2 Midterm Project Context\n\n3.2.2.1 Choosing the source and dataset\n\nThe source that I chose for this class is the Federal Reserve Economic Data (FRED), with the link: https://fred.stlouisfed.org/\nI chose the FRED source because I would like to delve more into the US economy to conduct economic analysis and financial research as a business major. Additionally, there are a lot of different data sets, including from different countries, that I an explore from the source.\n\n\n\n3.2.2.2 Context of the FRED data source\nThe FRED (Federal Reserve Economic Data) data set contains various economic indicators from the U.S., such as employment statistics, inflation rates, GDP data, and other key economic variables. Here’s a breakdown of the context of this data set:\n\n3.2.2.2.1 Variables\nAs the FRED source contains a lot of different datasets, here are only some of the variables included in the dataset source.\n\nGDP (Gross Domestic Product): A measure of the total economic output of a country.\nUnemployment Rate: The percentage of the labor force that is unemployed but actively seeking work.\nInflation Rate (CPI or PPI): The rate at which the general level of prices for goods and services is rising.\nInterest Rates: Such as the Federal Funds Rate, which is the interest rate at which banks lend to each other overnight.\nIndustrial Production Index: A measure of the real output of the manufacturing, mining, electric, and gas industries.\nRetail Sales: A measure of the total sales within the retail sector.\nHousing Starts: The number of new residential construction projects that have begun in a given period.\nConsumer Confidence Index: A measure of consumer sentiment about the economy.\nExports and Imports: The total value of goods and services exported and imported by a country.\n\n\n\n3.2.2.2.2 Who collected the data\nThe data is collected by various U.S. government agencies such as:\n\nThe U.S. Bureau of Labor Statistics (BLS)\nThe U.S. Bureau of Economic Analysis (BEA)\nThe Federal Reserve Board\nThe U.S. Census Bureau It is then compiled and made available to the public by the Federal Reserve Bank of St. Louis via the FRED platform.\n\n\n\n3.2.2.2.3 Purpose of the source and data set\n\n3.2.2.2.3.1 The primary purpose of the FRED data set is to provide economic data that can be used for:\n\nEconomic Research and Analysis: By economists, policymakers, and academic researchers to understand economic trends and conditions.\nPolicy Making: By government agencies and the Federal Reserve to inform monetary and fiscal policy decisions.\nMarket Analysis: By financial analysts and investors to forecast economic conditions and make informed decisions.\nEducation: As a resource for students and educators in understanding economic principles and trends.\n\n\n\n3.2.2.2.3.2 There are several objectives/purposes of the data set that can be relevant fo the mid-tern project. I can delve into the economic indicators over time, such as:\n\nGDP Growth Rate: To observe economic expansion or contraction.\nUnemployment Rate: To identify periods of high or low employment.\nInflation Rate: To detect inflationary or deflationary trends.\nInterest Rates: To understand changes in the cost of borrowing.\n\n\n\n3.2.2.2.3.3 For comparative analysis, I can also use bar charts, line charts, scatter plots, pivot charts, and other visualizatioons to compare different indicators:\n\nInterest Rates vs. Inflation Rates: To show the relationship between monetary policy and inflation.\nGDP vs. Unemployment Rate: To show how economic output correlates with employment levels.\nConsumer Confidence vs. Retail Sales: To visualize how sentiment aligns with spending behavior.\n\n\n\n3.2.2.2.3.4 For a deeper exploration using the source and data set, I can also create sector-specific visualizations to focus on specific sectors such as manufacturing, housing, or consumer spending:\n\nIndustrial Production Index: To visualize the output of key industries.\nHousing Starts: To visualize the health of the real estate sector.\nRetail Sales by Category: To understand consumer spending patterns.\n\n\n\n3.2.2.2.3.5 I can create geographical visualizations from different countries that the data set including:\n\nUnemployment Rates by Country: To visualize regional differences in employment.\nGDP Growth by Country: To compare economic performance across different areas.\n\nAdditionally, I can delve into leading and lagging Indicators by visualize leading indicators (like consumer confidence) against lagging indicators (like GDP) to understand potential future economic conditions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Federal Reserve Economic Data - Janice Oenardi</span>"
    ]
  },
  {
    "objectID": "janice.html#week-3---0909-0913",
    "href": "janice.html#week-3---0909-0913",
    "title": "3  Federal Reserve Economic Data - Janice Oenardi",
    "section": "3.3 Week 3 - 09/09 ~ 09/13",
    "text": "3.3 Week 3 - 09/09 ~ 09/13\n\n3.3.1 Wednesday\n                   \n\n3.3.1.1 Description\nThis famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\n\n\n3.3.1.2 Format\niris is a data frame with 150 cases (rows) and 5 variables (columns). The columns are named:\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\niris3 gives the same data arranged as a 3-dimensional array of size 50 by 4 by 3, as represented by S-PLUS. The first dimension gives the case number within the species subsample, the second the measurements with names Sepal L., Sepal W., Petal L., and Petal W., and the third the species.\n\n\n3.3.1.3 Source and references\nFisher, R. A. (1936) The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7, Part II, 179–188.\nThe data were collected by Anderson, Edgar (1935). The irises of the Gaspe Peninsula, Bulletin of the American Iris Society, 59, 2–5.\nBecker, R. A., Chambers, J. M. and Wilks, A. R. (1988) The New S Language. Wadsworth & Brooks/Cole. (has iris3 as iris.)\n\n\n3.3.1.4 Figure 1:\n\n\nThe chart shows the relationship between petal width and petal length for three iris species: setosa, versicolor, and virginica.\nSetosa has the smallest petal length and width, clustering around the lower left corner.\nVersicolor occupies the middle range of petal length and width, showing a positive correlation between these two variables.\nVirginica has the largest petal dimensions, forming a cluster in the upper right corner, also showing a clear positive trend.\nThere is little overlap between setosa and the other species, but versicolor and virginica have some overlap in their measurements.\n\n\n\n3.3.1.5 Figure 2:\n\n\nSetosa has the shortest average sepal length, smallest petal length, and width compared to the other species.\nVersicolor (orange) displays moderate values for sepal and petal length/width, generally in between setosa and virginica.\nVirginica has the largest average values for all four measurements (sepal length, sepal width, petal length, petal width).\nSepal width for setosa is the highest among the three species, while virginica has the largest values for both petal length and width.\n\n\n\n3.3.1.6 Figure 3:\n\n\nThe boxplot visualizes the distribution of the average petal and sepal dimensions (length and width) across different iris species:\nAverage Petal Length: Ranges from around 1 to 5, with most data concentrated between 3 and 4.\nAverage Petal Width: A narrower range from 0.5 to 2.0, with the majority between 1.0 and 1.5.\nAverage Sepal Length: A wider range, mostly between 5 and 7, indicating greater variation in sepal length.\nAverage Sepal Width: Ranges between 2.0 and 4.0, with a tighter spread, suggesting less variability.\nOverall, petal and sepal dimensions show distinct ranges, with sepal measurements generally larger than petal measurements.\n\n\n\n3.3.1.7 Figure 4:\n\n\nA deeper analysis of the scatter plot reveals the following patterns and relationships among the three iris species:\n\nSetosa:\n\nThis species has a distinct cluster with relatively small sepal lengths (around 4.5 to 5.5) and larger sepal widths (typically 3.2 to 4.4).\nThe clear separation of this cluster indicates that setosa has distinct structure and form characteristics compared to the other two species.\n\nVersicolor:\n\nVersicolor occupies a middle ground in terms of both sepal length and width. Its sepal lengths range from 5.0 to 7.0, while its widths fall mostly between 2.5 and 3.2.\nThere is some overlap with virginica, making it less distinct in terms of sepal dimensions. However, it is separable from setosa, especially in sepal width.\n\nVirginica:\n\nVirginica shows the highest sepal lengths, ranging from 6.0 to 8.0, but with sepal widths similar to versicolor (around 2.5 to 3.5).\nThe overlap in width with versicolor shows that there is some ambiguity between these two species based on sepal dimensions alone, but virginica’s larger sepal length helps differentiate it.\n\nPetal Length (Point Size):\n\nThe size of the circles represents average petal length, with larger circles indicating longer petals.\nSetosa has the smallest petals on average, as indicated by the small circle sizes.\nVirginica has the largest petals, reflected in its larger circle sizes.\nVersicolor is in between, with moderate petal lengths.\n\nInter-Species Overlap:\n\nSetosa is well-separated from the other two species in terms of both sepal length and width, making it easy to distinguish.\nVersicolor and Virginica overlap significantly in their sepal width and partially in sepal length. The main distinction between these two species lies in sepal length, where virginica tends to have longer sepals, and in petal length, with virginica showing larger petals.\n\nThe chart shows clear separation of setosa from the other species based on sepal dimensions, while versicolor and virginica show partial overlap, making them more challenging to differentiate. Petal length, represented by circle size, is an important additional factor in distinguishing virginica from versicolor.\n\n\n\n3.3.2 Friday\n                   \n\n3.3.2.1 Context of data set\n\nThe source that I chose for this class is the Federal Reserve Economic Data (FRED), with the link: https://fred.stlouisfed.org/.\nI chose the FRED source because I would like to delve more into the US economy to conduct economic analysis and financial research as a business major. Additionally, there are a lot of different data sets, including from different countries, that I an explore from the source.\nThe FRED (Federal Reserve Economic Data) data set contains various economic indicators from the U.S., such as employment statistics, inflation rates, GDP data, and other key economic variables.\nFor this assignment, here is the breakdown of the variables in the dataset:\n\n\n\n3.3.2.2 Variables\n\n3.3.2.2.1 1. Labor Force Participation Rate\n\nSource: U.S. Bureau of Labor Statistics\nDefinition: A measure of the total economic output of a country.\nUnits: Percent\nFrequency: Monthly\nDescription: The labor force participation rate is defined by the Current Population Survey (CPS) as “the number of people in the labor force as a percentage of the civilian noninstitutional population”. The participation rate is the percentage of the population that is either working or actively looking for work.”\n\nThe Labor Force Participation Rate is collected in the CPS and published by the BLS. It is provided on a monthly basis, so this data is used in part by macroeconomists as an initial economic indicator of current labor market trends. The labor force participation rate helps government agencies, financial markets, and researchers gauge the overall health of the economy.\n\n\n3.3.2.2.2 2. Unemployment Rate\n\nSource: U.S. Bureau of Labor Statistics\nDefinition: The unemployment rate represents the number of unemployed as a percentage of the labor force.\nUnits: Percent\nFrequency: Monthly\nDescription: Labor force data are restricted to people 16 years of age and older, who currently reside in 1 of the 50 states or the District of Columbia, who do not reside in institutions (e.g., penal and mental facilities, homes for the aged), and who are not on active duty in the Armed Forces.\n\n\n\n3.3.2.2.3 3. 30-Year Breakeven Inflation Rate\n\nSource: Federal Reserve Bank of St. Louis\nDefinition: The breakeven inflation rate represents a measure of expected inflation derived from 30-Year Treasury Constant Maturity Securities (BC_30YEAR) and 30-Year Treasury Inflation-Indexed Constant Maturity Securities (TC_30YEAR).\nUnits: Percent\nFrequency: Monthly\nDescription: The latest value implies what market participants expect inflation to be in the next 30 years, on average. Starting with the update on June 21, 2019, the Treasury bond data used in calculating interest rate spreads is obtained directly from the U.S. Treasury Department.\n\n\n\n3.3.2.2.4 4. 10-Year Real Interest Rates\n\nSource: Federal Reserve Bank of Cleveland\nDefinition: the expected rate of inflation over the next 30 years along with the inflation risk premium, the real risk premium, and the real interest rate.\nUnits: Percent\nFrequency: Monthly\nDescription: Their estimates are calculated with a model that uses Treasury yields, inflation data, inflation swaps, and survey-based measures of inflation expectations.\n\n\n\n3.3.2.2.5 5. Real Disposable Personal Income\n\nSource: U.S. Bureau of Economic Analysis\nDefinition: a measure of an individual or household’s purchasing power after adjusting for inflation and taxes.\nUnits: Billions of Chained 2017 Dollars\nFrequency: Monthly\nDescription: It’s calculated by subtracting taxes and other mandatory payments from a household’s disposable income, and then adjusting for inflation.\n\n\n\n3.3.2.2.6 6. Federal Funds Effective Rate\n\nSource: Board of Governors of the Federal Reserve System (US)\nDefinition: The federal funds rate is the interest rate at which depository institutions trade federal funds (balances held at Federal Reserve Banks) with each other overnight.\nUnits: Percent\nFrequency: Monthly\nDescription: When a depository institution has surplus balances in its reserve account, it lends to other banks in need of larger balances. In simpler terms, a bank with excess cash, which is often referred to as liquidity, will lend to another bank that needs to quickly raise liquidity. (1) The rate that the borrowing institution pays to the lending institution is determined between the two banks; the weighted average rate for all of these types of negotiations is called the effective federal funds rate.(2) The effective federal funds rate is essentially determined by the market but is influenced by the Federal Reserve through open market operations to reach the federal funds rate target.(2)\n\n\n\n3.3.2.2.7 7. Personal Consumption Expenditures\n\nSource:U.S. Bureau of Economic Analysis\nDefinition: a measure of the value of goods and services purchased by U.S. residents\nUnits: Billions of Dollars\nFrequency: Monthly\nDescription: The PCE price index is a key measure of inflation and consumer spending in the U.S.. It’s calculated using PCE data and reflects changes in consumer behavior and inflation across a wide range of consumer expenses\n\n\n\n3.3.2.2.8 8 Personal Income\n\nSource: U.S. Bureau of Economic Analysis\nDefinition: Personal income is the income that persons receive in return for their provision of labor, land, and capital used in current production and the net current transfer payments that they receive from business and from government\nUnits: Billions of Dollars\nFrequency: Monthly\nDescription: Personal income is equal to national income minus corporate profits with inventory valuation and capital consumption adjustments, taxes on production and imports less subsidies, contributions for government social insurance, net interest and miscellaneous payments on assets, business current transfer payments (net), current surplus of government enterprises, and wage accruals less disbursements, plus personal income receipts on assets and personal current transfer receipts.\n\n\n\n\n3.3.2.3 Format\nAs the FRED source contains a lot of different datasets, here are only some of the variables included in the dataset source.\n\n\n3.3.2.4 Who collected the data\nThe data is collected by various U.S. government agencies such as:\n\nThe U.S. Bureau of Labor Statistics (BLS)\nThe U.S. Bureau of Economic Analysis (BEA)\nThe Federal Reserve Board\n\nThe U.S. Census Bureau It is then compiled and made available to the public by the Federal Reserve Bank of St. Louis via the FRED platform.\n\n\n3.3.2.5 Purpose/Goal of data set\n\n3.3.2.5.1 There are several objectives/purposes of the data set that can be relevant fo the mid-term project:\n\nPersonal Income and Consumption Expenditures: To observe the trends in personal income and expenditures over a decade from 2014 to 2024\nUnemployment Rate: To identify periods of high or low employment.\nInflation Rate: To detect inflationary or deflationary trends.\nInterest Rates: To understand changes in the cost of borrowing.\n\n\n\n3.3.2.5.2 For comparative analysis, I create visualizations to compare different indicators:\n\nInterest Rates vs. Inflation Rates: To show the relationship between monetary policy and inflation.\n10-Year Real Interest Rate vs. Unemployment Rate: To show the relationship and correlation between interest rate and unemployment rate\nPersonal Consumption Expenditures vs. 10-Year Real Interest Rates: To visualize how inflation affects spending behavior.\nFederal Funds Effective Rate vs. 10-Year Real Interest Rates: To see how federal funds effective rate affect the 10-year real interest rates\nUnemployment Rate vs. Inflation Rate: To see the correlation between unemployment and inflation rate.\n\n\n\n\n3.3.2.6 References\nU.S. Bureau of Labor Statistics, Labor Force Participation Rate [CIVPART], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/CIVPART, September 14, 2024.\nU.S. Bureau of Labor Statistics, Unemployment Rate [UNRATE], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/UNRATE, September 14, 2024.\nFederal Reserve Bank of St. Louis, 30-year Breakeven Inflation Rate [T30YIEM], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/T30YIEM, September 14, 2024.\nFederal Reserve Bank of Cleveland, 10-Year Real Interest Rate [REAINTRATREARAT10Y], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/REAINTRATREARAT10Y, September 14, 2024.\nU.S. Bureau of Economic Analysis, Real Disposable Personal Income [DSPIC96], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/DSPIC96, September 14, 2024.\nBoard of Governors of the Federal Reserve System (US), Federal Funds Effective Rate [FEDFUNDS], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/FEDFUNDS, September 14, 2024.\nU.S. Bureau of Economic Analysis, Personal Consumption Expenditures [PCE], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/PCE, September 14, 2024.\n\n\n3.3.2.7 Visualizations and Observations\n\n3.3.2.7.1 Figure 1: Personal Income and Expenditures in 2014 to 2024\n\n\nTrend of Personal Income and Expenditures: From 2014 to 2024, both average personal income and personal consumption expenditures show an overall increasing trend. The growth in personal income appears to be more consistent and slightly steeper compared to personal consumption expenditures.\nYear 2020 Anomaly: There is a sharp peak in the percentage difference of both personal income and consumption expenditures around 2020, likely reflecting economic impacts due to COVID-19, such as stimulus payments affecting income and changes in consumption patterns during the pandemic.\nPost-2020 Variability: After 2020, both the percentage differences in income and expenditures show increased variability, indicating a more turbulent economic environment post-pandemic with fluctuations in growth rates.\nRecovery and Stabilization by 2024: By 2024, the percentage differences in both variables appear to stabilize somewhat, suggesting a return to a more steady state compared to the immediate post-pandemic years.\nIntersection and Divergence: The graph indicates that the percentage difference in income and expenditures were quite close around 2017 and then diverged significantly during and after 2020, highlighting differing rates of recovery or growth in these economic factors.\n\n\n\n3.3.2.7.2 Figure 2: Unemployment Rate vs. Inflation Rate\n\nBased on the scatter plot showing the relationship between the unemployment rate and the 30-year breakeven inflation rate, here are some concise observations:\n\nInverse Relationship: There is an inverse relationship between the unemployment rate and the 30-year breakeven inflation rate. As the unemployment rate increases, the inflation rate generally decreases.\nAbove and Below Threshold Clusters: Data points marked as below the threshold (orange) dominate the lower range of unemployment rates (between 3% and 6%) and tend to cluster around inflation rates of 1.8% to 2.4%.\nData points above the threshold occur at higher unemployment rates (above 6%), where the breakeven inflation rate drops, suggesting a stronger link between rising unemployment and lower inflation at these levels.\nHigh Unemployment, Low Inflation: At very high unemployment rates (above 10%), inflation rates are consistently below 2%, reinforcing the inverse relationship at extreme unemployment levels.\nLow Unemployment, Higher Inflation Stability: At lower unemployment rates (below 6%), the breakeven inflation rate stays within a tighter range (1.8% to 2.4%), indicating more stability in inflation during periods of lower unemployment.\n\n\n\n3.3.2.7.3 Figure 3: Trends in Unemployment Rate, Inflation Rate, Federal Reserve Effective Rate, and Interest Rate\n\n\nUnemployment Rate: There is a steady decline in the unemployment rate from 2014 to 2019. However, a sharp increase occurs in 2020, likely due to the pandemic, followed by a sharp recovery by 2021, and then a slight increase in 2023 and 2024.\nFederal Reserve Effective Rate: This rate remained relatively stable and low from 2014 to 2015, then gradually increased from 2016 to 2019. There was a notable dip in 2020, corresponding with the economic stimulus and policy changes during the pandemic, followed by a steep rise starting in 2022.\nInflation Rate: The inflation rate remains relatively flat from 2014 to 2020, with a slight increase during 2021-2022, followed by a stabilization trend in 2023 and 2024.\nInterest Rate (Dotted Line): This line shows a more gradual and consistent upward trend over the decade, likely reflecting a long-term increase in interest rates.\nCOVID-19 Impact (2020-2021): The chart shows significant volatility in all metrics during 2020-2021, with unemployment spiking and interest rates dropping drastically before recovering post-pandemic.\n\n\n\n3.3.2.7.4 Figure 4: Personal Consumption and Inflation Rate\n\n\nAverage Personal Consumption Expenditures show a steady increase from 2014 to 2024 and grew from 11,874 in 2014 to 19,028 in 2024. There are significant jumps in expenditures from 2020 onward.\nMeanwhile, Average 30-year Breakeven Inflation Rate remains relatively stable. It fluctuates slightly around 1.5% - 2.0% throughout the period.\nWhile personal consumption expenditures increase consistently, inflation rate maintains a relatively flat trajectory, suggesting consumption grows regardless of inflation stability.\n\n\n\n3.3.2.7.5 Figure 5: Interest Rate vs. Federal Funds Effective Rate\n\n\nFederal Funds Effective Rate and 10-Year Real Interest Rate follow a somewhat parallel paths with some fluctuations. Both rates were relatively stable and low from 2014 to around 2018.\nThere is a significant spike in the 10-Year Real Interest Rate around 2023, exceeding 50%.\nThere is a sharp decline in both rates from 2023 to 2024, with the Federal Funds Rate dropping back near 0%.\nBoth rates saw a sharp increase in 2018, a drop in 2019, and a moderate rise towards 2021.\nThe unprecedented peak in 2023 for the 10-Year Real Interest Rate is notably out of trend with historical data.\nBoth rates have shown a sharp decline heading into 2024, suggesting a rapid loosening of monetary policy or economic conditions requiring significant rate cuts.\nThe 10-Year Real Interest Rate and the Federal Funds Effective Rate start to diverge significantly around 2022 with the 10-Year rate peaking drastically higher than the Federal Funds rate. This might indicate a market anticipation of inflation or other economic pressures not immediately reflected in the Federal Funds rate.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Federal Reserve Economic Data - Janice Oenardi</span>"
    ]
  },
  {
    "objectID": "janice.html#week-4---0916-0920",
    "href": "janice.html#week-4---0916-0920",
    "title": "3  Federal Reserve Economic Data - Janice Oenardi",
    "section": "3.4 Week 4 - 09/16 ~ 09/20",
    "text": "3.4 Week 4 - 09/16 ~ 09/20\n\n3.4.1 Activating virtual environment\n\nIn class, we activated our virtual environment by typing in the terminal: source venv477/bin/activate\nThen, I typed: jupyter lab\nThis brings me to my browser, opening the jupyter lab where I created a new file to do the Python tutorial\n\n\n\n3.4.2 This is a markdown title\nin markdown we can create lists:\n\nitem 1\nitem 2\nitem 3\n\nalso we can create enumerated lists\n\nHola\nHi\nNamaste\n\nwe can do bold, also italic\n\n\n3.4.3 Kernels in Jupyter\n\nThere are three types of kernels that you can make: code, markdown, and raw.\nMake sure to save the airquality.csv file within the same folder of the jupyter lab file.\n\n##  Here we are importing numpy with a nickname np\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n\n## Click control+enter to run\n1\n[1 2 3 4 5]\n# list are native to Python\nmy_list = [1, 2, 3, 4, 5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n## We will be using a lot of dataframes, so we need pandas library\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\n## 4. Loading csv files\n\n## To load .csv files into a 'DataFrame', we use the pandas function 'read_csv'\nprint(df)\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\ndf = pd.read_csv('airquality_datasets.csv')\nprint(df.info())\nprint(df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\n\n\n3.4.4 Histograms\n\ndropna will erase the NA values that are in ozone and other columns\nplt.hist will format the bins, including the number of bins, color, and outline color of the histogram.\nplt.xlabel is the name of the x-axis\nplt.ylabel is the name of the axis\nplot.show is the command to show the histograms\n\nimport matplotlib.pyplot as plt\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\nDistribution of Ozone Levels\n\n\n\n\n\nDistribution of Temperature",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Federal Reserve Economic Data - Janice Oenardi</span>"
    ]
  },
  {
    "objectID": "janice.html#week-5---0923-0927",
    "href": "janice.html#week-5---0923-0927",
    "title": "3  Federal Reserve Economic Data - Janice Oenardi",
    "section": "3.5 Week 5 - 09/23 ~ 09/27",
    "text": "3.5 Week 5 - 09/23 ~ 09/27\n\n3.5.1 Wednesday\n\n3.5.1.1 Plotnine Tutorial\n\n3.5.1.1.1 Plotnine Tutorial: Understanding the Grammar of Graphics\n\n3.5.1.1.1.1 1. Introduction to Plotnine\nplotnine is a data visualization package for Python based on the Grammar of Graphics, which is a system for understanding and building plots. The grammar describes how plots are constructed by combining data, aesthetic mappings, geometric objects, and other components.\nTo begin, you’ll need to install the plotnine package if you don’t have it installed:\npip install plotnine\n\n\n3.5.1.1.1.2 2. The Grammar of Graphics\nThe Grammar of Graphics consists of the following key components:\n\nData: The data you want to visualize.\n\nAesthetics (aes): How the data is mapped to visual properties, such as x and y coordinates, color, size, etc.\n\nGeometries (geom): The type of plot, like points, lines, bars, etc.\n\nFacets: Subplots based on the data.\n\nScales: Control the mapping from data to aesthetic properties.\n\nCoordinate systems: Adjust how data is projected on the plane (Cartesian, rotations, polar, etc.).\n\nThemes: Adjust the non-data elements like background, labels, gridlines, etc.\n\n\n\n3.5.1.1.1.3 3. Creating Your First Plot\nLet’s begin by creating a simple scatter plot using the famous mtcars dataset. We’ll show how to set up the basic structure and gradually build complexity.\n# Import required libraries\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_point, labs\n# Load the mtcars dataset\n#the below lines have been added by chris to avoid the error\nimport requests, io\nurl = 'https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv'\ns = requests.get(url).content\n\nmtcars = pd.read_csv(io.StringIO(s.decode('utf-8')))\nmtcars.info()\n\n# Create a basic scatter plot\nfirstplot = (ggplot(mtcars, aes(x ='wt', y ='mpg')) +\n geom_point() +\n labs(title ='Scatter Plot of MPG vs Weight',\n      x ='Weight (1000 lbs)',\n      y ='Miles per Gallon'))\n\nfirstplot.save(\"firstplot_oenardi.png\")\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32 entries, 0 to 31\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   mpg      32 non-null     float64\n 1   cyl      32 non-null     int64  \n 2   disp     32 non-null     float64\n 3   hp       32 non-null     int64  \n 4   drat     32 non-null     float64\n 5   wt       32 non-null     float64\n 6   qsec     32 non-null     float64\n 7   vs       32 non-null     int64  \n 8   am       32 non-null     int64  \n 9   gear     32 non-null     int64  \n 10  carb     32 non-null     int64  \n 11  fast     32 non-null     int64  \n 12  cars     32 non-null     object \n 13  carname  32 non-null     object \ndtypes: float64(5), int64(7), object(2)\nmemory usage: 3.6+ KB\n\n\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: firstplot_oenardi.png\n\n\n\nFirst Plot\n\n\n\n\n3.5.1.1.1.4 4. Adding Aesthetic Mappings\nIn the Grammar of Graphics, aesthetics control how data points are represented visually. You can map variables to size, color, shape, and more.\nExample: Color by cyl (number of cylinders)\nplot = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\nplot.save(\"plot_oenardi.png\")\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: plot_oenardi.png\n\n\n\nPlot\n\n\nExample: Size by horsepower (hp)\nplotcolor = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\n\nplotcolor.save(\"plot-colored_oenardi.png\")\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: plot-colored_oenardi.png\n\n\n\nPlot Colored\n\n\n\n\n3.5.1.1.1.5 5. Geometric Objects\ngeom_* specifies the type of plot. You can create scatter plots, line charts, bar plots, histograms, etc.\nExample: Adding a smooth line (geom_smooth)\nfrom plotnine import geom_smooth\n\ngeometric = (ggplot(mtcars, aes(x='wt', y='mpg', color = 'factor(cyl)')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\n\ngeometric.save(\"geometric_objects_oenardi.png\")\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: geometric_objects_oenardi.png\n\n\n\nGeometri Objects\n\n\n\n\n3.5.1.1.1.6 6. Faceting\nFaceting allows you to split your plot into multiple panels based on a factor.\nExample: Facet by cyl\nfrom plotnine import facet_wrap\n\nfaceting = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title = 'MPG vs Weight Faceted by Cylinder',\n      x = 'Weight (1000 lbs)',\n      y = 'Miles per Gallon'))\n\nfaceting.save(\"faceting_oenardi.png\")\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: faceting_oenardi.png\n\n\n\nFaceting Plot\n\n\n\n\n3.5.1.1.1.7 7. Customizing Scales\nScales control the mapping from data to aesthetic attributes. You can customize scales for color, size, and more.\nExample: Custom Color Scale\nfrom plotnine import scale_color_manual\n\nscales = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n scale_color_manual(values=['#1f77b4', '#ff7f0e', '#2ca02c']) +  # Custom colors\n labs(title='MPG vs Weight with Custom Colors',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\nscales.save(\"customizing_scales_oenardi.png\")\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: customizing_scales_oenardi.png\n\n\n\nCustom Scale Plot\n\n\n\n\n3.5.1.1.1.8 8. Flip Coordinates\nCreate a bar plot showing distribution of cylinder\nExample: Fliping coordinates axis\nfrom plotnine import geom_bar, coord_flip\n\n# Create a bar plot showing distribution of cylinders\nflip = (ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\n\nflip.save(\"flip_coordinates_Oenardi_092524.png\")\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: flip_coordinates_Oenardi_092524.png\n\n\n\nFlip Coords Plot\n\n\n\n\n3.5.1.1.1.9 9. Themes\nThemes allow you to adjust the non-data aspects of the plot, such as background, axis labels, and gridlines.\nExample: Apply a Minimal Theme\nfrom plotnine import theme_minimal\n\ntheme = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\ntheme.save(\"minimal_theme.png\")\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: minimal_theme.png\n\n\n\nMinimal Plot\n\n\n\n\n3.5.1.1.1.10 10. Saving the Plot\nYou can save your plot using the save method.\nExample: Save the plot\n# Save the plot to a file\np = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n     geom_point() +\n     labs(title='MPG vs Weight',\n          x='Weight (1000 lbs)',\n          y='Miles per Gallon'))\n\np.save(\"mpg_vs_weight.png\")\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\n/Users/janice/venv477/lib/python3.12/site-packages/plotnine/ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight.png\n\n\n\nMPG vs Weight Plot\n\n\n\n\n\n\n\n3.5.2 Friday\n\n3.5.2.1 Graph\n\n\n\nEconomic Indicators\n\n\nThis graph illustrates the trends of various key economic indicators over time, including the Consumer Price Index, U.S. National Home Price Index, and Industrial Production Index, and Personal Saving Rate from 2014 to mid-2024,s providing insights into their fluctuations.\nThe source that I chose is the Federal Reserve Economic Data (FRED), with the link: https://fred.stlouisfed.org/.\nThe dataset contains quarterly data on the corporate profits with inventory valuation adjustment for different industries, including non-financial, financial, federal, and many other sub-categories such as manufacturing, wholesale trade, and utilities. The data is presented in billions of dollars, seasonally adjusted annual rate. To set parameter, the study for this assignment only looks at the 10 year span from first quarter of 2014 to the first quarter of 2024.\nThe data is collected by various U.S. government agencies such as:\n\nThe U.S. Bureau of Labor Statistics (BLS)\nThe U.S. Bureau of Economic Analysis (BEA)\nThe Federal Reserve Board\nThe U.S. Census Bureau It is then compiled and made available to the public by the Federal Reserve Bank of St. Louis via the FRED platform.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Federal Reserve Economic Data - Janice Oenardi</span>"
    ]
  },
  {
    "objectID": "chris.html",
    "href": "chris.html",
    "title": "4  National Center for Education Statistics - Chris Jacob",
    "section": "",
    "text": "4.1 Week 1 - 08/26 ~ 08/30",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>National Center for Education Statistics - Chris Jacob</span>"
    ]
  },
  {
    "objectID": "chris.html#week-1---0826-0830",
    "href": "chris.html#week-1---0826-0830",
    "title": "4  National Center for Education Statistics - Chris Jacob",
    "section": "",
    "text": "4.1.1 Wednesday\n\n4.1.1.1 Diamonds Dataset\nI applied the learnings and findings from the Excel Unit to another dataset, the diamonds_ggplot2.csv set. -  - The Diamonds dataset defines the different characteristics of a diamond namely, carat, cut, color, clarity, depth, price, and lengths in the x, y, and z direction The dataset itself did not have any NA values, and hence did not need any additional steps for cleaning the data. An interesting observation to be made was that there are several interesting correlations between the columns that can help us observe some basic trends in the diamond trading business. There were more than 53,000 records collected, and so this dataset is very comprehensive!\n\n4.1.1.1.1 Preparing the dataset\nConsidering the size of the dataset, I had to use 500 random records to create my visualizations. Here’s a step by step guide on how I did that.\n\n4.1.1.1.1.1 Add a random number column\n\nChoose a column and add the RAND() function to the column. Double click on the bottom right of the cell to paste the function to the rest of the rows in the dataset. This creates a random number between 0 and 1. \nNext, we apply a filter to the entire dataset, and use the random number column (sorting it from smallest to largest). This automatically rearranges the data records, making the first 500 random. \nCopy the first 500 records to a new sheet. This now becomes your 500 records that give you a true random overview of the Diamonds dataset. \nSince the records didn’t contain any NA values, this looked like the only preparation I needed to do.\n\n\n\n4.1.1.1.1.2 Creating visualizations\n\nScatter Plot: Carat VS Price\n\nThe first plot I created was a scatter plot that visualized the relationship between the carat of the diamond and the price it would sell at \nFrom this, we can see that the dimaonds that have a higher carat sell for more.\n\n\n\nHistogram: Sum of Price by Color\n\nThe next plot I created was a histogram that visualized the relationship between the prices and the color of the diamond. \nFrom this chart, it was clear to see that the diamonds that had the most value were those with the ‘H’ coloration.\n\n\n\nOther charts\n\nWhen trying to create more visualizations, I observed that not all charts were capable of visualizing this data, and furthermore, given the size of the dataset, all operations were much slower on the online excel sheet.\nI believe the best visualization for such data is the Scatter Plot of Price vs Carats, because it captures the inherent reality that more carats means more price. All in all, it was an interesting start to the class and helped me better understand how visualizations can be achieved on excel.\n\n\n\n\n\n\n\n4.1.2 Friday\n\n4.1.2.1 NCES Education Dataset\nNCES Education Dataset is extremely vast and includes several different studies conducted over a range of years. Today, we’ll only be focusing the High School Longitudinal Study of 2009. You can find out more here\nThe High School Longitudinal Study of 2009 (HSLS:09) is a comprehensive study conducted by the National Center for Education Statistics (NCES). It tracks a cohort of students who began 9th grade in 2009, following them through their high school years and beyond into postsecondary education and the workforce. The study focuses on students’ educational trajectories, especially in STEM, examining factors like course-taking patterns, college aspirations, and career choices. The data collected provides insights into how high school experiences influence long-term educational and career outcomes.\n\n4.1.2.1.1 Working with the datset\n\nThis can be an extremely complicated process. The first step should be to go to the registration page and registering yourself to access all the benefits that NCES datalabs offer. \nThis gives us access to the data lab dashboard. Since we can’t use excel because of the extremely large size, so we’ll have to use the datalabs power stats tool for this. \n\n\n\n4.1.2.1.2 Simple Visualization\n\nConsidering that I couldn’t use excel for this process, it was interesting to see the amount of data and variables I had at my disposal for this project. For example, just the keyword ‘STEM’ has 294 variables attached to it.\nOne comparison I made using the PowerStats tool to create a percentage analysis of the relationship between variables S1 and S3.\n\nS1 looks into how math teachers make math so easy to understand. This is divided as ‘Strongly Agree’, ‘Agree’, ‘Neutral’, ‘Disagree’, ‘Strongly Disagree’\nS3 shows how students might consider STEM as a major.\n\nThis visualization further strengthens the premise that teachers are the defining factors in a lot of the future decisions that students make.\n\n\n\n\nPecentage Analysis\n\n\n\nIt further allowed me to use the bar chart to show this information visually. \n\n\n\n4.1.2.1.3 Inference\n\nThese visualizations make it clear that teachers who help make concepts like math and science easier to understand, help these students choose STEM as a major in university.\nThis insight will further add to the ‘STEM Focused’ dataset that I’ll be building during this semester.\n\n\n\n\n4.1.2.2 Conclusion\n\nGiven the vastness of this dataset, I believe it’s important for me to better understand and comprehend this dataset, so that I can fully utilize and appreciate all the data collected. Although I wasn’t able to use Excel on this dataset, the skills learned in the previous classes definitely helped set me up for success with this.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>National Center for Education Statistics - Chris Jacob</span>"
    ]
  },
  {
    "objectID": "chris.html#week-2---0902-0906",
    "href": "chris.html#week-2---0902-0906",
    "title": "4  National Center for Education Statistics - Chris Jacob",
    "section": "4.2 Week 2 - 09/02 ~ 09/06",
    "text": "4.2 Week 2 - 09/02 ~ 09/06\n\n4.2.1 Wednesday & Friday\n\n4.2.1.1 Data Analysis & Pivot Techniques\nThis week, we focused on using pivot tables and pivot charts on the cleaned dataset from last week, to explore relationships between variables in the dataset. Pivot tables allow for a quick and efficient way to summarize, reorganize, and analyze data without needing to manually group or sort data. They provide insights by aggregating data across categories and displaying results in a table format.\n\nPivot tables: Help summarize large datasets by grouping them according to key variables, allowing for quick identification of patterns and trends.\nPivot charts: Visual representations of the data in pivot tables. These charts help communicate the relationships and trends identified in the data through pivot analysis.\n\nUsing these tools, we were able to gain deeper insights into the environmental data from the airquality dataset. Key relationships between temperature, wind speed, ozone concentration, and solar radiation were analyzed, leading to findings that provided a better understanding of weather and air quality trends.\n\n\n4.2.1.2 Context of the AirQuality Dataset\nThe airquality.csv dataset is a popular dataset that contains information about New York’s air quality measurements recorded in the year 1973. It consists of daily readings of various air quality parameters such as ozone concentration, solar radiation, wind speed, and temperature.\n\n4.2.1.2.1 Key Variables:\n\nOzone: The concentration of ozone in parts per billion (ppb).\nSolar.R: Solar radiation in langleys.\nWind: Wind speed in miles per hour.\nTemp: Temperature in degrees Fahrenheit.\nMonth: The month during which the data was recorded (from May to September).\nDay: The day of the month.\n\nThis dataset provides valuable insights into how different environmental factors interact over time and can be used to analyze trends related to air pollution and weather patterns.\n\n\n\n4.2.1.3 Quick Recap of Data Preprocessing\n\n4.2.1.3.1 Download data\nI downloaded a fresh version of the airquality.csv file, and then saved it as a .xlsx file to then manipulate using Excel.\n\n\n\nNew Download of airquality.csv\n\n\n\n\n4.2.1.3.2 Clean Data\nSince it was a fresh download of the file, we needed to clean the data of any NA values. Here is a high-level overview of the steps we take to achieve this:\n\nStep 1: Apply a filter to the data \nStep 2: Filter for all NA data in a column (in this case, Ozone) \nStep 3: Delete all the rows with an NA value \nStep 4: Repeat for any other columns with NA values \n\nNow we have a clean dataset that we can use to create pivot tables and pivot charts.\n\n\n\n4.2.1.4 Pivot Tables and Insights\n\n4.2.1.4.1 Creating a genearl pivot table\n\nTo create a general pivot table, you first select all the data in your dataset as seen below \nYou then go to the insert tab and click on Pivot Table to insert a pivot table \nHere is where you will specify where you want to insert this new table. As suggested by Dr. V, we’ll insert it into a new sheet. \nThe next step is to select the fields you want to use in your pivot table. These fields can either be dragged and dropped or checked and unchecked. As the names of the boxes suggest, you have the option to add a filter, to add a column, to add a row, or to specify what values you want to use. An interesting point to note here is that you can stack rows and columns, essentially creating a dropdown like effect, making data more manageable and understandable. \nGo ahead and select the fields as below \nAfter the fields have been selected, we now have the option to change the value field settings. This means that we can change the kind of data we interpret from the dataset. \nIn this example, we’re going to change it from the sum to the average values for each field. \nAs part of ‘prettifying’ our data, let’s decrease the decimal places in our data, to only the 2 decimal places. \nAfter some more prettifying, we end up with the following pivot table, your first pivot table! \n\n\n\n4.2.1.4.2 Creating your first pivot chart\n\nNow that we have the pivot table, we can create a pivot chart. To do this, the first step would be to select your pivot table and click on Pivot Chart on the insert tab. \nWe can then select what type of chart we’d like to use. Excel automatically tells you what are possible and what aren’t possible. \nAnd after choosing the appropriate chart type, bar chart in our case, we have created our first pivot chart. Congratulations \nAnother interesting thing you can do with pivot charts, is that we can apply filters to the rows and columns on the chart itself. \n\n\n\n4.2.1.4.3 Pivot Chart Variations\nNow that we know how to create pivot tables and pivot charts, let’s use this knowledge to create comparisons in our dataset to further analyze how these different variables interact with each other.\n\n4.2.1.4.3.1 Pivot Chart 1: Average of Ozone, Solar Radiation, Temperature, Wind by Month and Day\n\nThis is the pivot chart we just created. This chart shows us a few general trends. Specifically:\n\nOzone levels seem to correlate with temperature where higher temperatures in July and August correspond with higher ozone concentrations.\nSolar radiation is also highest in the summer months, contributing to higher temperatures, but interestingly, ozone concentrations are lower in September despite similar solar radiation levels.\nWind speeds decrease as temperatures rise, indicating that calmer wind conditions occur during the hotter summer months, potentially allowing pollutants like ozone to accumulate more in the atmosphere.\n\n\n\n\nAverage of Ozone, Solar R., Temp, Wind by Month and Day\n\n\n\n\n\n4.2.1.4.3.2 Pivot Chart 2: Average Temp vs Average Ozone by Month\n\nThis pivot chart looks at the average values of temperature vs the average values of ozone by month. The general understanding this gives us is that:\n\nThe ozone concentration increases with rising temperature during the summer months, particularly in July and August.\nAfter peaking in the summer, both temperature and ozone levels decrease in September, though the decline in ozone levels is more pronounced than the drop in temperature.\nThis pattern suggests that while temperature is a significant factor in ozone concentration, other elements such as sunlight intensity and wind speed might also contribute to the sudden drop in ozone levels observed in September.\n\n\n\n\nPivot table\n\n\n\n\n\nPivot Chart - A trend line on a bar chart\n\n\n\n\n\n4.2.1.4.3.3 Pivot Chart 3: Average Solar Radiation vs Average of Wind speeds by Month\n\nThis pivot chart looks at the average values of solar radiation vs the average values of wind speeds by month. Here are the trends we can observe:\n\nThere is an inverse relationship between solar radiation and wind speed during the summer months. As solar radiation peaks in July, wind speeds drop to their lowest levels. This could be due to more stable weather conditions with fewer disturbances (such as storms) that typically bring stronger winds.\nIn the transition months of May and September, the wind speeds are higher, possibly due to more atmospheric variability as seasons change, while solar radiation is moderate.\nThe decrease in wind speed during the summer may contribute to higher ozone levels observed in earlier analyses, as calmer winds allow pollutants to accumulate.\n\n\n\n\nPivot table\n\n\n\n\n\nPivot Chart - 2 trend lines with markers with a secondary axis\n\n\n\n\n\n\n4.2.1.4.4 Pivot Chart Conclusion\n\nThere are several different charts that can be made from this simple dataset, which further demonstrates just how powerful the pivot table and pivot chart features are!\nLearning how to use this feature is definitely going to be helpful in my future endeavors with data.\n\n\n\n\n\n4.2.2 Midterm Project Context\nThe goal of this midterm project is to analyze data from the High School Longitudinal Study of 2009 (HSLS:09) to explore how early exposure to STEM education affects future career outcomes. The project will focus on: - How students’ interest in STEM subjects during high school influences their college major and career choices. - The role of parents and school environments in shaping STEM aspirations. - (Possible consideration) A comparative analysis of HSLS:09 and other studies such as ELS:2002 and NELS:88 to identify broader educational trends.\nThis project will aim to answer key questions regarding the role of early STEM education in fostering career readiness and bridging gaps for underrepresented groups in STEM.\n\n4.2.2.1 Introduction to NCES Data Source\nThe National Center for Education Statistics (NCES) is a key provider of data on education in the U.S., offering a vast array of longitudinal studies, surveys, and tools. The NCES collects comprehensive data that is useful for understanding student pathways, outcomes, and how factors such as socioeconomic status, race, and school characteristics impact education.\n\n4.2.2.1.1 How to Use NCES Data\nYou can access NCES data at NCES Data Tools. The NCES website offers a range of tools that allow users to: - Download datasets for longitudinal studies such as HSLS:09. - Create custom reports using tools like Data Lab. - Visualize educational trends using tools like the Trend Generator.\n\n4.2.2.1.1.1 Related Resources:\n\nNCES Data Lab Tool\nNCES Online Codebook\n\n\n\n\n\n4.2.2.2 HSLS Dataset\nThe High School Longitudinal Study of 2009 (HSLS:09), launched by the National Center for Education Statistics (NCES), provides a rich source of data for educational research. By tracking students from the beginning of high school through postsecondary education and into their careers, HSLS:09 offers valuable insights into student pathways, especially in STEM (Science, Technology, Engineering, Mathematics).\nLet’s break down the context of HSLS:09 using the 5 W’s framework to better understand its significance.\n\n4.2.2.2.1 Who?\n\nParticipants: HSLS:09 initially sampled over 23,000 ninth-grade students from 944 high schools across the United States. It also collected data from parents, teachers, counselors, and administrators.\nDemographics: The participants represented a diverse cross-section of students, varying in socioeconomic backgrounds, ethnicities, and geographic locations.\nStakeholders: Aside from students, the survey data also involves insights from educators and family members, making it a comprehensive longitudinal study.\n\n\n\n4.2.2.2.2 What?\n\nScope: The HSLS:09 focuses on understanding students’ educational choices, performance, and postsecondary transitions. One of the study’s unique features is its emphasis on STEM education and career trajectories. Cognitive assessments were administered alongside surveys to measure student aptitude in mathematics and science.\nData Collected: The study captures a wide range of data, including academic achievement, cognitive tests, postsecondary aspirations, parental influence, school characteristics, and career goals.\nAssessment: The students underwent math assessments in the ninth grade, and their progress was tracked through follow-up surveys in 2012, 2016, and beyond.\n\n\n\n4.2.2.2.3 Where?\n\nGeographic Coverage: The survey covered high schools throughout the United States, including urban, suburban, and rural areas. This allowed for a broad understanding of educational experiences across different geographic and socioeconomic environments.\nPostsecondary Transitions: The follow-up surveys track students as they transition into a variety of postsecondary environments, including two- and four-year colleges, vocational programs, and directly into the workforce.\n\n\n\n4.2.2.2.4 When?\n\nInitial Data Collection: The study began in 2009 when the participants were in ninth grade. Subsequent data collection points occurred in 2012, 2013, 2016, and more recently, following students through their postsecondary education and career entry phases.\nLongitudinal Approach: HSLS:09 is designed to track participants over an extended period, providing insights into the long-term effects of high school experiences on educational attainment and career success.\n\n\n\n4.2.2.2.5 Why?\n\nSTEM Emphasis: A primary goal of HSLS:09 is to understand how high school experiences, especially in STEM fields, influence later educational and career outcomes. The study explores what motivates students to pursue careers in STEM and what barriers may exist, such as socioeconomic factors or lack of resources.\nEducational Equity: Another key objective is to assess educational equity. HSLS:09 allows researchers to examine disparities in student experiences based on race, gender, socioeconomic status, and geographic location, providing a wealth of information for policymakers.\nPolicy Implications: The data serves as a foundation for designing interventions that can address gaps in education, particularly for underrepresented groups in STEM.\n\n\n\n4.2.2.2.6 How?\n\nMethodology: HSLS:09 uses a combination of surveys, cognitive assessments, and follow-up interviews to gather data. The study includes input from students, parents, educators, and administrators, creating a holistic view of each student’s educational journey.\nAnalysis Frameworks: Researchers use the HSLS:09 data to conduct both quantitative and qualitative analyses. The longitudinal design enables researchers to look at changes over time, such as shifts in career aspirations or academic performance.\n\n\n\n4.2.2.2.7 Conclusion\nThe HSLS:09 dataset offers invaluable insights into the educational trajectories of U.S. high school students, with a particular focus on STEM fields. The data gathered from this study can be used to develop strategies that support student success, particularly in underrepresented communities. For this midterm project, analyzing this dataset allows us to explore key questions about educational pathways, parental influence, and career readiness, providing a solid foundation for research in educational policy and student development.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>National Center for Education Statistics - Chris Jacob</span>"
    ]
  },
  {
    "objectID": "chris.html#week-3---0909-0913",
    "href": "chris.html#week-3---0909-0913",
    "title": "4  National Center for Education Statistics - Chris Jacob",
    "section": "4.3 Week 3 - 09/09 ~ 09/13",
    "text": "4.3 Week 3 - 09/09 ~ 09/13\n\n4.3.1 Wednesday\nI decided to use the Violent Crime Rates by US State, otherwise known as the USArrests dataset. Here’s some key information about the USArrests dataset.\n\n4.3.1.1 Description\nThis data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.\n\n\n4.3.1.2 Format\nA data frame with 50 observations on 4 variables.\n\nMurder numeric Murder arrests (per 100,000)\nAssault numeric Assault arrests (per 100,000)\nUrbanPop numeric Percent urban population\nRape numeric Rape arrests (per 100,000)\n\n\n\n4.3.1.3 Notes\nUSArrests contains the data as in McNeil’s monograph. For the UrbanPop percentages, a review of the table (No. 21) in the Statistical Abstracts 1975 reveals a transcription error for Maryland (and that McNeil used the same “round to even” rule that R’s round() uses), as found by Daniel S Coven (Arizona).\n\n\n4.3.1.4 Source\nWorld Almanac and Book of facts 1975. (Crime rates).\nStatistical Abstracts of the United States 1975, p.20, (Urban rates), possibly available as https://books.google.ch/books?id=zl9qAAAAMAAJ&pg=PA20\n\n\n4.3.1.5 References\nMcNeil, D. R. (1977) Interactive Data Analysis. New York: Wiley.\n\n\n4.3.1.6 Tableau Dashboard\nThis is the dashboard created for the USArrests dataset on tableau. Tableau was fairly easy to use and the USArrests dataset was easy to import and implement.\n                   \nHere’s some insights from the tableau dashboard\n\n4.3.1.6.1 Comparison of Variables - Observations\n\n\n\nComparison of Variables\n\n\n\n4.3.1.6.1.1 Key Insights from the Chart:\n\nUrban Population Dominates:\n\nThe urban population (represented in teal) is significantly higher compared to the crime-related measures (murder, rape, and assault) in all states.\nStates like California, Florida, and New York have noticeably high urban populations, reflecting their dense urban areas.\n\nAssault is the Most Prominent Crime:\n\nAmong the crime-related variables, assault (in blue) has the highest values across most states compared to murder and rape.\nStates like California, Florida, and Texas show higher assault rates than smaller or less populated states.\n\nMurder and Rape Trends:\n\nMurder (red) and rape (green) have lower values compared to assault.\nCertain states, such as Texas and Florida, show moderate increases in both murder and rape rates compared to other states.\n\nState Comparisons:\n\nNew York, California, Texas, and Florida are outliers in terms of high urban population and assault rates, while smaller states like Rhode Island, Vermont, and North Dakota have much lower values across all variables.\n\nGeneral Trends:\n\nIn general, states with larger urban populations tend to have higher crime rates, particularly assaults. However, the murder and rape rates don’t show as sharp an increase, suggesting that assault is more widely reported or prevalent compared to murder and rape in many states.\n\n\n\n\n4.3.1.6.1.2 Overall Observations:\n\nUrbanization and Crime:\n\nThere’s a clear pattern showing that states with larger urban populations also tend to report higher crime rates, especially assault. This could be due to factors like higher population density, socioeconomic conditions, or more effective reporting mechanisms.\n\nAssault:\n\nThe prominence of assault across all states suggests it may be the most common form of violent crime, while murder and rape remain lower.\n\n\n\n\n\n4.3.1.6.2 Murder & Assault Comparison - Observations\n\n\n\nMurder VS Assault Comparison\n\n\n\n4.3.1.6.2.1 Key Observations:\n\nTop States with High Assault and Murder Rates:\n\nFlorida, North Carolina, Maryland, and Arizona are at the top of the list, indicating that these states have the highest combined average rates of murder and assault.\nThese states likely face a combination of socioeconomic factors, larger urban populations, or more prevalent crime, resulting in higher overall violent crime rates.\n\nMid-Ranking States:\n\nStates like California, Mississippi, Alaska, Michigan, New York, Louisiana, and Nevada fall in the middle range of the rankings. These states still have higher than average violent crime rates but are not at the extreme end like Florida or North Carolina.\nThe presence of larger states like California and New York here suggests that while they are populous and urbanized, their combined violent crime rates are relatively controlled when compared to states like Florida.\n\nLower Crime Rate States:\n\nIowa, Wisconsin, Hawaii, Vermont, and North Dakota rank at the bottom of the chart, indicating significantly lower combined rates of murder and assault.\nStates like Vermont and North Dakota are typically less populated and more rural, which may contribute to their lower violent crime rates. This suggests that crime in these states may be less of a social concern.\n\nState-wise Trends:\n\nSouthern and Eastern states like Florida, North Carolina, and Maryland tend to have higher violent crime rates, as opposed to Midwestern and Northern states like North Dakota, Vermont, and Minnesota, which generally report lower rates.\nThis pattern could reflect regional socioeconomic disparities, population density, or differences in law enforcement efficacy.\n\nGeneral Trend:\n\nStates with larger urban populations and economic challenges seem to rank higher on the combined assault and murder rates.\nOn the other hand, smaller, more rural states with lower population densities have less violent crime overall.\n\n\n\n\n4.3.1.6.2.2 Conclusion:\n\nFlorida stands out as the state with the highest combined average of murder and assault rates, potentially indicating broader societal challenges in terms of violence and crime control.\nThe Southern and Eastern U.S. regions tend to show higher violent crime rates, whereas Midwestern and Northern regions demonstrate much lower combined murder and assault rates.\nState size and urbanization do not necessarily guarantee higher crime rates, as demonstrated by California and New York’s middle placement in the chart.\n\nThis chart provides a comprehensive view of how different U.S. states compare in terms of violent crime rates, offering valuable insights into which regions face more crime-related challenges and which regions enjoy greater safety.\n\n\n\n4.3.1.6.3 Assault VS Crime Comparisons - Observations\n\n\n\nAssault VS Crime\n\n\n\n4.3.1.6.3.1 Breakdown and Insights:\n\nAssault vs Murder (Left Plot - Green)\n\nTrend: There is a positive correlation between murder rates and assault rates. States with higher murder rates also tend to have higher assault rates. The points generally follow an upward trend as the murder rate increases.\nOutliers: Some states, however, have relatively high assault rates but lower murder rates, and vice versa. This could indicate differences in the nature of violent crimes across certain states.\nKey Insight: States with higher murder rates are more likely to experience higher assault rates as well, reflecting an overall violent crime environment.\n\nAssault vs Rape (Middle Plot - Red)\n\nTrend: There is also a positive correlation between assault and rape, although it appears less consistent than the correlation between murder and assault.\nVariation: Some states have very high rape rates but relatively lower assault rates and vice versa. This suggests that these two types of violent crime might not always overlap to the same extent.\nKey Insight: While there is some correlation between rape and assault rates, the variation suggests that states may experience different patterns in the prevalence of these two crimes.\n\nAssault vs Urban Population (Right Plot - Blue)\n\nTrend: As expected, there is a positive correlation between the size of the urban population and assault rates. More densely populated states or those with large urban centers tend to report higher assault rates.\nKey Insight: Urbanization plays a significant role in the prevalence of assault crimes. States with larger urban populations often face more challenges related to violent crimes like assault.\n\n\n\n\n4.3.1.6.3.2 General Observations:\n\nOverall Correlation: Assault seems to correlate positively with other violent crimes (murder and rape), but the strongest relationship is with murder rates.\nUrbanization Impact: The third plot clearly demonstrates that higher urban populations are associated with higher assault rates, indicating that population density could be a major contributing factor to crime rates.\n\nThis chart provides a detailed look at how assault correlates with murder, rape, and urban population, showing that violent crimes tend to increase together, especially in more urbanized areas.\n\n\n\n4.3.1.6.4 High Crime VS Low Crime Observations\n\n\n\nHigh vs Low Crime\n\n\n\n4.3.1.6.4.1 Key Insights:\n\nStates with Predominantly High Crime Rates:\n\nStates like Alabama, Alaska, Arizona, Arkansas, and Florida show high overall crime severity (with larger blue bars), indicating that these states tend to experience more severe crime issues.\nIn these states, the high crime rates dominate, suggesting that these states might face significant challenges in curbing violent or severe crimes.\n\nStates with Balanced Crime Severity:\n\nSome states, such as Indiana, Illinois, Iowa, Kentucky, and Maine, show a more balanced distribution between high and low crime severity. The orange and blue bars are more evenly distributed, suggesting a mix of both high and low severity crimes.\nThis could indicate that while these states experience some severe crimes, they also have a significant portion of lower-severity crimes, perhaps petty crimes or less violent offenses.\n\nStates with Predominantly Low Crime Rates:\n\nStates like Nebraska, Vermont, and Wyoming show higher low-crime severity (with larger orange bars), indicating that these states experience fewer high-severity crimes. The majority of crimes in these states are less severe, suggesting generally safer environments.\nThese states may prioritize law enforcement for less severe crimes or have social factors that mitigate more violent crimes.\n\nRegional Trends:\n\nSouthern states such as Alabama, Louisiana, and Mississippi show higher overall crime severity, with high crime significantly outweighing low crime.\nNorthern and Midwestern states like Minnesota, Michigan, and Vermont seem to have much lower crime severity, especially in terms of high-severity crimes.\n\nTotal Crime Severity:\n\nStates like California, Florida, Illinois, and Texas stand out for having relatively high overall crime severity, with significant portions of both high and low crime. These states have large populations, which might account for the overall crime volume.\nVermont and Wyoming, on the other hand, show the lowest overall crime severity, indicating these states are among the safest in the country in terms of crime levels.\n\n\n\n\n4.3.1.6.4.2 General Observations:\n\nHigh vs. Low Crime Split: The split between high and low crime provides a clear comparison of the types of crimes affecting each state. States with a larger high crime bar indicate more severe criminal issues, whereas those with larger low crime bars may experience primarily less serious offenses.\nOutliers: Some states, like Florida and Illinois, show both high levels of crime severity across the board, indicating that while they face challenges with serious crime, they also have a substantial amount of less severe crime.\n\nThis chart provides a comprehensive look at how crime severity varies across states, giving a clearer picture of the types of crimes that dominate in different regions.\n\n\n\n\n4.3.1.7 Conclusion\n\n4.3.1.7.1 Key Conclusions Based on Crime Trends and Patterns Across U.S. States:\n\nCorrelation Between Different Violent Crimes:\n\nAcross all charts, there is a consistent positive correlation between different types of violent crimes, especially between murder and assault. States with higher rates of one violent crime tend to have higher rates of others.\nAssault emerges as a dominant crime in most states, correlating strongly with other violent crimes such as murder and rape. This suggests that regions facing assault issues may also struggle with broader violent crime challenges.\n\nUrbanization and Crime:\n\nThere is a clear link between urban population size and higher crime rates, particularly with assault. States with larger urban populations tend to report more violent crimes. This trend is evident in the scatter plots comparing assault rates to urban population size.\nStates like Florida, California, and Texas, with their large urban centers, display both higher overall crime severity and more violent crime rates. This highlights the challenges faced by densely populated states in controlling violent crime.\n\nHigh and Low Crime Severity:\n\nCertain states, like Florida, Arizona, and North Carolina, consistently rank high in both violent crime rates and crime severity, indicating significant challenges in addressing both high- and low-severity crimes.\nConversely, states like Vermont, Wyoming, and North Dakota show very low crime severity and lower violent crime rates overall, suggesting safer environments with fewer serious criminal offenses.\n\nRegional Crime Trends:\n\nSouthern and Western states tend to face greater crime challenges, particularly with violent crimes like assault and murder. These regions often rank higher in both crime rates and severity.\nMidwestern and Northern states such as Vermont, Iowa, and Minnesota typically report lower violent crime rates and severity, suggesting that these areas may benefit from a combination of social stability, effective law enforcement, or less urbanization.\n\n\n\n\n4.3.1.7.2 Final Conclusion:\nStates with larger urban populations, especially in the South and West, experience higher crime rates, particularly violent crimes like assault and murder. Assault is a prevalent issue, often correlating with other violent crimes, and more densely populated states tend to face higher crime severity overall. On the other hand, more rural or less densely populated states, especially in the Midwest and North, experience fewer violent crimes and lower crime severity, making them generally safer.\nThis analysis highlights the significant role that urbanization, regional differences, and socio-economic factors play in shaping crime patterns across the U.S.\n\n\n\n\n4.3.2 Friday\nWorking off of the NCES data, here’s an overview of the dataset.\n\n4.3.2.0.1 Dataset - HSLS Data\nThe dataset is drawn from the High School Longitudinal Study of 2009 (HSLS:09), which tracks a cohort of students who entered 9th grade in the fall of 2009. The study captures detailed data on students’ academic achievements, interests, attitudes, and family and school characteristics, with a particular focus on STEM (Science, Technology, Engineering, Mathematics) fields. The dataset includes measures on math and science performance, parental occupation, student demographics, and self-reported attitudes towards STEM subjects.\n\n\n4.3.2.1 Data Source\n\nSource: The dataset originates from the National Center for Education Statistics (NCES), a U.S. government organization that collects and analyzes educational data. The data is publicly available through NCES’s website and is widely used for educational research and policy evaluation.\nSurvey: HSLS:09, a comprehensive survey of U.S. high school students with follow-ups tracking their progress through high school, into postsecondary education, and into the workforce.\n\n\n\n4.3.2.2 30 Columns and Randomized Data\nWe meticulously selected 30 columns and selected 250 rows of randomized data representing key aspects of STEM participation, demographic factors, academic performance, and parental influence to analyze the impact of various factors on students’ STEM trajectories. These columns were chosen to enable analysis of the relationships between student performance in STEM subjects, family background, self-perception, and eventual aspirations in STEM careers.\n\n\n4.3.2.3 The 5 W’s Breakdown\n\n4.3.2.3.1 1. Who (Who does the data represent?)\n\nWho: The data represents students who began 9th grade in 2009 in public and private schools across the United States. It includes information on the students, their families, teachers, and schools.\nPopulation: The dataset covers a diverse cross-section of U.S. students, including different races, income levels, genders, and academic achievements.\n\n\n\n4.3.2.3.2 2. When (When was the data collected?)\n\nWhen: The baseline data collection occurred in 2009, with follow-up surveys conducted later as the students progressed through high school and beyond. This timeline allows us to observe students’ academic growth and career trajectories.\nSpecific Timeframes:\n\nBaseline: Fall 2009 (9th grade).\nFollow-up waves: Conducted in subsequent years to track academic progress and STEM involvement.\n\n\n\n\n4.3.2.3.3 3. Where (Where was the data collected?)\n\nWhere: The dataset covers schools in the United States across various regions, including urban, suburban, and rural areas. The dataset includes school-level identifiers, allowing for analysis of geographic or school-specific trends in STEM education.\nSchools: Public and private schools in different locales (urban, suburban, and rural).\n\n\n\n4.3.2.3.4 4. What (What does the dataset include?)\n\nWhat: The dataset includes a wide range of variables on student academic performance, demographic information, family background, school resources, and student attitudes towards STEM subjects.\n\nKey Data Fields:\n\nDemographics: Student gender (X1SEX), race/ethnicity (X1RACE), family income (X1FAMINCOME), and parental education (X1PAREDU).\nSTEM Participation: Interest in math (X1MTHINT) and science (X1SCIINT), math self-efficacy (X1MTHEFF), parental occupation in STEM fields (X1PAR1OCC_STEM1), and more.\nPerformance: Math and science test scores (X1TXMTH, X1SCIEFF), proficiency levels (X1TXMPROF1 to X1TXMPROF5), and school engagement (X1SCHOOLENG).\nParental Influence: Parental occupation and education levels (X1PAREDU, X1PAR1OCC_STEM1).\n\n\n\n\n\n4.3.2.3.5 5. Why (Why was the data collected? Why are we analyzing these columns?)\n\nWhy: The HSLS:09 dataset was collected to understand how students’ academic experiences, particularly in STEM fields, evolve over time. It explores how family background, school resources, and self-perceptions affect students’ performance and career aspirations, especially in STEM-related fields.\n\nPurpose of Analysis:\n\nTo analyze: The chosen columns allow us to study the relationship between student demographics (gender, race), academic performance in math and science, and future STEM aspirations.\nTo uncover: How parental STEM occupations or educational backgrounds influence students’ interest and success in STEM, and whether specific demographic groups are underrepresented in STEM participation.\n\n\n\n\n\n4.3.2.3.6 6. How (How is the data structured and how will we use it?)\n\nHow: The dataset is structured into variables representing both categorical (e.g., gender, race) and continuous (e.g., math scores, family income) data. The data is collected from students, their parents, schools, and teachers.\n\nData Analysis:\n\nWe will use Tableau for visualizations that can help correlate STEM performance with demographic variables, family influences, and student self-efficacy in math and science.\nCalculated fields and parameters in Tableau will be used to explore relationships such as math/science interest over different income levels, race, and gender.\n\n\n\n\n\n\n4.3.2.4 Use of the Data\n\nHere are some questions we’ll be answering:\n\nAre students from higher-income families more likely to pursue STEM careers?\nHow does parental occupation in STEM fields influence a student’s math and science performance?\nWhat are the disparities in STEM participation across different racial/ethnic groups?\nHow does self-efficacy in math affect student interest in STEM subjects?\n\n\n\n\n4.3.2.5 Conclusion\nThe chosen 30 columns provide a comprehensive view of student, family, and school characteristics, making it possible to explore STEM participation trends through factors such as demographics, academic performance, family influence, and self-perception. By leveraging these insights, we aim to identify gaps in STEM education and propose strategies to increase participation among underrepresented groups.\n\n4.3.2.5.1 Data Columns Chosen\nBefore we move on to the tableau dashboard, here are the columns chosen:\n\n4.3.2.5.1.1 1. SCH_ID – School ID\n\nRange: Unique numeric identifiers for each school.\nInterpretation: Each ID corresponds to a specific school, enabling differentiation in the dataset.\n\n\n\n4.3.2.5.1.2 2. X1NCESID – School Identification Number from CCD or PSS\n\nRange: Numeric IDs assigned to schools from official databases.\nInterpretation: Identifies schools using official government databases like the Common Core of Data (CCD).\n\n\n\n4.3.2.5.1.3 3. W1MATHTCH – Base Year Math-Course Enrollee Weight\n\nRange: Continuous values (e.g., 200-500).\nInterpretation: Weight applied to students enrolled in math courses to ensure the sample is representative.\n\n\n\n4.3.2.5.1.4 4. W1SCITCH – Base Year Science-Course Enrollee Weight\n\nRange: Continuous values (e.g., 150-450).\nInterpretation: Weight applied to students enrolled in science courses for representativeness.\n\n\n\n4.3.2.5.1.5 5. W3HSTRANS – High School Transcript Weight\n\nRange: Continuous values.\nInterpretation: A weight used for students included in the high school transcript study, ensuring representativeness.\n\n\n\n4.3.2.5.1.6 6. X1SEX – Student’s Sex\n\nRange: 1 = Male, 2 = Female.\nInterpretation: Binary variable representing the student’s gender.\n\n\n\n4.3.2.5.1.7 7. X1RACE – Student’s Race/Ethnicity Composite\n\nRange: Categorical:\n\n1 = White\n2 = Black or African American\n3 = Hispanic or Latino\n4 = Asian\n5 = Native Hawaiian/Pacific Islander\n6 = American Indian/Alaska Native\n7 = Two or More Races\n\nInterpretation: Represents the student’s racial/ethnic background.\n\n\n\n4.3.2.5.1.8 8. X1TXMTH – Math Theta Score\n\nRange: Continuous (typically -1.5 to 3.5).\nInterpretation: Measures math ability based on a standardized test. Higher scores reflect higher ability.\n\n\n\n4.3.2.5.1.9 9. X1TXMSEM – Math Standard Error of Measurement (SEM)\n\nRange: Continuous (e.g., 0.1-0.4).\nInterpretation: Indicates the degree of uncertainty or error in the math theta score.\n\n\n\n4.3.2.5.1.10 10. X1TXMSCR – Math IRT-Estimated Number Right Score\n\nRange: Continuous (e.g., 20-80).\nInterpretation: Estimated number of correct answers a student would have obtained on a math test.\n\n\n\n4.3.2.5.1.11 11. X1TXMTSCOR – Math Standardized Theta Score\n\nRange: Continuous, similar to X1TXMTH.\nInterpretation: Standardized measure of math ability across students in the dataset.\n\n\n\n4.3.2.5.1.12 12. X1TXMQUINT – Math Quintile Score\n\nRange: 1 to 5.\nInterpretation: Divides students into quintiles based on their math performance, with 5 representing the highest achievers.\n\n\n\n4.3.2.5.1.13 13. X1TXMPROF1 – Math Proficiency Probability (Basic Math Skills)\n\nRange: Continuous (0 to 1).\nInterpretation: Probability that the student is proficient in basic math skills.\n\n\n\n4.3.2.5.1.14 14. X1TXMPROF2 – Math Proficiency Probability (Simple Equations)\n\nRange: Continuous (0 to 1).\nInterpretation: Probability that the student is proficient in solving simple equations.\n\n\n\n4.3.2.5.1.15 15. X1TXMPROF3 – Math Proficiency Probability (Multi-Step Equations)\n\nRange: Continuous (0 to 1).\nInterpretation: Probability that the student is proficient in solving multi-step equations.\n\n\n\n4.3.2.5.1.16 16. X1TXMPROF4 – Math Proficiency Probability (Algebra)\n\nRange: Continuous (0 to 1).\nInterpretation: Probability that the student is proficient in algebra.\n\n\n\n4.3.2.5.1.17 17. X1TXMPROF5 – Math Proficiency Probability (Advanced Algebra)\n\nRange: Continuous (0 to 1).\nInterpretation: Probability that the student is proficient in advanced algebra.\n\n\n\n4.3.2.5.1.18 18. X1PAR1OCC_STEM1 – Parent 1’s STEM Occupation (Type 1)\n\nRange: 1 = Yes, -5 = Not Applicable.\nInterpretation: Indicates if Parent 1 works in a STEM-related occupation.\n\n\n\n4.3.2.5.1.19 19. X1PAR1OCC_STEM2 – Parent 1’s STEM Occupation (Type 2)\n\nRange: 1 = Yes, -5 = Not Applicable.\nInterpretation: A secondary category for Parent 1’s STEM-related occupation.\n\n\n\n4.3.2.5.1.20 20. X1PAR2OCC_STEM1 – Parent 2’s STEM Occupation (Type 1)\n\nRange: 1 = Yes, -5 = Not Applicable.\nInterpretation: Indicates if Parent 2 works in a STEM-related occupation.\n\n\n\n4.3.2.5.1.21 21. X1PAR2OCC_STEM2 – Parent 2’s STEM Occupation (Type 2)\n\nRange: 1 = Yes, -5 = Not Applicable.\nInterpretation: A secondary category for Parent 2’s STEM-related occupation.\n\n\n\n4.3.2.5.1.22 22. X1PAREDU – Parent’s Highest Level of Education\n\nRange: Categorical:\n\n1 = Less than high school\n2 = High school diploma\n3 = Some college\n4 = Associate’s degree\n5 = Bachelor’s degree\n6 = Master’s degree\n7 = Doctorate or professional degree\n\nInterpretation: Highest level of education attained by the parents.\n\n\n\n4.3.2.5.1.23 23. X1FAMINCOME – Family Income (2008)\n\nRange: Categorical:\n\n1 = Less than $10,000\n10 = $90,000 or more\n\nInterpretation: Family’s total income in 2008, grouped into categories.\n\n\n\n4.3.2.5.1.24 24. X1MTHID – Math Identity (Self-Perception in Math)\n\nRange: Continuous (-2 to 2).\nInterpretation: Reflects how much a student identifies with being “good at math” or sees math as part of their identity.\n\n\n\n4.3.2.5.1.25 25. X1MTHUTI – Perceived Math Utility\n\nRange: Continuous (-2 to 2).\nInterpretation: Measures how useful the student perceives math to be in their life.\n\n\n\n4.3.2.5.1.26 26. X1MTHEFF – Math Self-Efficacy\n\nRange: Continuous (-2 to 2).\nInterpretation: Reflects the student’s confidence in their math abilities.\n\n\n\n4.3.2.5.1.27 27. X1MTHINT – Interest in Math\n\nRange: Continuous (-2 to 2).\nInterpretation: Measures the student’s interest in math as a subject.\n\n\n\n4.3.2.5.1.28 28. X1SCIUTI – Perceived Science Utility\n\nRange: Continuous (-2 to 2).\nInterpretation: How useful the student perceives science to be in their life.\n\n\n\n4.3.2.5.1.29 29. X1SCIEFF – Science Self-Efficacy\n\nRange: Continuous (-2 to 2).\nInterpretation: Reflects the student’s confidence in their science abilities.\n\n\n\n4.3.2.5.1.30 30. X1SCIINT – Interest in Science\n\nRange: Continuous (-2 to 2).\nInterpretation: Measures the student’s interest in science.\n\n\n\n\n4.3.2.5.2 Tableau Dashboard\nHere’s the dashboard I created on Tableau using the data.\n                   \n\n\n\n4.3.2.5.3 Student Sex VS Parent STEM Occupation Visualization\n\n\n\nStudent Sex VS Parent STEM Occupation\n\n\nThis visualization illustrates the relationship between the gender of students and the type of STEM occupation their parents have. Below are the key insights derived from the visualization:\n\n4.3.2.5.3.1 Key Insights:\n\nParent Occupation Type 0 Dominates:\n\nThe majority of students have parents in Occupation Type 0, where most data points are concentrated.\nWithin this category, there is a significant count of both male and female students (represented by different shades of brown and orange), with a relatively balanced distribution between genders.\n\nSparse Representation in Other Occupation Types:\n\nFor Occupation Types 4 and 5, there is a much smaller number of students, indicating that fewer parents in these categories work in STEM occupations.\nThe counts are notably lower, especially when compared to Occupation Type 0.\n\nGender Distribution:\n\nThe gender distribution appears relatively even in the dominant category (Occupation Type 0). However, as we move to other occupation types, the data becomes sparser, making it harder to discern strong gender-based trends.\nIn Occupation Type 4, there seems to be a slight skew towards male students, although the sample size is relatively small.\n\nImplications for Parent Influence:\n\nThe strong presence of students associated with Parent Occupation Type 0 suggests that most students have parents who may not be specifically involved in STEM occupations or belong to a general category that includes STEM-adjacent roles.\nThe smaller representation in other occupation types might indicate that parents in more specialized STEM roles have fewer children, or those children are underrepresented in this dataset.\n\n\n\n\n4.3.2.5.3.2 Conclusion:\nThis visualization highlights the strong influence of parents in Occupation Type 0 on students, regardless of gender. There is a relatively balanced gender representation in this dominant category, but in the smaller STEM-specific occupation categories, there are fewer students overall. This could point to either a lack of representation in STEM fields for certain types of occupations or simply fewer parents in these roles.\n\n\n\n4.3.2.5.4 Race VS Math Score Visualization\n\n\n\nRace VS Math Score\n\n\nIn this visualization, we analyze the performance of various racial groups on their average math scores using the provided race categories. Here’s the breakdown based on the race list:\n\n4.3.2.5.4.1 Insights:\n\nRace 4 (Hispanic, no race specified):\n\nHispanic (no race specified) students show the most significant negative deviation, with an average math score well below zero. This indicates that this group is underperforming significantly in math compared to others.\nThe drop in math scores for this group may highlight educational or socioeconomic disparities affecting their performance.\n\nRace 2 (Asian, non-Hispanic):\n\nAsian, non-Hispanic students are performing slightly above average in math. This group shows a small positive deviation, indicating that they are outperforming other racial groups on average.\nThis aligns with general trends observed in educational datasets where Asian students often excel in math-related subjects.\n\nRace 3 (Black/African-American, non-Hispanic):\n\nBlack/African-American, non-Hispanic students show a small negative deviation from the average. While they underperform in math compared to other racial groups, the deviation is not as drastic as the Hispanic (no race specified) group.\nThis performance may reflect challenges related to access to resources, educational opportunities, or broader social factors.\n\nRace 8 (White, non-Hispanic):\n\nWhite, non-Hispanic students show minimal deviation from the average math score, suggesting that their performance is close to the overall mean.\nThis indicates that White, non-Hispanic students are generally performing at or around the national average in math.\n\nRace 5 (Hispanic, race specified):\n\nThere are fewer significant trends for Race 5, but there may be a smaller negative deviation for Hispanic students whose race is specified.\nThis suggests that while both groups of Hispanic students (Race 4 and 5) underperform, those with a specified race perform slightly better than those with an unspecified race.\n\n\n\n\n4.3.2.5.4.2 General Observations:\n\nHispanic Students (Race 4) are struggling the most with math performance, while Asian Students (Race 2) show the best results, performing above average.\nBlack/African-American students (Race 3) perform slightly below average, while White students (Race 8) remain close to the average performance.\nThe visualization highlights racial disparities in math education and suggests potential areas for further investigation, such as how socio-economic status or educational resources may impact these differences.\n\n\n\n4.3.2.5.4.3 Conclusion:\nThis analysis sheds light on how different racial groups perform in math and suggests that targeted interventions may be needed to address the performance gap, particularly for Hispanic students.\n\n\n\n4.3.2.5.5 Parent Education Level VS STEM Interest (Math & Science) Visualization\n\n\n\nParent Education Level VS STEM Interest (Math)\n\n\n\n\n\nParent Education Level VS STEM Interest (Science)\n\n\nThis analysis focuses on two scatter plot visualizations: Math Interest and Science Interest. Each visualization shows the relationship between students’ interest in STEM subjects (math and science) and their parent’s highest level of education.\n\n4.3.2.5.5.1 Parent Education Levels:\n\n1: Less than high school\n2: High school diploma or GED\n3: Associate’s degree\n4: Bachelor’s degree\n5: Master’s degree\n7: Ph.D./M.D./Law/Other high-level professional degree\n\n\n\n4.3.2.5.5.2 Math Interest:\nIn the first visualization, the vertical axis represents the calculated/standardized interest in math, while the horizontal axis shows the parent’s highest level of education. Higher values on the vertical axis represent greater interest in math.\n\nKey Insights:\nConcentration Around GED and Bachelor’s Degree (Levels 2 and 4):\n\nThere is a notable concentration of data points at education levels 2 (high school diploma or GED) and 4 (bachelor’s degree), suggesting that most students with parents at these education levels show a range of math interest values.\nThe majority of the data points are spread between -0.5 and +0.5, suggesting average math interest, regardless of parental education level for these categories.\n\nOutliers in PhD/Professional Degree (Level 7):\n\nA few outliers show extremely high interest in math among students whose parents have achieved higher professional degrees (level 7). This indicates that students with highly educated parents may show greater interest in math.\nHowever, there are also students in this category with negative math interest, indicating that parent education alone may not be a definitive predictor of math interest.\n\nOverall Spread:\n\nMost students fall between -1 to +1 on the math interest scale, with no dramatic spikes or drops. The relationship between parent education and math interest appears moderate, but students with higher-educated parents (levels 4-7) show more positive trends.\n\n\n\n\n4.3.2.5.5.3 Science Interest:\nIn the second visualization, the vertical axis represents science interest, and the horizontal axis shows parent education levels. The structure is similar to the math interest plot, with higher values representing greater interest in science.\n\nKey Insights:\nBroad Spread of Interest:\n\nThe science interest values show a broader distribution compared to math, with many students clustered between -0.5 to 0.5, but also some outliers reaching up to +2.\nSimilar to math interest, parent education levels 2 (GED) and 4 (bachelor’s degree) dominate in terms of data points.\n\nMore Positive Interest for Higher Parent Education:\n\nStudents with parents holding higher degrees (levels 4-7) generally display positive science interest. Many fall near the average, but there is a cluster of students with significantly higher interest.\nStudents with parents holding Ph.D. or professional degrees (level 7) also show a range of interest, with some outliers exhibiting higher science interest.\n\nLess Impact for Lower Education Levels:\n\nThe lower education levels (1 to 3) have fewer students with high interest in science. The interest levels tend to hover around the zero mark, suggesting neutral or average interest in science.\n\n\n\n\n4.3.2.5.5.4 General Observations:\n\nHigher parental education (levels 4-7) tends to correlate with slightly increased interest in both math and science, particularly for students whose parents hold bachelor’s degrees or higher.\nWhile there is a concentration of average interest in both subjects, the presence of outliers shows that highly educated parents may foster greater interest in STEM.\nLower education levels (1 to 3) correspond with more neutral or average interest in both subjects. There isn’t a dramatic decline in interest among students from these education backgrounds, but the positive trend is more prominent among higher-educated parents.\n\n\n\n4.3.2.5.5.5 Conclusion:\nThe scatter plots show a moderate but positive relationship between parent education levels and student interest in STEM fields. Higher parental education levels are associated with greater interest in math and science, particularly for parents holding bachelor’s degrees or higher. However, the data also suggests that parent education level alone is not the sole determinant of student interest in STEM.\n\n\n\n4.3.2.5.6 Family Income VS Math Proficiency Visualization\n\n\n\nFamily Income VS Math Proficiency\n\n\nThis analysis focuses on the relationship between family income and math proficiency using a treemap. Below are insights based on the income categories provided.\n\n4.3.2.5.6.1 Income Categories:\n\nCategory 1: Family income ≤ $15,000\nCategory 2: Family income &gt; $15,000 and ≤ $35,000\nCategory 3: Family income &gt; $35,000 and ≤ $55,000\nCategory 4: Family income &gt; $55,000 and ≤ $75,000\nCategory 5: Family income &gt; $75,000 and ≤ $95,000\nCategory 6: Family income &gt; $95,000 and ≤ $115,000\nCategory 7: Family income &gt; $115,000 and ≤ $135,000\nCategory 8: Family income &gt; $135,000 and ≤ $155,000\nCategory 9: Family income &gt; $155,000 and ≤ $175,000\nCategory 10: Family income &gt; $175,000 and ≤ $195,000\nCategory 11: Family income &gt; $195,000 and ≤ $215,000\nCategory 12: Family income &gt; $215,000 and ≤ $235,000\nCategory 13: Family income &gt; $235,000\n\n\n\n4.3.2.5.6.2 Key Insights:\n\nHigher Income Groups:\n\nCategories 9, 5, and 10 (representing higher-income brackets between $75,000 and $195,000) occupy larger areas in the treemap and are shaded in blue, representing higher overall proficiency in math.\nFamilies with incomes higher than $75,000 are associated with better math proficiency, with category 9 (income &gt; $155,000 and ≤ $175,000) standing out for better performance.\n\nLower Income Groups:\n\nCategories 1, 2, 3, and 4 (representing family incomes up to $75,000) are shown in orange and red, indicating lower math proficiency scores.\nCategory 1 (income ≤ $15,000) shows the lowest math proficiency, suggesting a clear relationship between lower family income and lower math proficiency.\n\nModerate Income Groups:\n\nCategories like 6, 7, and 8 (income &gt; $95,000 and ≤ $155,000) are shown in light blue, indicating moderate proficiency in math. Students from these income levels perform better than those in the lower income brackets but slightly worse than those in the highest income categories.\n\nTop Income Category:\n\nCategory 13 (family income &gt; $235,000) is shaded in light blue, showing that even at the very top income level, proficiency is high but not the highest. This suggests that beyond a certain income threshold, the impact of family income on math proficiency levels off.\n\n\n\n\n4.3.2.5.6.3 General Observations:\n\nStrong Positive Correlation Between Family Income and Math Proficiency:\n\nHigher family income is clearly associated with better performance in math. As family income increases, students’ math proficiency improves, indicating that socioeconomic factors such as access to resources, tutoring, or educational opportunities may play a significant role.\n\nLower Income Students Struggle More:\n\nStudents from lower-income families (especially in categories 1 through 4) tend to have lower proficiency in math. These students may face challenges in education due to limited access to resources or support outside the classroom.\n\n\n\n\n4.3.2.5.6.4 Conclusion:\nThe treemap visualization emphasizes the importance of family income in determining math proficiency. Students from higher-income families generally perform better in math, while those from lower-income brackets tend to struggle. This suggests that addressing socioeconomic disparities could have a significant impact on improving math proficiency among students from lower-income families.\n\n\n\n4.3.2.5.7 Student Sex VS Math Self-Efficacy Visualization\n\n\n\nStudent Sex VS Math Self-efficacy\n\n\nIn this visualization, we analyze the distribution of math self-efficacy across student gender (1 = male, 2 = female) while also considering student race as an additional variable (indicated by color).\n\n4.3.2.5.7.1 Key Insights:\n\nGeneral Trends in Math Self-Efficacy:\n\nMales (Student Sex = 1) show a broad range of math self-efficacy scores, with values ranging from -100 to +20. There is more spread across the scale, with both high and low self-efficacy scores represented.\nFemales (Student Sex = 2) display a similar distribution in math self-efficacy, ranging from -110 to +20, suggesting that both genders have a similar overall spread of math self-efficacy.\n\nRace Influence on Self-Efficacy:\n\nRace categories are depicted using colors. The blue-shaded students (higher values like White/Asian) tend to show higher math self-efficacy, particularly at the higher end of the scale.\nStudents from Race 3 (Black/African-American) and Race 2 (Asian) show different shades, with Race 2 students more represented in the higher math self-efficacy scores (above zero).\nLower self-efficacy values (below -50) tend to be associated with orange/red-colored races (Hispanic), highlighting potential challenges faced by these racial groups in math.\n\nComparison Between Genders:\n\nThe visualization shows no drastic differences between male and female students when it comes to math self-efficacy. Both genders have students with high self-efficacy (above 0) and low self-efficacy (below -50).\nThere is no clear distinction in the overall distribution of self-efficacy between genders, implying that gender might not be the key differentiating factor in math self-confidence.\n\nConcentration Around Zero:\n\nBoth male and female students have a significant concentration of self-efficacy scores around the zero mark, representing average self-efficacy. This suggests that most students, regardless of gender or race, report a standard level of math confidence.\n\n\n\n\n4.3.2.5.7.2 General Observations:\n\nRacial Disparities: Race plays a significant role in shaping math self-efficacy. Students from traditionally higher-performing racial groups in math (such as Asians and Whites) tend to show higher levels of math self-efficacy, while those from Hispanic and other groups tend to struggle more.\nGender Parity in Math Confidence: There is no clear, overwhelming gender gap in math self-efficacy. While individual students may vary, both male and female students show similar distributions across the self-efficacy scale.\n\n\n\n4.3.2.5.7.3 Conclusion:\nThis visualization highlights the complex interplay between gender, race, and math self-efficacy. While race appears to have a more noticeable impact on self-efficacy, gender differences are not as pronounced. To improve math self-efficacy among students, targeted interventions might need to focus more on supporting students from racial groups that show lower confidence in their math abilities.\n\n\n\n4.3.2.5.8 Conclusion: Insights from the Visualizations on STEM, Math, and Education Factors\nThe analysis of the last six visualizations offers a comprehensive view of how various factors like gender, race, parental education, family income, and math self-efficacy contribute to student outcomes in math and STEM-related areas.\n\n4.3.2.5.8.1 1. Family Income vs Math Proficiency:\n\nStrong Positive Correlation: Family income has a strong positive correlation with math proficiency. Students from higher-income families (&gt; $75,000) consistently perform better in math, while students from lower-income brackets (&lt; $35,000) tend to struggle.\nSocioeconomic Advantages: The socioeconomic advantages of higher-income families likely translate into better access to educational resources, tutoring, and enrichment programs, contributing to these higher math scores.\nSystemic Issue: This highlights a systemic issue where economic disparity directly impacts student performance, suggesting the need for greater support for students from lower-income families to bridge this gap.\n\n\n\n4.3.2.5.8.2 2. Parent Education vs STEM Interest:\n\nParental Education Influence: Parental education level plays a notable role in fostering STEM interest. Students whose parents have higher degrees (bachelor’s and above) tend to show increased interest in math and science.\nIncreased Interest: Students whose parents hold advanced degrees (e.g., Ph.D. or other professional degrees) show higher-than-average interest in STEM, reflecting how parental education influences children’s engagement in academic pursuits.\n\n\n\n4.3.2.5.8.3 3. Race vs Math Proficiency:\n\nRacial Disparities: Race is a significant factor in math proficiency, with students from traditionally underserved racial groups (Hispanic, Black/African-American) often underperforming compared to their White and Asian peers.\nPerformance Gaps: Hispanic students, in particular, show the lowest proficiency levels, while Asian students perform above the average. These disparities suggest that racial inequities in education—driven by factors like access to resources and systemic barriers—still persist.\n\n\n\n4.3.2.5.8.4 4. Student Sex vs Math Self-Efficacy:\n\nNo Pronounced Gender Difference: Gender does not show a pronounced difference in math self-efficacy, with both male and female students displaying similar confidence levels in their math abilities.\nRace’s Impact: Race plays a more crucial role, with White and Asian students showing higher math self-efficacy, while Hispanic students often show lower confidence.\nChallenges Remain: While gender stereotypes around math may be weakening, racial disparities in self-efficacy still present a challenge, impacting how students view their ability to succeed in math.\n\n\n\n4.3.2.5.8.5 5. Parent STEM Occupation Influence on Student Gender:\n\nNo Major Gender Difference: There is no significant gender difference in student representation across parental STEM occupations. However, there are fewer students overall with parents working in STEM fields.\nBroader Societal Influences: The influence of a parent’s STEM occupation appears to be less significant regarding gender representation, suggesting that other factors—such as societal influences—may play a more dominant role in shaping STEM career aspirations.\n\n\n\n4.3.2.5.8.6 Holistic Observations:\nAcross the board, these visualizations reveal that socioeconomic status and race are the most influential factors shaping math performance, STEM interest, and self-efficacy among students. While parental education and income provide advantages, it is clear that racial inequities in education still present significant barriers to success, particularly for Hispanic and Black students.\nGender appears to have less of an impact on math outcomes compared to race and socioeconomic factors, as both male and female students demonstrate similar levels of math self-efficacy and interest in STEM fields.\n\n\n4.3.2.5.8.7 Recommendations for Educational Improvement:\n\nTargeted Support for Lower-Income and Underrepresented Students:\n\nAccess to tutoring, educational resources, and extracurricular STEM programs should be increased, particularly for students from lower-income families and racial groups that tend to underperform.\n\nMentorship and Role Models:\n\nProviding mentorship and exposure to role models from similar racial and socioeconomic backgrounds in STEM fields can help boost self-efficacy and interest in math and science.\n\nParental Engagement and Support:\n\nEncouraging parental involvement and education programs for parents with lower educational attainment could help increase their children’s engagement and interest in math and STEM.\n\nAddressing Systemic Barriers:\n\nSchools and policymakers should continue to address systemic inequalities in education that disproportionately affect minority students, ensuring equal access to high-quality education and resources for all.\n\n\n\n\n4.3.2.5.8.8 Final Thoughts:\nThis comprehensive analysis emphasizes the need for systemic changes to level the playing field, ensuring that factors such as income and race no longer determine a student’s success in math and STEM fields.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>National Center for Education Statistics - Chris Jacob</span>"
    ]
  },
  {
    "objectID": "chris.html#week-4---0916-0920",
    "href": "chris.html#week-4---0916-0920",
    "title": "4  National Center for Education Statistics - Chris Jacob",
    "section": "4.4 Week 4 - 09/16 ~ 09/20",
    "text": "4.4 Week 4 - 09/16 ~ 09/20\n\n4.4.1 Monday/Wednesday\n\n4.4.1.1 This is markdown content\nHere is a list\n\nItem 1\nItem 2\nItem 3\n\nbold italic\n\n\n4.4.1.2 Jupyter Notebook - Python Basics\n# First we activate the virtual environment (venv477) - it has all the required libraries already downloaded and installed\n# In this code block, we'll be using numpy, a library to deal with numbers and complex calculations\n\nimport numpy as np\nprint(np.absolute(-1))\narr = np.array([1,2,3,4,5,])\nprint(arr)\n1\n[1 2 3 4 5]\n#lists are native to python\nmy_list = [1,2,3,4,5]\nprint(my_list)\n[1, 2, 3, 4, 5]\n\n\n4.4.1.3 Python Dataframes\n#For dataframes, we use pandas library, which will be used for handling large amounts of data.\nimport pandas as pd\ndata={'Ozone': [41,36,12],'Temp':[67,72,74]}\ndf = pd.DataFrame(data)\nprint(df)\nprint(df.describe())\nprint(df.info())\n   Ozone  Temp\n0     41    67\n1     36    72\n2     12    74\n           Ozone       Temp\ncount   3.000000   3.000000\nmean   29.666667  71.000000\nstd    15.502688   3.605551\nmin    12.000000  67.000000\n25%    24.000000  69.500000\n50%    36.000000  72.000000\n75%    38.500000  73.000000\nmax    41.000000  74.000000\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   Ozone   3 non-null      int64\n 1   Temp    3 non-null      int64\ndtypes: int64(2)\nmemory usage: 176.0 bytes\nNone\n\n\n4.4.1.4 Airquality Dataset\n\n4.4.1.4.1 Overview of the Airquality Dataset\nThe airquality dataset contains daily air quality measurements taken in New York from May to September 1973. It is frequently used for data analysis and visualization exercises. Below is a detailed overview of the dataset based on the image and general characteristics of the airquality dataset. Here is some analysis, with the help of ChatGPT.\n\n4.4.1.4.1.1 Dataset Structure:\nThe dataset consists of the following six columns: 1. Ozone: Mean ozone concentration in parts per billion (ppb). 2. Solar.R: Solar radiation in Langleys (ly). 3. Wind: Average wind speed in miles per hour (mph). 4. Temp: Maximum daily temperature in degrees Fahrenheit (°F). 5. Month: Coded as integers 5 (May) to 9 (September). 6. Day: The day of the month.\n\n\n4.4.1.4.1.2 Dataset Characteristics:\n\nThe dataset includes 153 observations (rows).\nSome columns contain missing values, especially for Ozone and Solar.R:\n\nOzone: 116 non-null entries, 37 missing (NaN).\nSolar.R: 146 non-null entries, 7 missing (NaN).\n\nThe Wind and Temp columns are complete, with no missing values.\nThe Month and Day columns represent the time variables and do not have missing values.\n\n\n\n4.4.1.4.1.3 Basic Statistics:\nHere are some basic statistics derived from the dataset:\n\nOzone:\n\nMean: 42.1 ppb\nMinimum: 1 ppb, Maximum: 168 ppb\n50th percentile (median): 31 ppb\n\nSolar.R:\n\nMean: 185.9 ly\nMinimum: 7 ly, Maximum: 334 ly\n50th percentile (median): 207 ly\n\nWind:\n\nMean: 9.96 mph\nMinimum: 1.7 mph, Maximum: 20.7 mph\n50th percentile (median): 9.7 mph\n\nTemp:\n\nMean: 77.88°F\nMinimum: 56°F, Maximum: 97°F\n50th percentile (median): 79°F\n\n\n\n\n4.4.1.4.1.4 Key Insights:\n\nOzone and Solar.R have missing data: The Ozone column has 37 missing values and Solar.R has 7 missing values, which may need to be handled for detailed analysis.\nTemperature and Wind show no missing values: These columns provide a full set of observations, which can be used for detailed statistical analysis without any data imputation.\nTemperature ranges: The daily maximum temperatures recorded range from 56°F to 97°F, typical for summer months in New York.\nOzone levels: The ozone concentration ranges from 1 ppb to 168 ppb, with a mean of 42.1 ppb, but there are notable missing values.\n\n\n\n4.4.1.4.1.5 Potential Analysis:\n\nCorrelation Analysis: Investigating how ozone levels relate to temperature, wind, and solar radiation.\nTime Series Analysis: Analyzing how air quality changes over time, such as by month and day.\nHandling Missing Data: Addressing missing values in Ozone and Solar.R before performing any advanced analysis to avoid bias.\n\n\n\n4.4.1.4.1.6 Conclusion:\nThe airquality dataset provides a rich set of environmental data that can be analyzed to explore the relationships between ozone levels, temperature, wind speed, and solar radiation. Missing data in Ozone and Solar.R will require careful handling to ensure meaningful insights. This dataset offers a great opportunity to explore seasonal air quality patterns and their environmental drivers.\n# Here we load the airquality dataset to start visualizing it.\nairquality_df = pd.read_csv('airquality_datasets.csv')\nprint(airquality_df) # This will print out a concise version of the dataset\n     Ozone  Solar.R  Wind  Temp  Month  Day\n0     41.0    190.0   7.4    67      5    1\n1     36.0    118.0   8.0    72      5    2\n2     12.0    149.0  12.6    74      5    3\n3     18.0    313.0  11.5    62      5    4\n4      NaN      NaN  14.3    56      5    5\n..     ...      ...   ...   ...    ...  ...\n148   30.0    193.0   6.9    70      9   26\n149    NaN    145.0  13.2    77      9   27\n150   14.0    191.0  14.3    75      9   28\n151   18.0    131.0   8.0    76      9   29\n152   20.0    223.0  11.5    68      9   30\n\n[153 rows x 6 columns]\nprint(airquality_df.info()) # This will print out the information of the dataframe.\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\nNone\nprint(airquality_df.describe()) # This will print out some basic stats on the dataset\n            Ozone     Solar.R        Wind        Temp       Month         Day\ncount  116.000000  146.000000  153.000000  153.000000  153.000000  153.000000\nmean    42.129310  185.931507    9.957516   77.882353    6.993464   15.803922\nstd     32.987885   90.058422    3.523001    9.465270    1.416522    8.864520\nmin      1.000000    7.000000    1.700000   56.000000    5.000000    1.000000\n25%     18.000000  115.750000    7.400000   72.000000    6.000000    8.000000\n50%     31.500000  205.000000    9.700000   79.000000    7.000000   16.000000\n75%     63.250000  258.750000   11.500000   85.000000    8.000000   23.000000\nmax    168.000000  334.000000   20.700000   97.000000    9.000000   31.000000\n\n\n\n\n4.4.1.5 Visualizations on the Airquality Dataset\n\n4.4.1.5.1 Distribution of Ozone Levels Histogram\n# Now we can start creating visualizations\nimport matplotlib.pyplot as plt # This import will be used for plotting\n\n# Ozone Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(airquality_df['Ozone'].dropna(), bins=20, color='blue', edgecolor='black') #dropna() will drop all NA values\nplt.title('Distribution of Ozone Levels')\nplt.xlabel('Ozone (ppb)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\nDistribution of Ozone Levels\n\n\nThe histogram visualizes the distribution of ozone levels (in parts per billion - ppb) from the dataset.\n\n4.4.1.5.1.1 Key Insights:\n\nSkewed Distribution:\n\nThe distribution of ozone levels is right-skewed. Most of the data points fall in the lower ozone ranges, particularly between 0 and 50 ppb.\nThere are fewer data points with higher ozone levels, with a long tail extending toward 175 ppb.\n\nHigh Frequency in Low Ozone Levels:\n\nThe highest frequency of ozone levels is concentrated between 0 and 25 ppb, where more than 20 occurrences are recorded.\nThis suggests that lower ozone levels are more common in the dataset.\n\nModerate Levels Between 25 and 50 ppb:\n\nThere is a significant drop in frequency between 25 and 50 ppb, with frequencies ranging between 10 to 15 occurrences.\nThese levels are still relatively common but less so than the lowest ozone levels.\n\nLow Frequency of High Ozone Levels:\n\nOzone levels greater than 50 ppb occur much less frequently.\nBetween 50 and 100 ppb, there are scattered occurrences, with frequencies generally below 10.\nBeyond 100 ppb, the frequencies drop dramatically, with very few data points between 100 and 175 ppb.\n\nOutliers in High Ozone Levels:\n\nThere are a few outliers with ozone levels exceeding 150 ppb, but these are rare occurrences, indicating that extremely high ozone levels are uncommon in this dataset.\n\n\n\n\n4.4.1.5.1.2 General Trends:\n\nThe data shows that low ozone levels dominate, while higher ozone levels are rare.\nThis type of distribution might reflect air quality patterns in the region where the data was collected, suggesting that low ozone is the norm, with fewer instances of high ozone pollution.\n\n\n\n4.4.1.5.1.3 Conclusion:\nThe histogram reveals that ozone levels are predominantly low, with a clear skew toward the lower end of the spectrum. The right-skew suggests that high ozone levels are infrequent, possibly indicating generally better air quality in the region of interest.\n\n\n\n4.4.1.5.2 Distribution of Temperature Levels Histogram\n# Temp Histogram\nplt.figure(figsize=(8, 6))\nplt.hist(airquality_df['Temp'].dropna(), bins=20, color='orange', edgecolor='black')\nplt.title('Distribution of Temperature')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\nDistribution of Temperature\n\n\nThis histogram visualizes the distribution of temperature (in °F) from the dataset.\n\n4.4.1.5.2.1 Key Insights:\n\nNormal Distribution with Slight Skew:\n\nThe distribution of temperatures appears to have a normal distribution with a slight right skew. The temperatures are mostly concentrated between 60°F and 90°F.\nThe peak temperature occurs around 80°F, with the highest frequency of over 20 occurrences.\n\nHigh Frequency at 80°F:\n\nThe highest frequency is at 80°F, where over 20 occurrences are recorded. This indicates that 80°F is a commonly observed temperature in the dataset.\n\nBalanced Spread Between 60°F and 90°F:\n\nTemperatures ranging from 60°F to 90°F are well represented, with a fairly even spread. Most temperatures in this range occur with moderate frequencies between 5 and 15 occurrences.\nThe distribution suggests that the temperature tends to fluctuate within this range, without extreme lows or highs.\n\nLow Frequency Below 60°F and Above 90°F:\n\nThere are fewer occurrences of temperatures below 60°F and above 90°F. These extremes are less common, and the dataset is largely concentrated within the 60°F to 90°F range.\nThe tail at the lower end (&lt; 60°F) suggests that colder temperatures are less frequent in this dataset.\n\nOutliers at Low and High Temperatures:\n\nThe lower and higher temperature extremes (below 60°F and above 90°F) are outliers in the dataset, as they occur with much lower frequency. This suggests that these temperatures are unusual in the context of this dataset.\n\n\n\n\n4.4.1.5.2.2 General Trends:\n\nThe dataset shows that temperatures between 70°F and 80°F are the most common, with 80°F being the peak.\nBoth lower temperatures (&lt; 60°F) and higher temperatures (&gt; 90°F) are uncommon, indicating that most temperature observations fall within a moderate range.\n\n\n\n4.4.1.5.2.3 Conclusion:\nThe temperature distribution in this dataset follows a fairly normal pattern, with a clear peak around 80°F. The histogram suggests that temperatures typically range between 60°F and 90°F, with a relatively low frequency of extreme temperatures. This distribution may indicate a moderate climate in the region where the data was collected.\n\n\n\n4.4.1.5.3 Boxplot of Ozone Levels\n# Boxplot for Ozone\nplt.figure(figsize=(8, 6))\nplt.boxplot(airquality_df['Ozone'].dropna())\nplt.title('Boxplot of Ozone Levels')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n\n\nBoxplot of Ozone Levels\n\n\nThis boxplot provides a summary of the distribution of ozone levels (in ppb - parts per billion), highlighting key statistics such as the median, interquartile range (IQR), and potential outliers.\n\n4.4.1.5.3.1 Key Insights:\n\nMedian and Interquartile Range (IQR):\n\n\nThe median ozone level is around 25 ppb, as indicated by the orange line inside the box.\nThe IQR (the box) ranges from approximately 20 ppb to 70 ppb, meaning that 50% of the data falls within this range.\n\n\n\n\n\n\n\nNote\n\n\n\nIQR, or Interquartile Range, is a measure of statistical dispersion and is used to describe the spread of data. It indicates the range within which the middle 50% of values in a dataset fall.\n\n\n\nThe lower quartile is just above 20 ppb, and the upper quartile is around 70 ppb, showing where the middle half of the ozone levels lie.\n\n\nWhiskers and Data Spread:\n\nThe lower whisker extends down to around 5 ppb, suggesting that the lowest ozone levels in the dataset are close to zero.\nThe upper whisker extends to about 125 ppb, indicating the highest ozone levels within the normal range (excluding outliers).\nThis spread indicates that the ozone levels vary widely, with most values lying between 5 ppb and 125 ppb.\n\nOutliers:\n\nThere are two outliers above the upper whisker, with values around 150 ppb and 175 ppb. These represent unusual, high ozone levels that are not common in the dataset.\nThe presence of these outliers suggests that while most ozone readings are moderate, there are some instances of much higher ozone levels.\n\nSymmetry of the Distribution:\n\nThe distribution appears to be slightly right-skewed, as the upper whisker is longer than the lower whisker and there are high-value outliers.\nThis implies that while most ozone levels are concentrated at lower values, there are occasional higher levels.\n\n\n\n\n4.4.1.5.3.2 General Trends:\n\nModerate Ozone Levels: The data shows that most ozone levels fall between 20 and 70 ppb, with a median of about 25 ppb, suggesting that moderate ozone levels are the norm.\nOutliers Indicating High Ozone Events: The outliers above 150 ppb indicate that there are occasional high ozone events, which could be of interest for further investigation (e.g., environmental factors causing ozone spikes).\n\n\n\n4.4.1.5.3.3 Conclusion:\nThe boxplot of ozone levels reveals that the distribution is slightly skewed to the right, with most values falling between 20 and 70 ppb. The median is around 25 ppb, and there are two notable outliers with much higher ozone levels, pointing to occasional extreme events. This provides a useful overview of ozone concentration variability and the presence of high-level ozone outliers in the dataset.\n\n\n\n4.4.1.5.4 Boxplot of Temperature Levels\n# Boxplot for Temp\nplt.figure(figsize=(8, 6))\nplt.boxplot(airquality_df['Temp'].dropna())\nplt.title('Boxplot of Temperature')\nplt.ylabel('Temperature (°F)')\nplt.show()\n\n\n\nBoxplot of Temperature\n\n\nThis boxplot provides a summary of the distribution of temperature (in °F), highlighting key statistics such as the median, interquartile range (IQR), and overall temperature spread.\n\n4.4.1.5.4.1 Key Insights:\n\nMedian and Interquartile Range (IQR):\n\nThe median temperature is approximately 80°F, as indicated by the orange line inside the box.\nThe IQR (the box) ranges from about 70°F to 85°F, meaning that 50% of the data falls within this range.\nThe lower quartile (25th percentile) is just above 70°F, while the upper quartile (75th percentile) is around 85°F, indicating where the middle half of the temperature data lies.\n\nWhiskers and Data Spread:\n\nThe lower whisker extends down to around 60°F, suggesting that the lowest temperatures in the dataset are close to this value.\nThe upper whisker extends to just under 95°F, indicating the highest temperatures within the normal range.\nThis spread shows that temperatures generally range between 60°F and 95°F, with most values concentrated around the median of 80°F.\n\nAbsence of Outliers:\n\nThere are no outliers in this dataset, meaning that all temperature observations fall within the expected range without any extreme values.\nThe data appears well-contained between 60°F and 95°F, suggesting a consistent temperature range.\n\nSymmetry of the Distribution:\n\nThe boxplot demonstrates a fairly symmetric distribution, with the median line positioned centrally within the IQR. This suggests a balanced temperature distribution.\nThe whiskers are of similar length, indicating that there is no extreme skewness in the temperature data.\n\n\n\n\n4.4.1.5.4.2 General Trends:\n\nModerate Temperature Range: The data shows that most temperature observations fall between 70°F and 85°F, with a median temperature of about 80°F.\nWell-Distributed Temperatures: The absence of outliers and the symmetry of the boxplot suggest that the temperatures in the dataset are fairly consistent and fall within a moderate range.\n\n\n\n4.4.1.5.4.3 Conclusion:\nThe temperature distribution in this dataset follows a normal and balanced pattern, with no outliers and a clear concentration around the median of 80°F. The data suggests that temperatures typically range between 60°F and 95°F, with most values within the moderate range of 70°F to 85°F. This indicates a stable and consistent temperature pattern in the dataset.\n\n\n\n4.4.1.5.5 Scatter plot of Temperature and Ozone by Month\nimport seaborn as sns\n\n# Scatter plot with regression line\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Temp', y='Ozone', hue='Month', data=airquality_df)\nplt.title('Temperature vs Ozone Levels by Month')\nplt.xlabel('Temperature (°F)')\nplt.ylabel('Ozone (ppb)')\nplt.show()\n\n\n\nTemperature vs Ozone Levels by Month\n\n\nThis scatter plot visualizes the relationship between temperature (°F) and ozone levels (ppb), while also distinguishing data points by month (represented by different shades of color).\n\n4.4.1.5.5.1 Key Insights:\n\nPositive Correlation Between Temperature and Ozone:\n\nThere is a clear positive correlation between temperature and ozone levels. As the temperature increases, ozone levels tend to increase as well.\nThis trend is particularly evident at temperatures above 75°F, where the ozone levels start to rise more significantly.\n\nOzone Increases with Higher Temperatures:\n\nOzone levels generally remain below 50 ppb for temperatures below 70°F, but as temperatures rise beyond this point, ozone levels increase, with many values exceeding 100 ppb when the temperature reaches around 90°F and above.\nThis indicates that higher temperatures contribute to higher ozone concentrations.\n\nMonthly Influence on Ozone and Temperature:\n\nThe data points are colored by month, with lighter colors (Month 5 = May) representing earlier months and darker colors (Month 9 = September) representing later months.\nIn the warmer months (July, August, September), represented by darker shades of purple and blue, there are more instances of high ozone levels, particularly at temperatures above 80°F.\nLower months (May and June), represented by lighter shades of pink, tend to have lower ozone levels overall, even at higher temperatures, which suggests a seasonal effect on ozone levels.\n\nOutliers and Extreme Ozone Levels:\n\nThere are a few outliers where ozone levels exceed 150 ppb, particularly for temperatures around 85°F to 90°F. These data points mostly appear in July and August, indicating that extreme ozone events occur during the peak summer months.\nThe outliers reflect instances where ozone levels spike dramatically even though the temperature is not exceptionally high.\n\n\n\n\n4.4.1.5.5.2 General Trends:\n\nTemperature-Driven Ozone Formation: The positive correlation between temperature and ozone levels suggests that ozone formation increases as temperatures rise, particularly during the summer months.\nSeasonal Influence: The coloring by month indicates a seasonal pattern, where ozone levels are generally higher in the later summer months (July to September) compared to earlier in the year (May and June).\nHigh Ozone in Summer: The majority of the higher ozone levels are observed in July and August, especially at temperatures above 80°F, reflecting the impact of summer heat on ozone concentration.\n\n\n\n4.4.1.5.5.3 Conclusion:\nThis scatter plot highlights the strong relationship between temperature and ozone levels, with higher temperatures leading to increased ozone concentrations, particularly during the summer months. The seasonal trend suggests that ozone levels peak during July and August, likely due to the combination of higher temperatures and environmental conditions that promote ozone formation. This information can be critical for air quality monitoring and predicting ozone pollution during the warmer months of the year.\n\n\n\n4.4.1.5.6 Correlation Matrix of Ozone, Temp, Wind\n# Correlation matrix\ncorr = airquality_df[['Ozone', 'Temp', 'Wind']].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\nCorrelation Matrix of Ozone, Temp and Wind\n\n\nThis correlation matrix visualizes the relationships between ozone levels, temperature, and wind speed. The matrix provides values between -1 and 1, where: - 1 indicates a perfect positive correlation, - -1 indicates a perfect negative correlation, and - 0 indicates no correlation.\n\n4.4.1.5.6.1 Key Insights:\n\nOzone and Temperature (Correlation = 0.7):\n\nThere is a strong positive correlation of 0.7 between ozone levels and temperature.\nThis suggests that as temperature increases, ozone levels tend to increase as well. This aligns with the previous visualizations where high temperatures were associated with higher ozone concentrations.\n\nOzone and Wind Speed (Correlation = -0.6):\n\nThere is a moderate negative correlation of -0.6 between ozone levels and wind speed.\nThis indicates that as wind speed increases, ozone levels tend to decrease. Wind likely disperses ozone, leading to lower concentrations.\n\nTemperature and Wind Speed (Correlation = -0.46):\n\nThere is a moderate negative correlation of -0.46 between temperature and wind speed.\nThis suggests that higher temperatures tend to occur when wind speeds are lower, and vice versa.\n\n\n\n\n4.4.1.5.6.2 General Trends:\n\nStrong Positive Correlation Between Ozone and Temperature: The positive correlation between ozone and temperature indicates that ozone levels tend to rise with higher temperatures, which may be due to increased sunlight and chemical reactions that generate ozone.\nNegative Correlation Between Ozone and Wind: The negative correlation between ozone and wind suggests that windier conditions help disperse ozone, leading to lower concentrations.\nInterplay of Wind and Temperature: The negative correlation between wind speed and temperature suggests that calmer, warmer conditions are conducive to higher ozone levels, while windier conditions are typically associated with lower temperatures and lower ozone concentrations.\n\n\n\n4.4.1.5.6.3 Conclusion:\nThe correlation matrix provides valuable insights into the relationship between ozone, temperature, and wind speed. The strong positive correlation between ozone and temperature suggests that hotter days lead to higher ozone concentrations, while the negative correlation between wind and ozone indicates that wind helps reduce ozone levels by dispersing it. This matrix highlights the complex interplay between these environmental factors, with temperature driving ozone formation and wind acting as a dispersive force.\n\n\n\n\n\n4.4.2 Friday\nOn Friday, I decided to go one step ahead and try these python basics on another dataset, the Swiss Dataset.\n\n4.4.2.1 Overview of the Swiss Dataset\nThe Swiss dataset, originally from RStudio, provides standardized fertility and socio-economic data for 47 French-speaking provinces of Switzerland in 1888. This dataset is commonly used for exploring relationships between fertility rates and various socio-economic factors.\n\n4.4.2.1.1 Dataset Structure:\nThe dataset consists of six variables: 1. Fertility: Standardized fertility measure. 2. Agriculture: Percentage of males involved in agriculture as an occupation. 3. Examination: Percentage of draftees receiving the highest mark on army examination. 4. Education: Percentage of education beyond primary school for draftees. 5. Catholic: Percentage of the population that is Catholic (a measure of religion). 6. Infant.Mortality: Number of live births who live less than 1 year per 1000 births.\n\n\n4.4.2.1.2 Dataset Characteristics:\n\nThe dataset contains 47 entries (rows) for each variable.\nThere are no missing values in any of the columns, ensuring a complete dataset for analysis.\nThe data types for the variables are primarily float64, except for Examination and Education, which are of type int64.\n\n\n\n4.4.2.1.3 Basic Statistics:\nBelow are the key statistics for each variable based on the data:\n\nFertility:\n\nMean: 70.14\nMinimum: 35, Maximum: 92.5\n50th percentile (median): 70.4\n\nAgriculture:\n\nMean: 50.66%\nMinimum: 1.2%, Maximum: 89.7%\n50th percentile (median): 54%\n\nExamination:\n\nMean: 16.49%\nMinimum: 3%, Maximum: 37%\n50th percentile (median): 16%\n\nEducation:\n\nMean: 10.98%\nMinimum: 1%, Maximum: 53%\n50th percentile (median): 8%\n\nCatholic:\n\nMean: 41.14%\nMinimum: 2.15%, Maximum: 100%\n50th percentile (median): 15.14%\n\nInfant.Mortality:\n\nMean: 19.94 per 1000 births\nMinimum: 10.8, Maximum: 26.6\n50th percentile (median): 20.0\n\n\n\n\n4.4.2.1.4 Key Insights:\n\nFertility:\n\nFertility rates range from 35 to 92.5, with a mean of 70.14. The median fertility is 70.4, indicating that most provinces have moderate to high fertility rates.\n\nAgriculture:\n\nThe percentage of males involved in agriculture varies significantly across provinces, ranging from 1.2% to 89.7%. The median is 54%, showing that agriculture was a major occupation in many regions during this time.\n\nExamination and Education:\n\nThe Examination score ranges from 3% to 37%, with a mean of 16.49%, indicating that only a small portion of draftees received the highest marks.\nThe Education level ranges from 1% to 53%, with a median of 8%, suggesting that a relatively small portion of the population had education beyond primary school.\n\nCatholic:\n\nThe percentage of Catholic population varies significantly, from 2.15% to 100%, reflecting the diversity in religious adherence across the provinces. The median is 15.14%, but the dataset includes provinces with a fully Catholic population.\n\nInfant Mortality:\n\nInfant Mortality ranges from 10.8 to 26.6 per 1000 births, with a mean of 19.94. The median is 20.0, indicating that most provinces had similar infant mortality rates, though some had much higher rates.\n\n\n\n\n4.4.2.1.5 Potential Analysis:\n\nCorrelation Analysis: Investigating how fertility is related to variables like education, agriculture, and religion (Catholic).\nGeographical Trends: Analyzing how these socio-economic factors vary across different regions in Switzerland.\nSocio-Economic Influence: Understanding the impact of education, religion, and occupation (agriculture) on fertility and infant mortality rates.\n\n\n\n4.4.2.1.6 Conclusion:\nThe Swiss dataset provides valuable insights into fertility rates and socio-economic factors in 47 Swiss provinces in the late 19th century. With complete data and a range of variables, this dataset is an excellent resource for studying the relationships between fertility, occupation, education, religion, and infant mortality in a historical context.\n\n\n\n4.4.2.2 Python Visualizations\nNext, we use python to create visualizations on the swiss dataset.\nimport pandas as pd\nswiss_df = pd.read_csv('swiss_datasets.csv')\nprint(swiss_df.info())\nprint(swiss_df.describe())\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 47 entries, 0 to 46\nData columns (total 6 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Fertility         47 non-null     float64\n 1   Agriculture       47 non-null     float64\n 2   Examination       47 non-null     int64  \n 3   Education         47 non-null     int64  \n 4   Catholic          47 non-null     float64\n 5   Infant.Mortality  47 non-null     float64\ndtypes: float64(4), int64(2)\nmemory usage: 2.3 KB\nNone\n       Fertility  Agriculture  Examination  Education   Catholic  \\\ncount  47.000000    47.000000    47.000000  47.000000   47.00000   \nmean   70.142553    50.659574    16.489362  10.978723   41.14383   \nstd    12.491697    22.711218     7.977883   9.615407   41.70485   \nmin    35.000000     1.200000     3.000000   1.000000    2.15000   \n25%    64.700000    35.900000    12.000000   6.000000    5.19500   \n50%    70.400000    54.100000    16.000000   8.000000   15.14000   \n75%    78.450000    67.650000    22.000000  12.000000   93.12500   \nmax    92.500000    89.700000    37.000000  53.000000  100.00000   \n\n       Infant.Mortality  \ncount         47.000000  \nmean          19.942553  \nstd            2.912697  \nmin           10.800000  \n25%           18.150000  \n50%           20.000000  \n75%           21.700000  \nmax           26.600000  \n\n4.4.2.2.1 Distribution of Infant Mortality\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,6))\nplt.hist(swiss_df['Infant.Mortality'].dropna(), bins=20, color='red', edgecolor='black')\nplt.title('Distribute of Infant Mortality')\nplt.xlabel('Infant Mortality')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\nDistribution of Infant Mortality\n\n\nThis histogram visualizes the distribution of Infant Mortality rates (per 1000 live births) for the Swiss dataset.\n\n4.4.2.2.1.1 Key Insights:\n\nCentral Tendency:\n\nThe peak of the distribution is around 20 infant deaths per 1000 births, with the highest frequency of provinces having this mortality rate.\nThe mean infant mortality rate is approximately 19.94, aligning closely with the histogram’s peak.\n\nModerate Spread:\n\nThe majority of the data is concentrated between 18 and 22 infant deaths per 1000 births. This indicates that most provinces had relatively similar infant mortality rates.\nThe central grouping of bars shows that most provinces had moderate infant mortality.\n\nTails on Both Ends:\n\nOn the lower end, a few provinces have relatively low infant mortality, with values around 12 to 16 per 1000 births.\nOn the higher end, a few provinces have infant mortality rates reaching up to 26 per 1000 births, but these higher mortality rates are less frequent.\n\nOutliers:\n\nThere is a slight outlier around 12 infant deaths per 1000 births, representing a province with significantly lower mortality than most others.\nSimilarly, provinces with mortality rates above 24 per 1000 are relatively rare, indicating that extremely high infant mortality rates were uncommon.\n\n\n\n\n4.4.2.2.1.2 General Trends:\n\nMost provinces in this dataset experienced infant mortality rates between 18 and 22 per 1000 births, showing that this range is the norm.\nThere is moderate variation in infant mortality rates, with a few provinces at the lower and higher extremes, indicating some regional differences.\n\n\n\n4.4.2.2.1.3 Conclusion:\nThe histogram illustrates that infant mortality in Swiss provinces during this period is concentrated around 20 deaths per 1000 births, with some variation on both ends. A few provinces experienced unusually high or low rates, but the overall trend is centered around moderate infant mortality. This provides insights into health conditions and regional disparities in 19th century Switzerland.\n\n\n\n4.4.2.2.2 Distribution of Education\nplt.figure(figsize=(8,6))\nplt.hist(swiss_df['Education'].dropna(), bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Education')\nplt.xlabel('Education')\nplt.ylabel('Frequency')\nplt.show()\n This histogram visualizes the distribution of Education in the Swiss dataset, where education represents the percentage of draftees with education beyond primary school.\n\n4.4.2.2.2.1 Key Insights:\n\nBimodal Distribution:\n\nThe distribution shows a bimodal pattern, with two clear peaks.\nThe first peak is around 10% education, with most provinces falling within the 0-15% range.\nThere is a second, smaller peak around 30% education, indicating a few provinces with significantly higher levels of education beyond primary school.\n\nConcentration at Low Education Levels:\n\nA large portion of the dataset is concentrated at lower education levels, particularly between 0% and 15%. The highest frequency occurs between 5% and 10%, indicating that most provinces had a relatively low percentage of draftees receiving education beyond primary school.\nThis highlights the limited access to advanced education for most provinces during this period.\n\nGaps in Education Levels:\n\nThere is a noticeable gap between 15% and 25% where very few provinces fall, indicating that there were fewer provinces with moderate education levels.\nThis could suggest that provinces either had low or relatively high education levels, but not many had intermediate values.\n\nHigh Education Outliers:\n\nThere are a few provinces with extremely high levels of education, with one outlier reaching 50%. These outliers represent provinces where a significant portion of the population pursued education beyond primary school, a rarity in the dataset.\n\n\n\n\n4.4.2.2.2.2 General Trends:\n\nMajority of Provinces with Low Education Levels: Most provinces in Switzerland during this period had low education rates, with the majority of values concentrated between 0% and 15%.\nHigher Education Outliers: A small number of provinces had significantly higher levels of education, particularly around 30% and 50%, suggesting substantial regional differences in access to education.\n\n\n\n4.4.2.2.2.3 Conclusion:\nThe histogram reveals that most provinces had low education levels, with few opportunities for education beyond primary school. However, a few provinces show significantly higher education rates, indicating regional disparities in educational attainment. This distribution underscores the uneven access to education in 19th-century Switzerland, where the majority of the population had limited access to further education.\n\n\n\n4.4.2.2.3 Boxplot of Examination\nplt.figure(figsize=(8, 6))\nplt.boxplot(swiss_df['Examination'].dropna())\nplt.title('Boxplot of Examination')\nplt.ylabel('Examination')\nplt.show()\n\n\n\nBoxplot of Examination\n\n\nThis boxplot visualizes the distribution of Examination scores in the Swiss dataset, where Examination refers to the percentage of draftees receiving the highest mark in the military examination.\n\n4.4.2.2.3.1 Key Insights:\n\nMedian Examination Score:\n\nThe median examination score is approximately 16%, as indicated by the orange line in the box. This suggests that, on average, 16% of draftees in each province achieved the highest mark.\n\nInterquartile Range (IQR):\n\nThe IQR (the range between the 25th and 75th percentiles) spans from about 12% to 22%. This indicates that the middle 50% of the provinces had between 12% and 22% of their draftees achieving the highest marks.\nThe relatively narrow IQR suggests that most provinces had similar examination outcomes within this range.\n\nWhiskers and Data Spread:\n\nThe lower whisker extends down to 3%, representing the province with the lowest percentage of draftees receiving the highest examination mark.\nThe upper whisker extends up to 37%, indicating that the highest-performing province had 37% of its draftees achieving the top marks.\nThe whiskers indicate that most provinces fall between 3% and 37%, with no extreme outliers in the data.\n\nSymmetry of Distribution:\n\nThe boxplot shows a relatively balanced distribution, with the median line close to the center of the IQR. The whiskers are roughly of equal length, indicating that the data is not heavily skewed.\n\n\n\n\n4.4.2.2.3.2 General Trends:\n\nThe majority of the provinces have between 12% and 22% of draftees receiving the highest marks on the military examination.\nThe examination scores do not have any outliers, suggesting that there is consistent performance across provinces with no extreme deviations.\n\n\n\n4.4.2.2.3.3 Conclusion:\nThis boxplot of the Examination scores reveals a moderately concentrated distribution, with most provinces achieving similar performance. The median score of 16% and the spread between 3% and 37% suggest some variation between provinces, but the overall distribution is balanced. This data provides insights into the level of military readiness and education among draftees in different provinces of Switzerland during this period.\n\n\n\n4.4.2.2.4 Scatterplot of Fertility vs Infant Mortality by Catholic Population\nimport seaborn as sns\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(x='Fertility', y='Infant.Mortality', hue='Catholic', data=swiss_df)\nplt.title('Fertility vs Infant Mortality by Catholic population')\nplt.xlabel('Fertility')\nplt.ylabel('Infant Mortality')\nplt.show()\n\n\n\nFertility vs Infant Mortality by Catholic Population\n\n\nThis scatter plot visualizes the relationship between fertility rates and infant mortality across the Swiss dataset, with the color hue representing the percentage of Catholic population in each province.\n\n4.4.2.2.4.1 Key Insights:\n\nPositive Correlation Between Fertility and Infant Mortality:\n\nThere is a positive correlation between fertility and infant mortality. As the fertility rate increases, infant mortality tends to increase as well. Provinces with higher fertility generally experience higher rates of infant mortality.\nThis relationship indicates that regions with more births also have higher rates of infant deaths.\n\nImpact of Catholic Population:\n\nThe color hue represents the percentage of the population that is Catholic, with darker points indicating provinces with a higher percentage of Catholics.\nProvinces with higher fertility rates and higher infant mortality (upper right portion of the plot) tend to have a higher Catholic population (darker points). This suggests that provinces with a larger Catholic population tend to have higher fertility and infant mortality rates.\nIn contrast, provinces with lower fertility and lower infant mortality (lower left portion) generally have a lower Catholic population (lighter points).\n\nConcentration of Points Around Median Fertility:\n\nA significant number of points are concentrated between fertility rates of 60 and 80 and infant mortality rates between 18 and 22. These central values suggest that many provinces fall within this range, indicating a moderate relationship between fertility and infant mortality.\nProvinces with these mid-range values are spread across different levels of Catholic population, showing diversity in religious composition within this group.\n\nOutliers:\n\nSome provinces with extremely high fertility rates (above 80) and high infant mortality rates (above 24) are outliers, and they tend to have a high percentage of Catholic population.\nOn the lower end, a few provinces with low fertility rates (below 50) and low infant mortality also stand out as exceptions.\n\n\n\n\n4.4.2.2.4.2 General Trends:\n\nHigher Catholic Population: Provinces with a higher percentage of Catholics tend to exhibit both higher fertility rates and higher infant mortality.\nModerate Fertility and Mortality Range: Most provinces are concentrated around 60-80 fertility and 18-22 infant mortality, with varying Catholic population percentages.\nReligious Influence: The distribution of Catholic population suggests a potential cultural or socio-economic influence on fertility and infant mortality trends.\n\n\n\n4.4.2.2.4.3 Conclusion:\nThis scatter plot highlights a positive correlation between fertility and infant mortality, with provinces that have a higher Catholic population generally showing higher rates in both metrics. This relationship suggests that religious and cultural factors might play a role in shaping fertility and infant health outcomes in Swiss provinces. Further analysis could explore whether this pattern is driven by socio-economic factors or access to healthcare.\n\n\n\n4.4.2.2.5 Correlation Matrix of Catholic Population, Education and Infant Mortality\ncorr = swiss_df[['Catholic', 'Education', 'Infant.Mortality']].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\nCorrelation matrix of Catholic Population, Education, and Infant Mortality\n\n\nThis correlation matrix shows the relationships between three variables from the Swiss dataset: Catholic (percentage of Catholic population), Education (percentage of education beyond primary school for draftees), and Infant Mortality (infant deaths per 1000 live births).\n\n4.4.2.2.5.1 Key Insights:\n\nCatholic and Infant Mortality (0.18):\n\nThere is a positive correlation of 0.18 between the Catholic population and Infant Mortality. While this is a weak correlation, it suggests that provinces with a higher percentage of Catholics tend to have slightly higher infant mortality rates.\nThis positive correlation could be linked to cultural, religious, or socio-economic factors in predominantly Catholic regions that may affect infant mortality rates.\n\nCatholic and Education (-0.15):\n\nThere is a negative correlation of -0.15 between Catholic population and Education. Although the correlation is weak, it indicates that provinces with a higher percentage of Catholics tend to have slightly lower levels of education beyond primary school.\nThis trend might reflect historical or cultural differences in access to education in Catholic regions.\n\nEducation and Infant Mortality (-0.099):\n\nThe correlation between Education and Infant Mortality is -0.099, which is also weak but negative. This indicates that provinces with higher levels of education tend to have slightly lower infant mortality rates.\nThis finding suggests that increased education may have a minor protective effect on infant mortality, likely due to better healthcare knowledge or socioeconomic conditions associated with higher education levels.\n\n\n\n\n4.4.2.2.5.2 General Observations:\n\nWeak Correlations: All correlations in the matrix are relatively weak (close to 0), indicating that there is no strong linear relationship between these variables. The weak positive correlation between Catholic and Infant Mortality is the highest, but still not strong.\nCultural and Socio-Economic Factors: The relationships between Catholicism, education, and infant mortality may be influenced by a variety of socio-economic and cultural factors that are not fully captured by these simple correlations.\n\n\n\n4.4.2.2.5.3 Conclusion:\nThis correlation matrix reveals weak associations between Catholic population, education, and infant mortality. The slightly positive correlation between Catholicism and infant mortality suggests a possible link worth further investigation, while the negative correlation between education and infant mortality aligns with expectations that higher education may contribute to better health outcomes. However, the weak correlations indicate that other factors likely play a significant role in shaping these relationships.\n\n\n\n\n\n4.4.3 Week 4 Conclusion\n\n4.4.3.1 Python Basics and Data Analysis\nThis week, we focused on several fundamental and advanced Python concepts, particularly in the context of data analysis, using libraries like numpy, pandas, matplotlib, and seaborn. Here’s a breakdown of the key areas covered:\n\n4.4.3.1.1 1. Python Lists & Arrays\n\nPracticed working with basic Python lists and numpy arrays for handling numerical computations.\nReinforced foundational Python skills by manipulating lists and performing basic array operations.\n\n# Example of using numpy arrays\nimport numpy as np\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\n\n\n4.4.3.1.2 2. DataFrames with Pandas\n\nExplored pandas for creating and manipulating dataframes.\nExtracted statistics from datasets and analyzed their structure using key functions like describe() and info().\n\n# Creating a dataframe\nimport pandas as pd\ndata = {'Ozone': [41, 36, 12], 'Temp': [67, 72, 74]}\ndf = pd.DataFrame(data)\nprint(df.describe())\n\n\n4.4.3.1.3 3. Airquality Dataset Analysis\n\nOverview: Analyzed air quality data in New York from May to September 1973, focusing on variables like ozone, temperature, and wind speed.\nVisualizations: Created histograms and scatter plots to understand the distribution and relationships between these variables.\nKey Insights:\n\nHigher temperatures correlated with higher ozone levels.\nWind speeds showed a negative correlation with ozone concentration.\nMissing values in Ozone and Solar Radiation were identified as needing further handling.\n\n\n# Scatter plot of Temperature vs Ozone Levels\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.scatterplot(x='Temp', y='Ozone', data=df)\nplt.show()\n\n\n4.4.3.1.4 4. Swiss Dataset Analysis\n\nOverview: Explored fertility and socio-economic factors in 47 Swiss provinces from 1888, examining variables like fertility, education, and Catholic population percentages.\nVisualizations: Created visualizations like histograms and boxplots to understand the distribution of variables such as education and infant mortality.\nCorrelation Analysis: Investigated relationships between fertility, infant mortality, and Catholic population using a correlation matrix.\nKey Insights:\n\nHigher fertility rates correlated with higher infant mortality.\nCatholic provinces tended to show higher fertility and infant mortality.\nEducation was inversely correlated with infant mortality.\n\n\n# Correlation Matrix Heatmap\ncorr = df[['Catholic', 'Education', 'Infant.Mortality']].corr()\nsns.heatmap(corr, annot=True)\nplt.show()\n\n\n\n4.4.3.2 Conclusion\nThis week was an in-depth exploration of Python’s capabilities for data analysis, focusing on extracting meaningful insights from real-world datasets. The work demonstrated strong relationships between environmental and socio-economic variables, using visualizations and statistical analysis to uncover trends. These skills are essential for further data-driven projects, offering opportunities for deeper exploration of historical datasets and environmental data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>National Center for Education Statistics - Chris Jacob</span>"
    ]
  },
  {
    "objectID": "chris.html#week-5---0923-0927",
    "href": "chris.html#week-5---0923-0927",
    "title": "4  National Center for Education Statistics - Chris Jacob",
    "section": "4.5 Week 5 - 09/23 ~ 09/27",
    "text": "4.5 Week 5 - 09/23 ~ 09/27\n\n4.5.1 Wednesday\n\n4.5.1.1 Plotnine Tutorial: Understanding the Grammar of Graphics\n\n4.5.1.1.1 1. Introduction to Plotnine\nplotnine is a data visualization package for Python based on the Grammar of Graphics, which is a system for understanding and building plots. The grammar describes how plots are constructed by combining data, aesthetic mappings, geometric objects, and other components.\nTo begin, you’ll need to install the plotnine package if you don’t have it installed:\npip install plotnine\n\n\n4.5.1.1.2 2. The Grammar of Graphics\nThe Grammar of Graphics consists of the following key components:\n\nData: The data you want to visualize.\n\nAesthetics (aes): How the data is mapped to visual properties, such as x and y coordinates, color, size, etc.\n\nGeometries (geom): The type of plot, like points, lines, bars, etc.\n\nFacets: Subplots based on the data.\n\nScales: Control the mapping from data to aesthetic properties.\n\nCoordinate systems: Adjust how data is projected on the plane (Cartesian, rotations, polar, etc.).\n\nThemes: Adjust the non-data elements like background, labels, gridlines, etc.\n\n\n\n4.5.1.1.3 3. Creating Your First Plot\nLet’s begin by creating a simple scatter plot using the famous mtcars dataset. We’ll show how to set up the basic structure and gradually build complexity.\n# Import required libraries\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_point, labs\n\n# Load the mtcars dataset\nmtcars = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv')\nmtcars.info()\n\n# Create a basic scatter plot\np1 = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n labs(title='Scatter Plot of MPG vs Weight',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\np1.save(\"ScatterPlot_MPG_Weight.png\")\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32 entries, 0 to 31\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   mpg      32 non-null     float64\n 1   cyl      32 non-null     int64  \n 2   disp     32 non-null     float64\n 3   hp       32 non-null     int64  \n 4   drat     32 non-null     float64\n 5   wt       32 non-null     float64\n 6   qsec     32 non-null     float64\n 7   vs       32 non-null     int64  \n 8   am       32 non-null     int64  \n 9   gear     32 non-null     int64  \n 10  carb     32 non-null     int64  \n 11  fast     32 non-null     int64  \n 12  cars     32 non-null     object \n 13  carname  32 non-null     object \ndtypes: float64(5), int64(7), object(2)\nmemory usage: 3.6+ KB\n\n\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: ScatterPlot_MPG_Weight.png\n\n\n\nScatter Plot of MPG vs Weight\n\n\n\n\n4.5.1.1.4 4. Adding Aesthetic Mappings\nIn the Grammar of Graphics, aesthetics control how data points are represented visually. You can map variables to size, color, shape, and more.\n\n4.5.1.1.4.1 Example: Color by cyl (number of cylinders)\np2 = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\np2.save(\"MPG_Weight_Cylinder.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG_Weight_Cylinder.png\n\n\n\nMPG vs Weight by Cylinder\n\n\n\n\n4.5.1.1.4.2 Example: Size by horsepower (hp)\np3 = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)', size='hp')) +\n geom_point() +\n labs(title='MPG vs Weight by Cylinder and Horsepower',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders',\n      size='Horsepower'))\np3.save(\"MPG_Weight_Cyl_HP.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG_Weight_Cyl_HP.png\n\n\n\nMPG vs Weight by Cylinder and Horsepower\n\n\n\n\n\n4.5.1.1.5 5. Geometric Objects\ngeom_* specifies the type of plot. You can create scatter plots, line charts, bar plots, histograms, etc.\n\n4.5.1.1.5.1 Example: Adding a smooth line (geom_smooth)\nfrom plotnine import geom_smooth\n\np4 = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n geom_smooth(method='lm') +  # Linear regression line\n labs(title='MPG vs Weight with Regression Line',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\np4.save(\"MPG_Weight_Regression.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG_Weight_Regression.png\n\n\n\nMPG vs Weight with Regression Line\n\n\n\n\n\n4.5.1.1.6 6. Faceting\nFaceting allows you to split your plot into multiple panels based on a factor.\n\n4.5.1.1.6.1 Example: Facet by cyl\nfrom plotnine import facet_wrap\n\np5 = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n facet_wrap('~cyl') +  # Split into subplots by cylinders\n labs(title='MPG vs Weight Faceted by Cylinder',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\np5.save(\"MPG_Weight_Faceted.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG_Weight_Faceted.png\n\n\n\nMPG vs Weight Faceted by Cylinder\n\n\n\n\n\n4.5.1.1.7 7. Customizing Scales\nScales control the mapping from data to aesthetic attributes. You can customize scales for color, size, and more.\n\n4.5.1.1.7.1 Example: Custom Color Scale\nfrom plotnine import scale_color_manual\n\np6 = (ggplot(mtcars, aes(x='wt', y='mpg', color='factor(cyl)')) +\n geom_point() +\n scale_color_manual(values=['#1e07bf', '#fe0f0f', '#2e002f']) +  # Custom colors\n labs(title='MPG vs Weight with Custom Colors',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon',\n      color='Cylinders'))\n\np6.save(\"MPG_Weight_CustomColors.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: MPG_Weight_CustomColors.png\n\n\n\nMPG vs Weight with Custom Colors\n\n\n\n\n\n4.5.1.1.8 8. Flip Coordinates\nCreate a bar plot showing distribution of cylinders\n\n4.5.1.1.8.1 Example: Fliping coordinates axis\nfrom plotnine import geom_bar, coord_flip\n\n# Create a bar plot showing distribution of cylinders\np7 = (ggplot(mtcars, aes(x='factor(cyl)', fill='factor(cyl)')) +\n geom_bar(width=1) +\n coord_flip() +  # Flip coordinates as a simple workaround\n labs(title='Distribution of Cylinders',\n      x='Cylinders',\n      fill='Cylinders'))\np7.save(\"Dist_Cyl.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: Dist_Cyl.png\n\n\n\nDistribution of Cylinders\n\n\n\n\n\n4.5.1.1.9 9. Themes\nThemes allow you to adjust the non-data aspects of the plot, such as background, axis labels, and gridlines.\n\n4.5.1.1.9.1 Example: Apply a Minimal Theme\nfrom plotnine import theme_minimal\n\np8 = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n geom_point() +\n theme_minimal() +  # Minimalistic theme\n labs(title='MPG vs Weight with Minimal Theme',\n      x='Weight (1000 lbs)',\n      y='Miles per Gallon'))\np8.save(\"Minimal_Theme.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: Minimal_Theme.png\n\n\n\nMPG vs Weight with Minimal Theme\n\n\n\n\n\n4.5.1.1.10 10. Saving the Plot\nYou can save your plot using the save method.\n\n4.5.1.1.10.1 Example: Save the plot\n# Save the plot to a file\np9 = (ggplot(mtcars, aes(x='wt', y='mpg')) +\n     geom_point() +\n     labs(title='MPG vs Weight',\n          x='Weight (1000 lbs)',\n          y='Miles per Gallon'))\n\np9.save(\"mpg_vs_weight.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: mpg_vs_weight.png\n\n\n\nMPG vs Weight\n\n\np10 = (ggplot(mtcars, aes(x='wt', y='hp')) +\n geom_point() +\n geom_smooth(method='lm') +\n labs(title='Scatter Plot of HP vs Weight',\n      x='Weight (1000 lbs)',\n      y='Horsepower'))\np10.save(\"ScatterPlot_HP_vs_Weight_ErrorBars.png\")\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:606: PlotnineWarning: Saving 6.4 x 4.8 in image.\nc:\\users\\chris\\desktop\\oru folder\\fall 2024\\gcsc 577\\group4\\python_unit_1\\week_4_chris\\venv477\\lib\\site-packages\\plotnine\\ggplot.py:607: PlotnineWarning: Filename: ScatterPlot_HP_vs_Weight_ErrorBars.png\n\n\n\nScatter Plot of HP vs Weight\n\n\n\n\n\n\n\n4.5.2 Friday\n\n4.5.2.1 Parental Education and Early STEM Interest\n\n\n\nParental Education vs Early STEM Interest\n\n\nThis visualization examines the relationship between parental education levels and students’ early interest in STEM fields (math and science). While higher parental education, particularly Doctorate/Professional degrees, is associated with more consistent interest in STEM, there is moderate variation across all education levels, suggesting that other factors also influence STEM interest.\nThe findings highlight the role of parental education in fostering early STEM interest, contributing to the project’s goal of understanding the correlation between early STEM exposure and STEM career choices. The wide range of interest across education levels suggests that school or personal motivation may also be significant, pointing to opportunities for interventions, such as targeted STEM programs, to nurture interest across diverse family backgrounds.\n\n\n4.5.2.2 Dataset Context - HSLS Data\nThe dataset is drawn from the High School Longitudinal Study of 2009 (HSLS:09), which tracks a cohort of students who entered 9th grade in the fall of 2009. The study captures detailed data on students’ academic achievements, interests, attitudes, and family and school characteristics, with a particular focus on STEM (Science, Technology, Engineering, Mathematics) fields. The dataset includes measures on math and science performance, parental occupation, student demographics, and self-reported attitudes towards STEM subjects. You can find out more here\n\n4.5.2.2.0.1 Data Source\n\nSource: The dataset originates from the National Center for Education Statistics (NCES), a U.S. government organization that collects and analyzes educational data. The data is publicly available through NCES’s website and is widely used for educational research and policy evaluation.\nSurvey: HSLS:09, a comprehensive survey of U.S. high school students with follow-ups tracking their progress through high school, into postsecondary education, and into the workforce.\n\n\n\n4.5.2.2.0.2 30 Columns and Randomized Data\nWe meticulously selected 30 columns representing key aspects of STEM participation, demographic factors, academic performance, and parental influence to analyze the impact of various factors on students’ STEM trajectories. These columns were chosen to enable analysis of the relationships between student performance in STEM subjects, family background, self-perception, and eventual aspirations in STEM careers.\n\n\n\n4.5.2.3 The 5 W’s Breakdown\n\n4.5.2.3.1 1. Who (Who does the data represent?)\n\nWho: The data represents students who began 9th grade in 2009 in public and private schools across the United States. It includes information on the students, their families, teachers, and schools.\nPopulation: The dataset covers a diverse cross-section of U.S. students, including different races, income levels, genders, and academic achievements.\n\n\n\n4.5.2.3.2 2. When (When was the data collected?)\n\nWhen: The baseline data collection occurred in 2009, with follow-up surveys conducted later as the students progressed through high school and beyond. This timeline allows us to observe students’ academic growth and career trajectories.\nSpecific Timeframes:\n\nBaseline: Fall 2009 (9th grade).\nFollow-up waves: Conducted in subsequent years to track academic progress and STEM involvement.\n\n\n\n\n4.5.2.3.3 3. Where (Where was the data collected?)\n\nWhere: The dataset covers schools in the United States across various regions, including urban, suburban, and rural areas. The dataset includes school-level identifiers, allowing for analysis of geographic or school-specific trends in STEM education.\nSchools: Public and private schools in different locales (urban, suburban, and rural).\n\n\n\n4.5.2.3.4 4. What (What does the dataset include?)\n\nWhat: The dataset includes a wide range of variables on student academic performance, demographic information, family background, school resources, and student attitudes towards STEM subjects.\n\nKey Data Fields:\n\nDemographics: Student gender (X1SEX), race/ethnicity (X1RACE), family income (X1FAMINCOME), and parental education (X1PAREDU).\nSTEM Participation: Interest in math (X1MTHINT) and science (X1SCIINT), math self-efficacy (X1MTHEFF), parental occupation in STEM fields (X1PAR1OCC_STEM1), and more.\nPerformance: Math and science test scores (X1TXMTH, X1SCIEFF), proficiency levels (X1TXMPROF1 to X1TXMPROF5), and school engagement (X1SCHOOLENG).\nParental Influence: Parental occupation and education levels (X1PAREDU, X1PAR1OCC_STEM1).\n\n\n\n\n\n4.5.2.3.5 5. Why (Why was the data collected? Why are we analyzing these columns?)\n\nWhy: The HSLS:09 dataset was collected to understand how students’ academic experiences, particularly in STEM fields, evolve over time. It explores how family background, school resources, and self-perceptions affect students’ performance and career aspirations, especially in STEM-related fields.\n\nPurpose of Analysis:\n\nTo analyze: The chosen columns allow us to study the relationship between student demographics (gender, race), academic performance in math and science, and future STEM aspirations.\nTo uncover: How parental STEM occupations or educational backgrounds influence students’ interest and success in STEM, and whether specific demographic groups are underrepresented in STEM participation.\n\n\n\n\n\n4.5.2.3.6 6. How (How is the data structured and how will we use it?)\n\nHow: The dataset is structured into variables representing both categorical (e.g., gender, race) and continuous (e.g., math scores, family income) data. The data is collected from students, their parents, schools, and teachers.\n\nData Analysis:\n\nWe will create visualizations that can help correlate STEM performance with demographic variables, family influences, and student self-efficacy in math and science.\n\n\n\n\n\n\n4.5.2.4 Conclusion\nThe chosen 30 columns provide a comprehensive view of student, family, and school characteristics, making it possible to explore STEM participation trends through factors such as demographics, academic performance, family influence, and self-perception. By leveraging these insights, we aim to identify gaps in STEM education and propose strategies to increase participation among underrepresented groups.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>National Center for Education Statistics - Chris Jacob</span>"
    ]
  }
]